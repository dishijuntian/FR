#!/usr/bin/env python3

"""
èˆªç­æ’åç³»ç»Ÿä¸»å…¥å£ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
è§£å†³å¯åŠ¨å»¶è¿Ÿå’Œè­¦å‘Šé—®é¢˜
"""

import os
import sys
import time
import warnings
from pathlib import Path

# ==================== æ€§èƒ½ä¼˜åŒ–è®¾ç½® ====================
def setup_environment():
    """ä¼˜åŒ–ç¯å¢ƒè®¾ç½®"""
    # æŠ‘åˆ¶å„ç§è­¦å‘Šå’Œä¸å¿…è¦çš„è¾“å‡º
    warnings.filterwarnings('ignore')
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # TensorFlowé™é»˜
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # CUDAå¼‚æ­¥æ¨¡å¼
    os.environ['PYTHONWARNINGS'] = 'ignore'   # Pythonè­¦å‘Šé™é»˜
    os.environ['NUMBA_DISABLE_JIT'] = '0'     # å¯ç”¨NumbaåŠ é€Ÿ
    
    # XGBoostä¼˜åŒ–
    os.environ['OMP_NUM_THREADS'] = str(min(os.cpu_count(), 8))
    os.environ['MKL_NUM_THREADS'] = str(min(os.cpu_count(), 8))
    
    # è®¾ç½®CUDAç¼“å­˜
    os.environ['CUDA_CACHE_DISABLE'] = '0'
    os.environ['CUDA_CACHE_PATH'] = os.path.expanduser('~/.nv/ComputeCache')

def preload_gpu():
    """é¢„åŠ è½½GPUï¼Œå‡å°‘åç»­åˆå§‹åŒ–æ—¶é—´"""
    try:
        import torch
        if torch.cuda.is_available():
            print("ğŸš€ æ­£åœ¨é¢„çƒ­GPU...")
            start_time = time.time()
            
            # æ¸…ç†GPUå†…å­˜
            torch.cuda.empty_cache()
            
            # ç®€å•çš„GPUé¢„çƒ­æ“ä½œ
            device = torch.device('cuda:0')
            x = torch.randn(1000, 1000, device=device)
            y = torch.matmul(x, x.t())
            del x, y
            torch.cuda.empty_cache()
            
            warm_time = time.time() - start_time
            gpu_name = torch.cuda.get_device_name(0)
            print(f"âœ“ GPUé¢„çƒ­å®Œæˆ ({gpu_name}, è€—æ—¶: {warm_time:.2f}s)")
            return True
    except Exception as e:
        print(f"âš ï¸ GPUé¢„çƒ­å¤±è´¥: {e}")
        return False
    return False

def setup_python_path():
    """è®¾ç½®Pythonè·¯å¾„ - ä¼˜åŒ–ç‰ˆ"""
    current_file = Path(__file__).resolve()
    
    if current_file.parent.name == 'src':
        project_root = current_file.parent.parent
    else:
        project_root = current_file.parent
    
    paths_to_add = [
        str(project_root),
        str(project_root / "src"),
    ]
    
    for path in paths_to_add:
        if path not in sys.path:
            sys.path.insert(0, path)
    
    os.chdir(project_root)
    return project_root

def import_flight_ranking_core():
    """å¯¼å…¥æ ¸å¿ƒæ¨¡å— - ä¼˜åŒ–ç‰ˆ"""
    print("ğŸ“¦ æ­£åœ¨å¯¼å…¥æ ¸å¿ƒæ¨¡å—...")
    import_start = time.time()
    
    try:
        from src.core.Core import FlightRankingCore
        import_time = time.time() - import_start
        print(f"âœ“ æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ (è€—æ—¶: {import_time:.2f}s)")
        return FlightRankingCore
    except ImportError as e1:
        try:
            from core.Core import FlightRankingCore
            import_time = time.time() - import_start
            print(f"âœ“ æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ (è€—æ—¶: {import_time:.2f}s)")
            return FlightRankingCore
        except ImportError as e2:
            print(f"âŒ å¯¼å…¥é”™è¯¯1: {e1}")
            print(f"âŒ å¯¼å…¥é”™è¯¯2: {e2}")
            raise ImportError("æ— æ³•å¯¼å…¥FlightRankingCoreæ¨¡å—")

def create_optimized_config():
    """åˆ›å»ºä¼˜åŒ–é…ç½®"""
    return {
        'paths': {
            'data_dir': "data/aeroclub-recsys-2025",
            'model_input_dir': "data/aeroclub-recsys-2025/processed",
            'model_save_dir': "data/aeroclub-recsys-2025/models",
            'output_dir': "data/aeroclub-recsys-2025/submissions",
            'log_dir': "logs"
        },
        'data_processing': {
            'chunk_size': 500000,  # å¢å¤§chunkæå‡æ•ˆç‡
            'n_processes': min(os.cpu_count(), 6),  # é™åˆ¶è¿›ç¨‹æ•°
            'force_reprocess': False
        },
        'training': {
            'segments': [0, 1, 2],
            'model_names': ['XGBRanker', 'LGBMRanker', 'LambdaMART'],
            'use_gpu': True,
            'random_state': 42,
            'use_full_data': False,
            'model_configs': {
                'XGBRanker': {
                    'n_estimators': 100,  # å‡å°‘æ ‘æ•°é‡åŠ å¿«è®­ç»ƒ
                    'max_depth': 6,       # å‡å°‘æ·±åº¦
                    'learning_rate': 0.1, # æé«˜å­¦ä¹ ç‡
                    'subsample': 0.8,
                    'colsample_bytree': 0.8,
                    'max_bin': 256        # GPUä¼˜åŒ–
                },
                'LGBMRanker': {
                    'n_estimators': 100,
                    'max_depth': 6,
                    'learning_rate': 0.1,
                    'subsample': 0.8,
                    'colsample_bytree': 0.8,
                    'max_bin': 255,       # GPUä¼˜åŒ–
                    'force_col_wise': True
                },
                'LambdaMART': {
                    'n_estimators': 100,
                    'max_depth': 6,
                    'learning_rate': 0.1,
                    'max_bin': 256
                }
            }
        },
        'prediction': {
            'segments': [0, 1, 2],
            'model_names': ['XGBRanker', 'LGBMRanker', 'LambdaMART'],
            'use_gpu': True,
        },
        'pipeline': {
            'run_data_processing': False,  # è·³è¿‡æ•°æ®å¤„ç†åŠ å¿«æµ‹è¯•
            'run_training': True,
            'run_prediction': True
        },
        'logging': {
            'level': "INFO",
            'format': "%(asctime)s | %(levelname)8s | %(name)s | %(message)s"
        }
    }

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶ - ä¼˜åŒ–ç‰ˆ"""
    try:
        import yaml
        config_path = Path("config/conf.yaml")
        
        if config_path.exists():
            print("ğŸ“‹ æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶...")
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # åº”ç”¨æ€§èƒ½ä¼˜åŒ–
            if 'training' in config and 'model_configs' in config['training']:
                # ä¼˜åŒ–XGBoostå‚æ•°
                if 'XGBRanker' in config['training']['model_configs']:
                    config['training']['model_configs']['XGBRanker'].update({
                        'max_bin': 256,
                        'verbosity': 0
                    })
                
                # ä¼˜åŒ–LightGBMå‚æ•°
                if 'LGBMRanker' in config['training']['model_configs']:
                    config['training']['model_configs']['LGBMRanker'].update({
                        'max_bin': 255,
                        'verbose': -1,
                        'force_col_wise': True
                    })
            
            print("âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            return config
        else:
            print("âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¼˜åŒ–é»˜è®¤é…ç½®")
            return create_optimized_config()
    except Exception as e:
        print(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        print("ä½¿ç”¨ä¼˜åŒ–é»˜è®¤é…ç½®")
        return create_optimized_config()

def check_dependencies():
    """æ£€æŸ¥ä¾èµ– - ä¼˜åŒ–ç‰ˆ"""
    print("ğŸ” æ­£åœ¨æ£€æŸ¥ä¾èµ–...")
    
    required = {
        'pandas': 'pandas',
        'numpy': 'numpy', 
        'sklearn': 'scikit-learn',
        'xgboost': 'xgboost',
        'lightgbm': 'lightgbm'
    }
    
    missing = []
    for pkg_name, install_name in required.items():
        try:
            __import__(pkg_name)
        except ImportError:
            missing.append(install_name)
    
    if missing:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {missing}")
        print(f"è¯·å®‰è£…: pip install {' '.join(missing)}")
        return False
    
    # æ£€æŸ¥GPUåº“
    try:
        import torch
        if torch.cuda.is_available():
            print(f"âœ“ GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
        else:
            print("âš ï¸ GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
    except ImportError:
        print("âš ï¸ PyTorchæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")
    
    print("âœ“ ä¾èµ–æ£€æŸ¥å®Œæˆ")
    return True

def show_system_info():
    """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
    import psutil
    
    print("\n" + "="*50)
    print("ğŸ–¥ï¸  ç³»ç»Ÿä¿¡æ¯")
    print("="*50)
    print(f"CPUæ ¸å¿ƒæ•°: {os.cpu_count()}")
    print(f"å†…å­˜: {psutil.virtual_memory().total // (1024**3)}GB")
    
    try:
        import torch
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory // (1024**3)}GB")
        else:
            print("GPU: ä¸å¯ç”¨")
    except:
        print("GPU: æ£€æŸ¥å¤±è´¥")
    print("="*50 + "\n")

def main():
    """ä¸»å‡½æ•° - ä¼˜åŒ–ç‰ˆ"""
    total_start = time.time()
    
    print("="*60)
    print("ğŸš€ èˆªç­æ’åç³»ç»Ÿå¯åŠ¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ")
    print("="*60)
    
    try:
        # 1. ç¯å¢ƒä¼˜åŒ–
        setup_environment()
        
        # 2. æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
        show_system_info()
        
        # 3. è·¯å¾„è®¾ç½®
        project_root = setup_python_path()
        print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
        
        # 4. é¢„çƒ­GPUï¼ˆå¹¶è¡Œè¿›è¡Œï¼‰
        gpu_ready = preload_gpu()
        
        # 5. æ£€æŸ¥ä¾èµ–
        if not check_dependencies():
            return 1
        
        # 6. å¯¼å…¥æ ¸å¿ƒæ¨¡å—
        FlightRankingCore = import_flight_ranking_core()
        
        # 7. åŠ è½½é…ç½®
        config = load_config()
        
        # 8. ç³»ç»Ÿåˆå§‹åŒ–
        print("âš™ï¸ æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿ...")
        init_start = time.time()
        core = FlightRankingCore(config)
        init_time = time.time() - init_start
        print(f"âœ“ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {init_time:.2f}s)")
        
        # 9. æ˜¾ç¤ºå¯åŠ¨ç»Ÿè®¡
        startup_time = time.time() - total_start
        print(f"\nğŸ¯ ç³»ç»Ÿå¯åŠ¨å®Œæˆ! æ€»è€—æ—¶: {startup_time:.2f}s")
        print(f"ğŸ’¡ ä¼˜åŒ–æ•ˆæœ: GPUé¢„çƒ­ {'âœ“' if gpu_ready else 'âœ—'}")
        
        # 10. è¿è¡Œæµæ°´çº¿
        print("\n" + "="*60)
        print("ğŸƒ å¼€å§‹æ‰§è¡Œæµæ°´çº¿...")
        print("="*60)
        
        pipeline_start = time.time()
        success = core.run_full_pipeline()
        pipeline_time = time.time() - pipeline_start
        
        if success:
            print("\n" + "="*60)
            print("âœ… èˆªç­æ’åç³»ç»Ÿæ‰§è¡ŒæˆåŠŸ!")
            print(f"ğŸ“Š æµæ°´çº¿è€—æ—¶: {pipeline_time:.2f}s")
            print(f"ğŸ¯ æ€»è€—æ—¶: {time.time() - total_start:.2f}s")
            print("="*60)
        else:
            print("\n" + "="*60)
            print("âŒ èˆªç­æ’åç³»ç»Ÿæ‰§è¡Œå¤±è´¥")
            print("="*60)
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        print("\nâ¸ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return 1
    except Exception as e:
        print(f"\nğŸ’¥ ç³»ç»Ÿè¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
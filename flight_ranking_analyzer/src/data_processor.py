import pandas as pd
import numpy as np
import pyarrow.parquet as pq
import gc
import os
from typing import Tuple, List, Optional, Dict, Any
from sklearn.model_selection import train_test_split
import warnings

# å¯¼å…¥è¿›åº¦æ¡å·¥å…·
try:
    from .progress_utils import create_data_loading_progress, progress_bar
except ImportError:
    from progress_utils import create_data_loading_progress, progress_bar

warnings.filterwarnings('ignore')


class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, logger=None):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            logger: æ—¥å¿—è®°å½•å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨printè¾“å‡º
        """
        self.logger = logger
        self.feature_columns = None
        self.exclude_columns = ['Id', 'selected', 'ranker_id', 'profileId', 'companyID']
    
    def _log(self, message):
        """è®°å½•æ—¥å¿—"""
        if self.logger:
            self.logger.info(message)
        else:
            print(message)
    
    def load_data(self, file_path: str, use_sampling: bool = True, 
                  num_groups: int = 2000, min_group_size: int = 20) -> pd.DataFrame:
        """
        åŠ è½½æ•°æ®ï¼Œæ”¯æŒæŠ½æ ·å’Œå…¨é‡åŠ è½½ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            use_sampling: æ˜¯å¦ä½¿ç”¨æŠ½æ ·
            num_groups: æŠ½æ ·æ—¶çš„ç»„æ•°é‡
            min_group_size: æŠ½æ ·æ—¶æ¯ç»„æœ€å°æ•°æ®æ¡æ•°
            
        Returns:
            pd.DataFrame: åŠ è½½çš„æ•°æ®
        """
        self._log(f"ğŸ“‚ å¼€å§‹åŠ è½½æ•°æ®: {os.path.basename(file_path)}")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½è¿›åº¦æ¡
        with create_data_loading_progress("è¯»å–æ–‡ä»¶") as pbar:
            # è¯»å–æ•°æ®
            pf = pq.ParquetFile(file_path)
            df = pf.read().to_pandas()
            pbar.update(1, "æ–‡ä»¶è¯»å–å®Œæˆ")
            
            self._log(f"ğŸ“Š åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
            
            if use_sampling:
                pbar.set_description("æ‰§è¡Œæ•°æ®æŠ½æ ·")
                self._log(f"ğŸ¯ ä½¿ç”¨æŠ½æ ·æ¨¡å¼: {num_groups}ä¸ªç»„, æ¯ç»„è‡³å°‘{min_group_size}æ¡æ•°æ®")
                df = self._sample_data(df, num_groups, min_group_size)
                pbar.update(1, "æŠ½æ ·å®Œæˆ")
            else:
                self._log("ğŸ“ˆ ä½¿ç”¨å…¨é‡æ•°æ®æ¨¡å¼")
                pbar.update(1, "è·³è¿‡æŠ½æ ·")
            
            # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
            pbar.set_description("ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
            df = self._optimize_memory_usage(df)
            pbar.update(1, "å†…å­˜ä¼˜åŒ–å®Œæˆ")
        
        self._log(f"âœ… æœ€ç»ˆæ•°æ®å½¢çŠ¶: {df.shape}")
        self._log(f"ğŸ“Š æ•°æ®åŒ…å«çš„ç»„æ•°: {df['ranker_id'].nunique()}")
        
        return df
    
    def _sample_data(self, df: pd.DataFrame, num_groups: int, min_group_size: int) -> pd.DataFrame:
        """
        åŸºäºranker_idçš„åˆ†ç»„æŠ½æ ·ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        
        Args:
            df: åŸå§‹æ•°æ®
            num_groups: è¦æŠ½å–çš„ç»„æ•°é‡
            min_group_size: æ¯ç»„æœ€å°æ•°æ®æ¡æ•°
            
        Returns:
            pd.DataFrame: æŠ½æ ·åçš„æ•°æ®
        """
        # ç»Ÿè®¡æ¯ä¸ªranker_idçš„æ•°æ®é‡
        self._log("ğŸ“Š ç»Ÿè®¡ç»„ä¿¡æ¯...")
        group_counts = df['ranker_id'].value_counts()
        
        # ç­›é€‰æ»¡è¶³æœ€å°ç»„å¤§å°è¦æ±‚çš„ranker_id
        valid_groups = group_counts[group_counts >= min_group_size].index
        self._log(f"ğŸ“‹ æ»¡è¶³æœ€å°ç»„å¤§å°({min_group_size})çš„ç»„æ•°: {len(valid_groups)}")
        
        if len(valid_groups) < num_groups:
            self._log(f"âš ï¸  å¯ç”¨ç»„æ•°({len(valid_groups)})å°‘äºè¦æ±‚çš„ç»„æ•°({num_groups})")
            num_groups = len(valid_groups)
        
        # éšæœºæŠ½å–æŒ‡å®šæ•°é‡çš„ranker_id
        self._log("ğŸ² éšæœºé€‰æ‹©ç»„...")
        np.random.seed(42)
        selected_groups = np.random.choice(valid_groups, size=num_groups, replace=False)
        
        # åŸºäºé€‰ä¸­çš„ranker_idç­›é€‰æ•°æ®
        self._log("ğŸ”„ ç­›é€‰æ•°æ®...")
        sampled_df = df[df['ranker_id'].isin(selected_groups)].copy()
        
        self._log(f"ğŸ“Š æŠ½æ ·åæ•°æ®å½¢çŠ¶: {sampled_df.shape}")
        self._log(f"ğŸ“‹ å®é™…æŠ½å–çš„ç»„æ•°: {sampled_df['ranker_id'].nunique()}")
        
        return sampled_df
    
    def _optimize_memory_usage(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        # å°†æ•°å€¼åˆ—è½¬æ¢ä¸ºfloat32ä»¥èŠ‚çœå†…å­˜
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].astype(np.float32)
        
        return df
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        å‡†å¤‡ç‰¹å¾æ•°æ®
        
        Args:
            df: åŸå§‹æ•°æ®
            
        Returns:
            Tuple: (ç‰¹å¾çŸ©é˜µ, æ ‡ç­¾å‘é‡, ç‰¹å¾ååˆ—è¡¨)
        """
        self._log("å‡†å¤‡ç‰¹å¾æ•°æ®...")
        
        # é€‰æ‹©æ•°å€¼å‹ç‰¹å¾
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # æ’é™¤ä¸éœ€è¦çš„ç‰¹å¾
        feature_cols = [col for col in numeric_cols if col not in self.exclude_columns]
        self.feature_columns = feature_cols
        
        # å¤„ç†ç¼ºå¤±å€¼
        df[feature_cols] = df[feature_cols].fillna(df[feature_cols].median())
        
        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X = df[feature_cols].values.astype(np.float32)
        y = df['selected'].values.astype(np.float32)
        
        self._log(f"ç‰¹å¾ç»´åº¦: {X.shape}")
        self._log(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        
        return X, y, feature_cols
    
    def prepare_ranking_data(self, df: pd.DataFrame, test_size: float = 0.2, 
                           random_state: int = 42) -> Tuple[np.ndarray, ...]:
        """
        å‡†å¤‡æ’åºæ¨¡å‹æ•°æ®ï¼ŒåŒ…æ‹¬è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†
        
        Args:
            df: åŸå§‹æ•°æ®
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
            
        Returns:
            Tuple: åŒ…å«è®­ç»ƒæµ‹è¯•æ•°æ®çš„å…ƒç»„
        """
        self._log("å‡†å¤‡æ’åºæ¨¡å‹æ•°æ®...")
        
        # å‡†å¤‡ç‰¹å¾
        X, y, feature_cols = self.prepare_features(df)
        groups = df['ranker_id'].values
        
        # æŒ‰ranker_idåˆ†ç»„åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        unique_groups = df['ranker_id'].unique()
        train_groups, test_groups = train_test_split(
            unique_groups, test_size=test_size, random_state=random_state
        )
        
        train_mask = df['ranker_id'].isin(train_groups)
        test_mask = df['ranker_id'].isin(test_groups)
        
        X_train = X[train_mask]
        X_test = X[test_mask]
        y_train = y[train_mask]
        y_test = y[test_mask]
        
        # è®¡ç®—ç»„å¤§å°
        train_group_sizes = self._calculate_group_sizes(groups[train_mask])
        test_group_sizes = self._calculate_group_sizes(groups[test_mask])
        
        # æµ‹è¯•é›†ä¿¡æ¯
        test_info = df[test_mask][['ranker_id', 'selected']].copy()
        
        self._log(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}")
        self._log(f"æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}")
        self._log(f"è®­ç»ƒç»„æ•°: {len(train_group_sizes)}")
        self._log(f"æµ‹è¯•ç»„æ•°: {len(test_group_sizes)}")
        
        return (X_train, X_test, y_train, y_test, 
                train_group_sizes, test_group_sizes, 
                feature_cols, test_info)
    
    def _calculate_group_sizes(self, groups: np.ndarray) -> List[int]:
        """è®¡ç®—æ¯ä¸ªç»„çš„å¤§å°"""
        group_sizes = []
        current_group = groups[0]
        current_size = 1
        
        for i in range(1, len(groups)):
            if groups[i] == current_group:
                current_size += 1
            else:
                group_sizes.append(current_size)
                current_group = groups[i]
                current_size = 1
        group_sizes.append(current_size)  # æ·»åŠ æœ€åä¸€ä¸ªç»„
        
        return group_sizes
    
    def load_test_data(self, file_path: str) -> pd.DataFrame:
        """
        åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆä¸æŠ½æ ·ï¼ŒåŠ è½½å…¨éƒ¨æ•°æ®ï¼‰
        
        Args:
            file_path: æµ‹è¯•æ–‡ä»¶è·¯å¾„
            
        Returns:
            pd.DataFrame: æµ‹è¯•æ•°æ®
        """
        self._log(f"åŠ è½½æµ‹è¯•æ•°æ®: {file_path}")
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è¯»å–æµ‹è¯•æ•°æ®
        pf = pq.ParquetFile(file_path)
        test_df = pf.read().to_pandas()
        
        # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        test_df = self._optimize_memory_usage(test_df)
        
        self._log(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_df.shape}")
        
        return test_df
    
    def prepare_test_features(self, test_df: pd.DataFrame) -> Tuple[np.ndarray, List[int]]:
        """
        å‡†å¤‡æµ‹è¯•æ•°æ®ç‰¹å¾
        
        Args:
            test_df: æµ‹è¯•æ•°æ®
            
        Returns:
            Tuple: (ç‰¹å¾çŸ©é˜µ, ç»„å¤§å°åˆ—è¡¨)
        """
        if self.feature_columns is None:
            raise ValueError("ç‰¹å¾åˆ—å°šæœªç¡®å®šï¼Œè¯·å…ˆè°ƒç”¨prepare_featuresæ–¹æ³•")
        
        # ç¡®ä¿æµ‹è¯•æ•°æ®åŒ…å«æ‰€éœ€ç‰¹å¾
        missing_features = set(self.feature_columns) - set(test_df.columns)
        if missing_features:
            self._log(f"è­¦å‘Š: æµ‹è¯•æ•°æ®ç¼ºå°‘ç‰¹å¾: {missing_features}")
            # ä¸ºç¼ºå¤±ç‰¹å¾æ·»åŠ 0å€¼
            for feature in missing_features:
                test_df[feature] = 0.0
        
        # å¤„ç†ç¼ºå¤±å€¼
        test_df[self.feature_columns] = test_df[self.feature_columns].fillna(
            test_df[self.feature_columns].median()
        )
        
        X_test = test_df[self.feature_columns].values.astype(np.float32)
        
        # è®¡ç®—ç»„å¤§å°
        groups = test_df['ranker_id'].values
        group_sizes = self._calculate_group_sizes(groups)
        
        return X_test, group_sizes
    
    def save_predictions(self, test_df: pd.DataFrame, predictions: Dict[str, Dict[str, np.ndarray]], 
                        output_path: str) -> str:
        """
        ä¿å­˜é¢„æµ‹ç»“æœ
        
        Args:
            test_df: æµ‹è¯•æ•°æ®
            predictions: é¢„æµ‹ç»“æœå­—å…¸
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # åˆ›å»ºç»“æœDataFrame
        result_df = test_df[['Id', 'ranker_id']].copy()
        
        # æ·»åŠ é¢„æµ‹ç»“æœ
        for model_type, pred in predictions.items():
            if 'scores' in pred:
                result_df[f'{model_type}_score'] = pred['scores']
            if 'ranks' in pred:
                result_df[f'{model_type}_rank'] = pred['ranks']
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        result_df.to_parquet(output_path, index=False)
        self._log(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        return output_path


class PredictionMerger:
    """é¢„æµ‹ç»“æœåˆå¹¶å™¨ - ç®€åŒ–æ’åå¤„ç†ç‰ˆæœ¬"""
    
    def __init__(self, logger=None):
        """
        åˆå§‹åŒ–é¢„æµ‹ç»“æœåˆå¹¶å™¨
        
        Args:
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.logger = logger
    
    def _log(self, message):
        """è®°å½•æ—¥å¿—"""
        if self.logger:
            self.logger.info(message)
        else:
            print(message)
    
    def merge_predictions(self, prediction_files: List[str], 
                         submission_file: str, 
                         output_file: str,
                         ensemble_method: str = 'average') -> str:
        """
        åˆå¹¶å¤šä¸ªé¢„æµ‹æ–‡ä»¶å¹¶ä¸submissionæ–‡ä»¶å¯¹åº”ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            prediction_files: é¢„æµ‹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            submission_file: submissionæ¨¡æ¿æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            ensemble_method: é›†æˆæ–¹æ³• ('average', 'voting', 'weighted')
            
        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        self._log("å¼€å§‹åˆå¹¶é¢„æµ‹ç»“æœ...")
        
        # è¯»å–submissionæ–‡ä»¶
        if not os.path.exists(submission_file):
            raise FileNotFoundError(f"Submissionæ–‡ä»¶ä¸å­˜åœ¨: {submission_file}")
        
        try:
            if submission_file.endswith('.parquet'):
                submission_df = pd.read_parquet(submission_file)
            else:
                submission_df = pd.read_csv(submission_file)
            
            self._log(f"Submissionæ–‡ä»¶å½¢çŠ¶: {submission_df.shape}")
            
        except Exception as e:
            raise ValueError(f"æ— æ³•è¯»å–submissionæ–‡ä»¶: {str(e)}")
        
        # è¯»å–æ‰€æœ‰é¢„æµ‹æ–‡ä»¶
        prediction_dfs = []
        for file_path in prediction_files:
            if os.path.exists(file_path):
                try:
                    if file_path.endswith('.parquet'):
                        pred_df = pd.read_parquet(file_path)
                    else:
                        pred_df = pd.read_csv(file_path)
                    prediction_dfs.append(pred_df)
                    self._log(f"è¯»å–é¢„æµ‹æ–‡ä»¶: {file_path}, å½¢çŠ¶: {pred_df.shape}")
                except Exception as e:
                    self._log(f"è¯»å–é¢„æµ‹æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
            else:
                self._log(f"è­¦å‘Š: é¢„æµ‹æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if not prediction_dfs:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é¢„æµ‹æ–‡ä»¶")
        
        # åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ
        merged_predictions = pd.concat(prediction_dfs, ignore_index=True)
        self._log(f"åˆå¹¶é¢„æµ‹ç»“æœå½¢çŠ¶: {merged_predictions.shape}")
        
        # ä¸submissionæ–‡ä»¶å¯¹åº”
        try:
            if 'Id' in submission_df.columns and 'ranker_id' in submission_df.columns:
                final_df = submission_df.merge(
                    merged_predictions, 
                    on=['Id', 'ranker_id'], 
                    how='left'
                )
            elif 'Id' in submission_df.columns:
                final_df = submission_df.merge(
                    merged_predictions, 
                    on=['Id'], 
                    how='left'
                )
            else:
                # å¦‚æœæ²¡æœ‰åˆé€‚çš„åŒ¹é…åˆ—ï¼Œç›´æ¥ä½¿ç”¨é¢„æµ‹ç»“æœ
                self._log("è­¦å‘Š: æ— æ³•ä¸submissionæ–‡ä»¶åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨é¢„æµ‹ç»“æœ")
                final_df = merged_predictions
        
        except Exception as e:
            self._log(f"åˆå¹¶æ•°æ®æ—¶å‡ºé”™: {str(e)}")
            self._log("ä½¿ç”¨é¢„æµ‹æ•°æ®ä½œä¸ºæœ€ç»ˆç»“æœ")
            final_df = merged_predictions
        
        # å¤„ç†é›†æˆé¢„æµ‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        final_df = self._ensemble_predictions_simplified(final_df, ensemble_method)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        try:
            if output_file.endswith('.parquet'):
                final_df.to_parquet(output_file, index=False)
            else:
                final_df.to_csv(output_file, index=False, encoding='utf-8')
            self._log(f"æœ€ç»ˆé¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            # å¦‚æœä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¿å­˜ä¸ºCSV
            csv_output = output_file.replace('.parquet', '.csv')
            final_df.to_csv(csv_output, index=False, encoding='utf-8')
            self._log(f"æœ€ç»ˆé¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {csv_output}")
            output_file = csv_output
        
        return output_file
    
    def _ensemble_predictions_simplified(self, df: pd.DataFrame, method: str) -> pd.DataFrame:
        """
        ç®€åŒ–ç‰ˆé›†æˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
        
        Args:
            df: åŒ…å«å¤šä¸ªæ¨¡å‹é¢„æµ‹çš„DataFrame
            method: é›†æˆæ–¹æ³•
            
        Returns:
            pd.DataFrame: é›†æˆåçš„ç»“æœ
        """
        self._log(f"ğŸ¯ æ‰§è¡Œç®€åŒ–é›†æˆé¢„æµ‹ï¼Œæ–¹æ³•: {method}")
        
        # æ‰¾åˆ°æ‰€æœ‰åˆ†æ•°åˆ—å’Œæ’ååˆ—
        score_columns = [col for col in df.columns if col.endswith('_score')]
        rank_columns = [col for col in df.columns if col.endswith('_rank')]
        
        self._log(f"æ‰¾åˆ°åˆ†æ•°åˆ—: {score_columns}")
        self._log(f"æ‰¾åˆ°æ’ååˆ—: {rank_columns}")
        
        if not score_columns and not rank_columns:
            self._log("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°é¢„æµ‹åˆ†æ•°æˆ–æ’ååˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºåˆ†æ•°")
            # å°è¯•æ‰¾åˆ°æ•°å€¼åˆ—ä½œä¸ºåˆ†æ•°
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                score_columns = [numeric_cols[0]]
            else:
                # åˆ›å»ºé»˜è®¤æ’å
                return self._create_default_rankings(df)
        
        try:
            if method == 'average' and score_columns:
                # å¹³å‡åˆ†æ•°æ–¹æ³•
                df = self._average_score_ensemble_simplified(df, score_columns)
                
            elif method == 'voting' and rank_columns:
                # æ’åæŠ•ç¥¨æ–¹æ³•  
                df = self._voting_rank_ensemble_simplified(df, rank_columns)
                
            elif method == 'weighted' and score_columns:
                # åŠ æƒå¹³å‡æ–¹æ³•
                df = self._weighted_average_ensemble_simplified(df, score_columns)
                
            else:
                # å›é€€åˆ°é»˜è®¤æ–¹æ³•
                self._log("ä½¿ç”¨é»˜è®¤é›†æˆæ–¹æ³•")
                df = self._create_default_rankings(df)
            
            self._log("âœ… ç®€åŒ–é›†æˆé¢„æµ‹å®Œæˆ")
            
        except Exception as e:
            self._log(f"âŒ é›†æˆé¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            # å¦‚æœé›†æˆå¤±è´¥ï¼Œåˆ›å»ºå®‰å…¨çš„å¤‡ç”¨æ’å
            df = self._create_default_rankings(df)
        
        return df

    def _average_score_ensemble_simplified(self, df: pd.DataFrame, score_columns: list) -> pd.DataFrame:
        """ç®€åŒ–ç‰ˆå¹³å‡åˆ†æ•°é›†æˆ"""
        self._log("æ‰§è¡Œç®€åŒ–å¹³å‡åˆ†æ•°é›†æˆ...")
        
        # æ¸…ç†åˆ†æ•°åˆ—
        for col in score_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        df['ensemble_score'] = df[score_columns].mean(axis=1, skipna=True)
        
        # ä½¿ç”¨ç®€åŒ–çš„æ’ååˆ†é…æ–¹æ³•
        if 'ranker_id' in df.columns:
            df = self._assign_unique_rankings_simplified(df)
        else:
            df['selected'] = 1
        
        return df

    def _voting_rank_ensemble_simplified(self, df: pd.DataFrame, rank_columns: list) -> pd.DataFrame:
        """ç®€åŒ–ç‰ˆæ’åæŠ•ç¥¨é›†æˆ"""
        self._log("æ‰§è¡Œç®€åŒ–æ’åæŠ•ç¥¨é›†æˆ...")
        
        # æ¸…ç†æ’ååˆ—
        for col in rank_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(1.0)
        
        # è®¡ç®—å¹³å‡æ’å
        df['ensemble_rank'] = df[rank_columns].mean(axis=1, skipna=True)
        
        # å°†å¹³å‡æ’åè½¬æ¢ä¸ºåˆ†æ•°ï¼ˆæ’åè¶Šå°ï¼Œåˆ†æ•°è¶Šé«˜ï¼‰
        df['ensemble_score'] = -df['ensemble_rank']
        
        # é‡æ–°åˆ†é…æ’å
        if 'ranker_id' in df.columns:
            df = self._assign_unique_rankings_simplified(df)
        else:
            df['selected'] = df['ensemble_rank'].round().clip(lower=1).astype(int)
        
        return df

    def _weighted_average_ensemble_simplified(self, df: pd.DataFrame, score_columns: list) -> pd.DataFrame:
        """ç®€åŒ–ç‰ˆåŠ æƒå¹³å‡é›†æˆ"""
        self._log("æ‰§è¡Œç®€åŒ–åŠ æƒå¹³å‡é›†æˆ...")
        
        # æ¸…ç†åˆ†æ•°åˆ—
        for col in score_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)
        
        # ç®€å•ç­‰æƒé‡
        weights = np.ones(len(score_columns)) / len(score_columns)
        
        # è®¡ç®—åŠ æƒå¹³å‡
        df['ensemble_score'] = df[score_columns].multiply(weights).sum(axis=1)
        
        # åˆ†é…æ’å
        if 'ranker_id' in df.columns:
            df = self._assign_unique_rankings_simplified(df)
        else:
            df['selected'] = 1
        
        return df

    def _assign_unique_rankings_simplified(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä½¿ç”¨ç®€åŒ–æ–¹æ³•åˆ†é…å”¯ä¸€æ’å
        
        Args:
            df: åŒ…å«ranker_idå’Œensemble_scoreçš„DataFrame
            
        Returns:
            pd.DataFrame: æ·»åŠ äº†selectedåˆ—çš„DataFrame
        """
        self._log("ğŸ¯ ä½¿ç”¨ç®€åŒ–æ–¹æ³•åˆ†é…å”¯ä¸€æ’å...")
        
        # ç¡®ä¿Idåˆ—å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        if 'Id' not in df.columns:
            df['Id'] = range(len(df))
        
        # ç¡®ä¿å”¯ä¸€æ’åï¼šä½¿ç”¨Idä½œä¸ºtie-breaker
        df = df.sort_values(['ranker_id', 'ensemble_score', 'Id'], 
                          ascending=[True, False, True])
        df['selected'] = df.groupby('ranker_id').cumcount() + 1
        
        self._log("âœ… æ’ååˆ†é…å®Œæˆ")
        return df

    def _create_default_rankings(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºé»˜è®¤æ’å"""
        self._log("åˆ›å»ºé»˜è®¤æ’å...")
        
        if 'ranker_id' in df.columns:
            # ç¡®ä¿Idåˆ—å­˜åœ¨
            if 'Id' not in df.columns:
                df['Id'] = range(len(df))
            
            # æŒ‰ranker_idåˆ†ç»„ï¼ŒæŒ‰Idæ’åºåˆ†é…æ’å
            df = df.sort_values(['ranker_id', 'Id'])
            df['selected'] = df.groupby('ranker_id').cumcount() + 1
        else:
            df['selected'] = 1
        
        return df
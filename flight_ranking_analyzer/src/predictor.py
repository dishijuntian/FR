"""
æ”¹è¿›çš„èˆªç­æ’åºé¢„æµ‹å™¨ - ç®€åŒ–æ’åå¤„ç†ç‰ˆæœ¬

åŸºäºæ›´ä¼˜ç§€çš„é¢„æµ‹ä»£ç ç»“æ„é‡æ–°è®¾è®¡ï¼Œæä¾›ï¼š
- ç®€åŒ–äº†æ’åå¤„ç†é€»è¾‘
- ç§»é™¤äº†å¤æ‚çš„åˆ†æ‰¹éªŒè¯å¤„ç†
- ä½¿ç”¨ç®€å•é«˜æ•ˆçš„æ’ååˆ†é…æ–¹æ³•

ä½œè€…: Flight Ranking Team
ç‰ˆæœ¬: 3.3 (ç®€åŒ–æ’åå¤„ç†)
"""

import pandas as pd
import numpy as np
import joblib
import os
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
import warnings
import gc
import psutil

# å¯¼å…¥åŸæœ‰æ¨¡å—
try:
    from .config import Config
    from .models import ModelFactory
    from .data_processor import DataProcessor
    from .progress_utils import progress_bar, create_data_loading_progress
except ImportError:
    from config import Config
    from models import ModelFactory
    from data_processor import DataProcessor
    from progress_utils import progress_bar, create_data_loading_progress

warnings.filterwarnings('ignore')


class FlightRankingPredictor:
    """æ”¹è¿›çš„èˆªç­æ’åºé¢„æµ‹å™¨ - ç®€åŒ–æ’åå¤„ç†ç‰ˆæœ¬"""
    
    def __init__(self, 
                 data_path: str = None,
                 model_save_path: str = "models", 
                 output_path: str = "submissions",
                 use_gpu: bool = False, 
                 random_state: int = 42,
                 logger=None):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            data_path: æ•°æ®æ ¹ç›®å½•è·¯å¾„
            model_save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„
            use_gpu: æ˜¯å¦ä½¿ç”¨GPU
            random_state: éšæœºç§å­
            logger: æ—¥å¿—è®°å½•å™¨
        """
        # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„è·¯å¾„æˆ–ç”¨æˆ·æŒ‡å®šè·¯å¾„
        self.data_path = Path(data_path) if data_path else Path(Config.DATA_BASE_PATH)
        self.model_save_path = self.data_path / model_save_path
        self.output_path = self.data_path / output_path
        self.use_gpu = use_gpu
        self.random_state = random_state
        self.logger = logger
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_path.mkdir(parents=True, exist_ok=True)
        self.model_save_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        self.data_processor = DataProcessor(logger=logger)
        
        # ç¼“å­˜å·²åŠ è½½çš„æ¨¡å‹å’Œç‰¹å¾
        self.loaded_models = {}
        self.loaded_features = {}
        
        self._log(f"é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        self._log(f"æ•°æ®è·¯å¾„: {self.data_path}")
        self._log(f"æ¨¡å‹è·¯å¾„: {self.model_save_path}")
        self._log(f"è¾“å‡ºè·¯å¾„: {self.output_path}")
    
    def _log(self, message: str):
        """è®°å½•æ—¥å¿—"""
        if self.logger:
            self.logger.info(message)
        else:
            print(message)
    
    def _monitor_memory(self, stage: str = ""):
        """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            memory_info = psutil.Process().memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
            self._log(f"ğŸ“Š {stage} å†…å­˜ä½¿ç”¨: {memory_mb:.1f} MB")
            return memory_mb
        except:
            return 0
    
    def _optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        gc.collect()
        self._monitor_memory("å†…å­˜æ¸…ç†å")
    
    def save_model_and_features(self, model, model_name: str, segment_id: int, 
                               feature_names: List[str], performance: float = None):
        """
        ä¿å­˜æ¨¡å‹å’Œç‰¹å¾ä¿¡æ¯
        
        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            model_name: æ¨¡å‹åç§°
            segment_id: æ•°æ®æ®µID
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            performance: æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
        """
        try:
            # ä¿å­˜æ¨¡å‹
            model_path = self.model_save_path / f"{model_name}_segment_{segment_id}.pkl"
            joblib.dump(model, model_path)
            
            # ä¿å­˜ç‰¹å¾åç§°
            feature_path = self.model_save_path / f"features_segment_{segment_id}.pkl"
            joblib.dump(feature_names, feature_path)
            
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯
            info_path = self.model_save_path / f"info_{model_name}_segment_{segment_id}.json"
            import json
            model_info = {
                'model_name': model_name,
                'segment_id': segment_id,
                'feature_count': len(feature_names),
                'performance': performance,
                'saved_at': pd.Timestamp.now().isoformat()
            }
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, indent=2, ensure_ascii=False)
            
            self._log(f"å·²ä¿å­˜æ¨¡å‹: {model_path}")
            self._log(f"å·²ä¿å­˜ç‰¹å¾: {feature_path}")
            
        except Exception as e:
            self._log(f"ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            raise
    
    def load_model_and_features(self, model_name: str, segment_id: int) -> Tuple[Any, List[str]]:
        """
        åŠ è½½æ¨¡å‹å’Œç‰¹å¾ä¿¡æ¯
        
        Args:
            model_name: æ¨¡å‹åç§°
            segment_id: æ•°æ®æ®µID
            
        Returns:
            Tuple: (æ¨¡å‹å¯¹è±¡, ç‰¹å¾åç§°åˆ—è¡¨)
        """
        cache_key = f"{model_name}_segment_{segment_id}"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.loaded_models:
            return self.loaded_models[cache_key], self.loaded_features[cache_key]
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_path = self.model_save_path / f"{model_name}_segment_{segment_id}.pkl"
        feature_path = self.model_save_path / f"features_segment_{segment_id}.pkl"
        
        if not model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        if not feature_path.exists():
            raise FileNotFoundError(f"ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {feature_path}")
        
        try:
            # åŠ è½½æ¨¡å‹å’Œç‰¹å¾
            model = joblib.load(model_path)
            feature_names = joblib.load(feature_path)
            
            # ç¼“å­˜åŠ è½½çš„å†…å®¹
            self.loaded_models[cache_key] = model
            self.loaded_features[cache_key] = feature_names
            
            self._log(f"å·²åŠ è½½æ¨¡å‹: {model_path}")
            return model, feature_names
            
        except Exception as e:
            self._log(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            raise
    
    def predict_segment(self, segment_id: int, model_name: str = 'XGBRanker') -> Optional[pd.DataFrame]:
        """
        é¢„æµ‹å•ä¸ªæ•°æ®æ®µï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            segment_id: æ•°æ®æ®µID
            model_name: æ¨¡å‹åç§°
            
        Returns:
            Optional[pd.DataFrame]: é¢„æµ‹ç»“æœ
        """
        self._log(f"å¼€å§‹é¢„æµ‹ segment_{segment_id}")
        self._monitor_memory("é¢„æµ‹å¼€å§‹å‰")
        
        try:
            # åŠ è½½æ¨¡å‹å’Œç‰¹å¾
            self._log(f"æ­£åœ¨åŠ è½½æ¨¡å‹å’Œç‰¹å¾...")
            model, feature_names = self.load_model_and_features(model_name, segment_id)
            self._monitor_memory("æ¨¡å‹åŠ è½½å")
            
            # æŸ¥æ‰¾æµ‹è¯•æ•°æ®æ–‡ä»¶
            self._log(f"æ­£åœ¨æŸ¥æ‰¾æµ‹è¯•æ•°æ®æ–‡ä»¶...")
            possible_test_files = [
                self.data_path / "segmented" / "test" / f"test_segment_{segment_id}.parquet",
                self.data_path / "encode" / "test" / f"test_segment_{segment_id}_encoded.parquet",
                self.data_path / "test" / f"test_segment_{segment_id}.parquet"
            ]
            
            test_file = None
            for file_path in possible_test_files:
                if file_path.exists():
                    test_file = file_path
                    break
            
            if test_file is None:
                raise FileNotFoundError(f"æ‰¾ä¸åˆ° segment_{segment_id} çš„æµ‹è¯•æ–‡ä»¶")
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            self._log(f"æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
            test_df = pd.read_parquet(test_file)
            self._log(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_df.shape}")
            self._monitor_memory("æµ‹è¯•æ•°æ®åŠ è½½å")
            
            # æ‰§è¡Œç®€åŒ–çš„é¢„æµ‹æµç¨‹
            return self._predict_segment_simplified(test_df, model, feature_names, segment_id)
                
        except Exception as e:
            self._log(f"é¢„æµ‹ segment_{segment_id} å¤±è´¥: {str(e)}")
            return None
        finally:
            # æ¸…ç†å†…å­˜
            self._optimize_memory()
    
    def _predict_segment_simplified(self, test_df: pd.DataFrame, model, 
                                  feature_names: List[str], segment_id: int) -> pd.DataFrame:
        """ç®€åŒ–ç‰ˆé¢„æµ‹æµç¨‹"""
        self._log(f"ğŸš€ æ‰§è¡Œç®€åŒ–ç‰ˆé¢„æµ‹æµç¨‹...")
        
        # å‡†å¤‡ç‰¹å¾æ•°æ®
        self._log(f"å‡†å¤‡ç‰¹å¾æ•°æ®...")
        X_test = self._prepare_test_features_simplified(test_df, feature_names)
        self._monitor_memory("ç‰¹å¾å‡†å¤‡å")
        
        # æ‰§è¡Œé¢„æµ‹
        self._log(f"æ‰§è¡Œæ¨¡å‹é¢„æµ‹...")
        pred_scores = model.predict(X_test)
        self._monitor_memory("æ¨¡å‹é¢„æµ‹å")
        
        # åˆ›å»ºç»“æœDataFrame
        self._log(f"åˆ›å»ºé¢„æµ‹ç»“æœ...")
        results = test_df[['Id', 'ranker_id']].copy()
        results['prediction_score'] = pred_scores
        
        # ä½¿ç”¨ç®€åŒ–çš„æ’ååˆ†é…æ–¹æ³•
        self._log(f"åˆ†é…å”¯ä¸€æ’å...")
        results = self._assign_unique_rankings(results)
        
        self._log(f"âœ… ç®€åŒ–é¢„æµ‹å®Œæˆï¼Œç»“æœå½¢çŠ¶: {results.shape}")
        
        # å¿«é€ŸéªŒè¯ï¼ˆä»…æŠ½æ ·ï¼‰
        self._quick_validation(results)
        
        return results[['Id', 'ranker_id', 'selected']]
    
    def _prepare_test_features_simplified(self, test_df: pd.DataFrame, 
                                        feature_names: List[str]) -> np.ndarray:
        """
        ç®€åŒ–ç‰ˆç‰¹å¾å‡†å¤‡
        
        Args:
            test_df: æµ‹è¯•æ•°æ®
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            
        Returns:
            np.ndarray: ç‰¹å¾çŸ©é˜µ
        """
        # ç¡®ä¿æµ‹è¯•æ•°æ®åŒ…å«æ‰€éœ€ç‰¹å¾
        missing_features = set(feature_names) - set(test_df.columns)
        if missing_features:
            self._log(f"è­¦å‘Š: æµ‹è¯•æ•°æ®ç¼ºå°‘ç‰¹å¾: {missing_features}")
            # ä¸ºç¼ºå¤±ç‰¹å¾æ·»åŠ 0å€¼
            for feature in missing_features:
                test_df[feature] = 0.0
        
        # å¤„ç†ç¼ºå¤±å€¼
        test_df[feature_names] = test_df[feature_names].fillna(
            test_df[feature_names].median()
        )
        
        return test_df[feature_names].values.astype(np.float32)
    
    def _assign_unique_rankings(self, results: pd.DataFrame) -> pd.DataFrame:
        """
        ä½¿ç”¨ç®€åŒ–æ–¹æ³•åˆ†é…å”¯ä¸€æ’å
        
        Args:
            results: åŒ…å«Id, ranker_id, prediction_scoreçš„DataFrame
            
        Returns:
            pd.DataFrame: æ·»åŠ äº†selectedåˆ—çš„DataFrame
        """
        self._log("ğŸ¯ ä½¿ç”¨ç®€åŒ–æ–¹æ³•åˆ†é…å”¯ä¸€æ’å...")
        
        # ç¡®ä¿å”¯ä¸€æ’åï¼šä½¿ç”¨Idä½œä¸ºtie-breaker
        results = results.sort_values(['ranker_id', 'prediction_score', 'Id'], 
                                    ascending=[True, False, True])
        results['selected'] = results.groupby('ranker_id').cumcount() + 1
        
        self._log("âœ… æ’ååˆ†é…å®Œæˆ")
        return results
    
    def _quick_validation(self, results: pd.DataFrame, sample_size: int = 10):
        """
        å¿«é€ŸéªŒè¯æ’åçš„æœ‰æ•ˆæ€§
        
        Args:
            results: é¢„æµ‹ç»“æœ
            sample_size: éªŒè¯æ ·æœ¬æ•°é‡
        """
        self._log("ğŸ” æ‰§è¡Œå¿«é€ŸéªŒè¯...")
        
        unique_rankers = results['ranker_id'].unique()
        sample_rankers = np.random.choice(
            unique_rankers, 
            min(sample_size, len(unique_rankers)), 
            replace=False
        )
        
        all_valid = True
        for ranker_id in sample_rankers:
            group_data = results[results['ranker_id'] == ranker_id]
            ranks = sorted(group_data['selected'].values)
            expected_ranks = list(range(1, len(group_data) + 1))
            
            if ranks != expected_ranks:
                self._log(f"âŒ ranker_id {ranker_id} æ’åæ— æ•ˆ: {ranks[:5]}...")
                all_valid = False
                break
        
        if all_valid:
            self._log(f"âœ… æŠ½æ ·éªŒè¯é€šè¿‡ ({sample_size}ä¸ªç»„)")
        else:
            self._log(f"âš ï¸ æŠ½æ ·éªŒè¯å‘ç°é—®é¢˜ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
    
    def predict_all(self, 
                   segments: List[int] = None, 
                   model_names: List[str] = None,
                   ensemble_method: str = 'average') -> Optional[pd.DataFrame]:
        """
        é¢„æµ‹æ‰€æœ‰æŒ‡å®šæ•°æ®æ®µå¹¶ç”Ÿæˆæœ€ç»ˆæäº¤æ–‡ä»¶ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            segments: è¦é¢„æµ‹çš„æ•°æ®æ®µåˆ—è¡¨
            model_names: è¦ä½¿ç”¨çš„æ¨¡å‹åç§°åˆ—è¡¨
            ensemble_method: é›†æˆæ–¹æ³•
            
        Returns:
            Optional[pd.DataFrame]: æœ€ç»ˆé¢„æµ‹ç»“æœ
        """
        if segments is None:
            segments = [0, 1, 2]  # é»˜è®¤é¢„æµ‹å‰3ä¸ªæ®µ
        if model_names is None:
            model_names = ['XGBRanker']  # é»˜è®¤ä½¿ç”¨XGBRanker
        
        self._log(f"å¼€å§‹é¢„æµ‹æ‰€æœ‰æ•°æ®æ®µ: {segments}")
        self._log(f"ä½¿ç”¨æ¨¡å‹: {model_names}")
        self._monitor_memory("é¢„æµ‹å¼€å§‹å‰")
        
        all_predictions = []
        
        # å¯¹æ¯ä¸ªæ•°æ®æ®µè¿›è¡Œé¢„æµ‹
        for segment_id in progress_bar(segments, desc="é¢„æµ‹æ•°æ®æ®µ"):
            self._log(f"\n{'='*50}")
            segment_predictions = {}
            
            # ä½¿ç”¨æ¯ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹
            for model_name in model_names:
                try:
                    self._log(f"ğŸ”„ ä½¿ç”¨{model_name}é¢„æµ‹segment_{segment_id}...")
                    prediction = self.predict_segment(segment_id, model_name)
                    if prediction is not None:
                        segment_predictions[model_name] = prediction
                        
                        # ä¿å­˜å•ä¸ªæ®µå•ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
                        segment_output = self.output_path / f"{model_name}_segment_{segment_id}_prediction.csv"
                        prediction.to_csv(segment_output, index=False)
                        self._log(f"å·²ä¿å­˜é¢„æµ‹ç»“æœ: {segment_output}")
                    
                except Exception as e:
                    self._log(f"æ¨¡å‹ {model_name} é¢„æµ‹ segment_{segment_id} å¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæœ‰å¤šä¸ªæ¨¡å‹ï¼Œè¿›è¡Œç®€åŒ–é›†æˆ
            if len(segment_predictions) > 1:
                ensemble_prediction = self._ensemble_predictions_simplified(
                    segment_predictions, ensemble_method
                )
                all_predictions.append(ensemble_prediction)
            elif len(segment_predictions) == 1:
                all_predictions.append(list(segment_predictions.values())[0])
            else:
                self._log(f"segment_{segment_id} æ²¡æœ‰æˆåŠŸçš„é¢„æµ‹ç»“æœ")
                continue
            
            # æ¸…ç†å½“å‰æ®µçš„å†…å­˜
            self._optimize_memory()
        
        # åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ
        if not all_predictions:
            self._log("æ²¡æœ‰æˆåŠŸçš„é¢„æµ‹ç»“æœ")
            return None
        
        self._log(f"ğŸ”— åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ...")
        self._monitor_memory("åˆå¹¶å‰")
        
        final_submission = pd.concat(all_predictions, ignore_index=True)
        
        # æ¸…ç†ä¸­é—´ç»“æœ
        del all_predictions
        self._optimize_memory()
        
        # æŒ‰Idæ’åº
        final_submission = final_submission.sort_values('Id').reset_index(drop=True)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        model_suffix = "_".join(model_names) if len(model_names) > 1 else model_names[0]
        final_output = self.output_path / f"{model_suffix}_final_submission.csv"
        final_submission.to_csv(final_output, index=False)
        
        # ç»“æœæ€»ç»“
        self._log(f"\n{'='*50}")
        self._log(f"é¢„æµ‹å®Œæˆ!")
        self._log(f"æœ€ç»ˆæäº¤æ–‡ä»¶: {final_output}")
        self._log(f"æ€»è®°å½•æ•°: {len(final_submission)}")
        self._log(f"å”¯ä¸€ranker_idæ•°é‡: {final_submission['ranker_id'].nunique()}")
        
        self._monitor_memory("æœ€ç»ˆå®Œæˆ")
        return final_submission
    
    def _ensemble_predictions_simplified(self, 
                                       predictions: Dict[str, pd.DataFrame],
                                       method: str = 'average') -> pd.DataFrame:
        """
        ç®€åŒ–ç‰ˆé›†æˆå¤šä¸ªæ¨¡å‹é¢„æµ‹
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœå­—å…¸
            method: é›†æˆæ–¹æ³•
            
        Returns:
            pd.DataFrame: é›†æˆåçš„é¢„æµ‹ç»“æœ
        """
        self._log(f"é›†æˆ {len(predictions)} ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œæ–¹æ³•: {method}")
        
        if len(predictions) == 1:
            return list(predictions.values())[0]
        
        # è·å–åŸºç¡€æ•°æ®æ¡†æ¶
        base_df = list(predictions.values())[0][['Id', 'ranker_id']].copy()
        
        # ç®€åŒ–çš„å¹³å‡é›†æˆ
        all_scores = []
        for model_name, pred_df in predictions.items():
            # åˆå¹¶è·å–åˆ†æ•°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            temp_df = base_df.merge(pred_df, on=['Id', 'ranker_id'], how='left')
            if 'prediction_score' in temp_df.columns:
                all_scores.append(temp_df['prediction_score'].values)
            else:
                # å¦‚æœæ²¡æœ‰åˆ†æ•°ï¼Œç”¨æ’åçš„å€’æ•°ä½œä¸ºåˆ†æ•°
                max_rank = temp_df.groupby('ranker_id')['selected'].transform('max')
                scores = max_rank - temp_df['selected'] + 1
                all_scores.append(scores.values)
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        if all_scores:
            avg_scores = np.mean(all_scores, axis=0)
            base_df['prediction_score'] = avg_scores
            
            # é‡æ–°åˆ†é…æ’å
            base_df = self._assign_unique_rankings(base_df)
        else:
            # å¦‚æœæ²¡æœ‰åˆ†æ•°ä¿¡æ¯ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„ç»“æœ
            first_result = list(predictions.values())[0]
            base_df = base_df.merge(first_result[['Id', 'ranker_id', 'selected']], 
                                  on=['Id', 'ranker_id'], how='left')
        
        return base_df[['Id', 'ranker_id', 'selected']]
    
    def validate_predictions(self, submission: pd.DataFrame, sample_size: int = 5):
        """
        éªŒè¯é¢„æµ‹ç»“æœçš„æœ‰æ•ˆæ€§
        
        Args:
            submission: æäº¤ç»“æœ
            sample_size: æŠ½æ ·éªŒè¯çš„æ•°é‡
        """
        self._log("\néªŒè¯é¢„æµ‹ç»“æœ:")
        
        unique_rankers = submission['ranker_id'].unique()
        total_groups = len(unique_rankers)
        valid_groups = 0
        
        # æ£€æŸ¥æ‰€æœ‰ç»„
        for ranker_id in unique_rankers:
            group_data = submission[submission['ranker_id'] == ranker_id]
            ranks = sorted(group_data['selected'].values)
            expected_ranks = list(range(1, len(group_data) + 1))
            if ranks == expected_ranks:
                valid_groups += 1
        
        self._log(f"æ€»ç»„æ•°: {total_groups}")
        self._log(f"æœ‰æ•ˆç»„æ•°: {valid_groups}")
        self._log(f"æœ‰æ•ˆç‡: {valid_groups/total_groups:.2%}")
        
        # éšæœºæŠ½æ ·è¯¦ç»†æ£€æŸ¥
        if sample_size > 0:
            sample_rankers = np.random.choice(
                unique_rankers, 
                min(sample_size, len(unique_rankers)), 
                replace=False
            )
            
            self._log(f"\næŠ½æ ·æ£€æŸ¥ {len(sample_rankers)} ä¸ªç»„:")
            for ranker_id in sample_rankers:
                group_data = submission[submission['ranker_id'] == ranker_id]
                ranks = sorted(group_data['selected'].values)
                expected_ranks = list(range(1, len(group_data) + 1))
                is_valid = ranks == expected_ranks
                self._log(f"  ranker_id {ranker_id}: æ’å{'æœ‰æ•ˆ' if is_valid else 'æ— æ•ˆ'} "
                         f"(å¤§å°: {len(group_data)})")
                if not is_valid:
                    self._log(f"    å®é™…æ’å: {ranks[:10]}...")
                    self._log(f"    æœŸæœ›æ’å: {expected_ranks[:10]}...")
    
    def get_available_models(self) -> Dict[str, List[int]]:
        """
        è·å–å¯ç”¨çš„æ¨¡å‹å’Œå¯¹åº”çš„æ•°æ®æ®µ
        
        Returns:
            Dict: {æ¨¡å‹åç§°: [å¯ç”¨çš„æ®µIDåˆ—è¡¨]}
        """
        available_models = {}
        
        if not self.model_save_path.exists():
            return available_models
        
        # æ‰«ææ¨¡å‹æ–‡ä»¶
        for model_file in self.model_save_path.glob("*.pkl"):
            if model_file.name.startswith("features_"):
                continue
                
            # è§£ææ–‡ä»¶å: ModelName_segment_X.pkl
            name_parts = model_file.stem.split("_")
            if len(name_parts) >= 3 and name_parts[-2] == "segment":
                model_name = "_".join(name_parts[:-2])
                try:
                    segment_id = int(name_parts[-1])
                    if model_name not in available_models:
                        available_models[model_name] = []
                    available_models[model_name].append(segment_id)
                except ValueError:
                    continue
        
        # æ’åºæ®µID
        for model_name in available_models:
            available_models[model_name].sort()
        
        return available_models
    
    def print_model_summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦ä¿¡æ¯"""
        available_models = self.get_available_models()
        
        if not available_models:
            self._log("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹")
            return
        
        self._log("\nå¯ç”¨æ¨¡å‹æ‘˜è¦:")
        self._log("="*50)
        
        for model_name, segments in available_models.items():
            self._log(f"{model_name}: æ®µ {segments}")
            
            # å°è¯•è¯»å–æ¨¡å‹ä¿¡æ¯
            for segment_id in segments[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæ®µçš„è¯¦ç»†ä¿¡æ¯
                info_path = self.model_save_path / f"info_{model_name}_segment_{segment_id}.json"
                if info_path.exists():
                    try:
                        import json
                        with open(info_path, 'r', encoding='utf-8') as f:
                            info = json.load(f)
                        self._log(f"  æ®µ{segment_id}: ç‰¹å¾æ•°={info.get('feature_count', 'N/A')}, "
                                 f"æ€§èƒ½={info.get('performance', 'N/A'):.4f}")
                    except:
                        pass
"""
æ’åºæ¨¡å‹å®šä¹‰æ–‡ä»¶ - ä¿®å¤ç‰ˆæœ¬

è¯¥æ¨¡å—åŒ…å«æ‰€æœ‰æ’åºæ¨¡å‹çš„å®šä¹‰å’Œå®ç°
- ä¿®å¤äº†RankNetå’ŒTransformerRankerä¸­validation_splitçš„é—®é¢˜
- æ”¹è¿›äº†å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒç¨³å®šæ€§

ä½œè€…: Flight Ranking Team
ç‰ˆæœ¬: 2.3 (ä¿®å¤ç‰ˆ)
"""
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from xgboost import XGBRanker
from lightgbm import LGBMRanker
from rank_bm25 import BM25Okapi
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Union, Any
import warnings

warnings.filterwarnings('ignore')


class BaseRanker(ABC):
    """æ’åºæ¨¡å‹åŸºç±»"""
    
    @abstractmethod
    def fit(self, X, y, group, **kwargs):
        """è®­ç»ƒæ¨¡å‹"""
        pass
    
    @abstractmethod
    def predict(self, X):
        """é¢„æµ‹åˆ†æ•°"""
        pass
    
    @abstractmethod
    def get_model_name(self):
        """è·å–æ¨¡å‹åç§°"""
        pass
    
    def get_params(self):
        """è·å–æ¨¡å‹å‚æ•°"""
        return getattr(self, 'params', {})
    
    def set_params(self, **params):
        """è®¾ç½®æ¨¡å‹å‚æ•°"""
        self.params = params
        return self


class XGBRankerModel(BaseRanker):
    """XGBoostæ’åºæ¨¡å‹"""
    
    def __init__(self, use_gpu=True, **params):
        default_params = {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'eval_metric': 'ndcg'
        }
        default_params.update(params)
        
        if use_gpu:
            default_params.update({
                'tree_method': 'gpu_hist',
                'predictor': 'gpu_predictor'
            })
        else:
            default_params['n_jobs'] = -1
        
        self.params = default_params
        self.model = XGBRanker(**default_params)
        
    def fit(self, X, y, group, **kwargs):
        self.model.fit(X, y, group=group, **kwargs)
    
    def predict(self, X):
        return self.model.predict(X)
    
    def get_model_name(self):
        return "XGBRanker"
    
    @property
    def feature_importances_(self):
        return self.model.feature_importances_


class LGBMRankerModel(BaseRanker):
    """LightGBMæ’åºæ¨¡å‹"""
    
    def __init__(self, use_gpu=True, **params):
        default_params = {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'objective': 'lambdarank',
            'metric': 'ndcg'
        }
        default_params.update(params)
        
        if use_gpu:
            default_params['device'] = 'gpu'
        else:
            default_params['n_jobs'] = -1
        
        self.params = default_params
        self.model = LGBMRanker(**default_params)
        
    def fit(self, X, y, group, **kwargs):
        self.model.fit(X, y, group=group, **kwargs)
    
    def predict(self, X):
        return self.model.predict(X)
    
    def get_model_name(self):
        return "LGBMRanker"
    
    @property
    def feature_importances_(self):
        return self.model.feature_importances_


class LambdaMART(BaseRanker):
    """LambdaMARTå®ç°ï¼ˆåŸºäºXGBoostï¼‰"""
    
    def __init__(self, use_gpu=True, **params):
        default_params = {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42
        }
        default_params.update(params)
        
        xgb_params = {
            'objective': "rank:pairwise",
            **default_params
        }
        
        if use_gpu:
            xgb_params.update({
                'tree_method': 'gpu_hist',
                'predictor': 'gpu_predictor'
            })
        else:
            xgb_params['n_jobs'] = -1
        
        self.params = default_params
        self.model = XGBRanker(**xgb_params)
        
    def fit(self, X, y, group, **kwargs):
        self.model.fit(X, y, group=group, **kwargs)
    
    def predict(self, X):
        return self.model.predict(X)
    
    def get_model_name(self):
        return "LambdaMART"
    
    @property
    def feature_importances_(self):
        return self.model.feature_importances_


class ListNetModel(BaseRanker):
    """ListNetå®ç°ï¼ˆåŸºäºLightGBMï¼‰"""
    
    def __init__(self, use_gpu=True, **params):
        default_params = {
            'n_estimators': 100,
            'learning_rate': 0.05,
            'max_depth': 7,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42
        }
        default_params.update(params)
        
        lgb_params = {
            'objective': "lambdarank",
            'metric': "ndcg",
            **default_params
        }
        
        if use_gpu:
            lgb_params['device'] = 'gpu'
        else:
            lgb_params['n_jobs'] = -1
        
        self.params = default_params
        self.model = LGBMRanker(**lgb_params)
        
    def fit(self, X, y, group, **kwargs):
        self.model.fit(X, y, group=group, **kwargs)
    
    def predict(self, X):
        return self.model.predict(X)
    
    def get_model_name(self):
        return "ListNet"
    
    @property
    def feature_importances_(self):
        return self.model.feature_importances_


class NeuralRanker(BaseRanker):
    """ç¥ç»ç½‘ç»œæ’åºæ¨¡å‹"""
    
    def __init__(self, input_dim, **params):
        default_params = {
            'hidden_units': [256, 128, 64],
            'learning_rate': 0.001,
            'epochs': 10,
            'batch_size': 32
        }
        default_params.update(params)
        
        self.input_dim = input_dim
        self.params = default_params
        self.model = self._build_model()
        
    def _build_model(self):
        inputs = keras.Input(shape=(self.input_dim,))
        
        x = layers.Dense(self.params['hidden_units'][0], activation='relu')(inputs)
        x = layers.Dropout(0.2)(x)
        
        for units in self.params['hidden_units'][1:]:
            x = layers.Dense(units, activation='relu')(x)
            x = layers.Dropout(0.2)(x)
        
        outputs = layers.Dense(1)(x)
        
        model = keras.Model(inputs=inputs, outputs=outputs)
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=self.params['learning_rate']),
            loss='mse'
        )
        return model
    
    def fit(self, X, y, group, **kwargs):
        epochs = kwargs.get('epochs', self.params['epochs'])
        batch_size = kwargs.get('batch_size', self.params['batch_size'])
        
        # æ‰‹åŠ¨åˆ†å‰²éªŒè¯é›†ï¼ˆå¦‚æœæ•°æ®é‡è¶³å¤Ÿå¤§ï¼‰
        if len(X) > 1000:
            val_size = int(len(X) * 0.2)
            train_size = len(X) - val_size
            
            X_train_split = X[:train_size].astype(np.float32)
            y_train_split = y[:train_size].astype(np.float32)
            X_val_split = X[train_size:].astype(np.float32)
            y_val_split = y[train_size:].astype(np.float32)
            
            # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦éªŒè¯ï¼‰
            self.model.fit(
                X_train_split, y_train_split,
                validation_data=(X_val_split, y_val_split),
                epochs=epochs,
                batch_size=batch_size,
                verbose=1
            )
        else:
            # è®­ç»ƒæ¨¡å‹ï¼ˆæ— éªŒè¯ï¼‰
            self.model.fit(
                X.astype(np.float32), y.astype(np.float32),
                epochs=epochs,
                batch_size=batch_size,
                verbose=1
            )
    
    def predict(self, X):
        return self.model.predict(X.astype(np.float32)).flatten()
    
    def get_model_name(self):
        return "NeuralRanker"


class RankNet(BaseRanker):
    """RankNetæ’åºæ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬
    
    RankNetæ˜¯ä¸€ä¸ªåŸºäºç¥ç»ç½‘ç»œçš„pairwise rankingæ–¹æ³•ï¼Œ
    é€šè¿‡å­¦ä¹ æ–‡æ¡£å¯¹ä¹‹é—´çš„ç›¸å¯¹æ’åºå…³ç³»æ¥è®­ç»ƒæ¨¡å‹ã€‚
    """
    
    def __init__(self, input_dim, **params):
        default_params = {
            'hidden_units': [128, 64, 32],
            'learning_rate': 0.001,
            'epochs': 15,
            'batch_size': 64,
            'dropout_rate': 0.3
        }
        default_params.update(params)
        
        self.input_dim = input_dim
        self.params = default_params
        self.model = self._build_model()
        self._feature_importance = None
        
    def _build_model(self):
        """æ„å»ºRankNetæ¨¡å‹"""
        inputs = keras.Input(shape=(self.input_dim,))
        
        # ç‰¹å¾å±‚
        x = layers.Dense(self.params['hidden_units'][0], activation='relu', name='dense_1')(inputs)
        x = layers.Dropout(self.params['dropout_rate'])(x)
        x = layers.BatchNormalization()(x)
        
        for i, units in enumerate(self.params['hidden_units'][1:], 2):
            x = layers.Dense(units, activation='relu', name=f'dense_{i}')(x)
            x = layers.Dropout(self.params['dropout_rate'])(x)
            x = layers.BatchNormalization()(x)
        
        # è¾“å‡ºå±‚ - è¾“å‡ºå•ä¸ªåˆ†æ•°
        outputs = layers.Dense(1, activation='linear', name='score_output')(x)
        
        model = keras.Model(inputs=inputs, outputs=outputs, name='RankNet')
        
        # ä½¿ç”¨MSEæŸå¤±å‡½æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=self.params['learning_rate']),
            loss='mse',
            metrics=['mae']
        )
        
        return model
    
    def fit(self, X, y, group, **kwargs):
        """è®­ç»ƒRankNetæ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬"""
        epochs = kwargs.get('epochs', self.params['epochs'])
        batch_size = kwargs.get('batch_size', self.params['batch_size'])
        
        print(f"å¼€å§‹è®­ç»ƒRankNetæ¨¡å‹ï¼Œæ•°æ®å½¢çŠ¶: {X.shape}")
        
        # ä¿®å¤ï¼šæ‰‹åŠ¨å¤„ç†éªŒè¯é›†åˆ†å‰²
        if len(X) > 1000:
            # è®¡ç®—éªŒè¯é›†å¤§å°
            val_size = int(len(X) * 0.2)
            train_size = len(X) - val_size
            
            # åˆ†å‰²æ•°æ®
            X_train_split = X[:train_size].astype(np.float32)
            y_train_split = y[:train_size].astype(np.float32)
            X_val_split = X[train_size:].astype(np.float32)
            y_val_split = y[train_size:].astype(np.float32)
            
            # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦éªŒè¯ï¼‰
            history = self.model.fit(
                X_train_split, y_train_split,
                validation_data=(X_val_split, y_val_split),
                epochs=epochs,
                batch_size=batch_size,
                verbose=1
            )
        else:
            # è®­ç»ƒæ¨¡å‹ï¼ˆæ— éªŒè¯ï¼‰
            history = self.model.fit(
                X.astype(np.float32), y.astype(np.float32),
                epochs=epochs,
                batch_size=batch_size,
                verbose=1
            )
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§
        self._compute_feature_importance(X[:min(1000, len(X))])
        
        print("RankNetæ¨¡å‹è®­ç»ƒå®Œæˆ")
        return history
    
    def _compute_feature_importance(self, X_sample):
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§"""
        try:
            X_tensor = tf.convert_to_tensor(X_sample.astype(np.float32))
            
            with tf.GradientTape() as tape:
                tape.watch(X_tensor)
                predictions = self.model(X_tensor)
            
            grads = tape.gradient(predictions, X_tensor)
            if grads is not None:
                # ä½¿ç”¨æ¢¯åº¦çš„ç»å¯¹å€¼å‡å€¼ä½œä¸ºç‰¹å¾é‡è¦æ€§
                importance = np.mean(np.abs(grads.numpy()), axis=0)
                # å½’ä¸€åŒ–
                self._feature_importance = importance / (np.sum(importance) + 1e-8)
            else:
                # å¦‚æœæ— æ³•è®¡ç®—æ¢¯åº¦ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
                self._feature_importance = np.ones(self.input_dim) / self.input_dim
                
        except Exception as e:
            print(f"è®¡ç®—RankNetç‰¹å¾é‡è¦æ€§æ—¶å‡ºé”™: {e}")
            # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºåå¤‡æ–¹æ¡ˆ
            self._feature_importance = np.ones(self.input_dim) / self.input_dim
    
    def predict(self, X):
        """é¢„æµ‹åˆ†æ•°"""
        return self.model.predict(X.astype(np.float32), verbose=0).flatten()
    
    def get_model_name(self):
        return "RankNet"
    
    @property
    def feature_importances_(self):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self._feature_importance is None:
            # å¦‚æœè¿˜æ²¡æœ‰è®¡ç®—ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒ
            return np.ones(self.input_dim) / self.input_dim
        return self._feature_importance


class TransformerRanker(BaseRanker):
    """ä¿®å¤ç‰ˆTransformerRanker - ç¨³å®šå¯ç”¨çš„å®ç°"""
    
    def __init__(self, input_dim, **params):
        # ä½¿ç”¨æ›´ä¿å®ˆçš„é»˜è®¤å‚æ•°
        default_params = {
            'num_heads': 4,           # å‡å°‘æ³¨æ„åŠ›å¤´æ•°
            'num_layers': 2,          # å‡å°‘å±‚æ•°
            'd_model': 64,            # å‡å°‘æ¨¡å‹ç»´åº¦
            'dff': 128,               # å‡å°‘å‰é¦ˆç½‘ç»œç»´åº¦
            'learning_rate': 0.001,
            'epochs': 10,
            'batch_size': 64,
            'dropout_rate': 0.1,
            'max_seq_length': 16      # å›ºå®šåºåˆ—é•¿åº¦
        }
        default_params.update(params)
        
        self.input_dim = input_dim
        self.params = default_params
        self._feature_importance = None
        
        print(f"ğŸ”§ åˆå§‹åŒ–TransformerRanker:")
        print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
        print(f"   åºåˆ—é•¿åº¦: {self.params['max_seq_length']}")
        print(f"   æ¨¡å‹ç»´åº¦: {self.params['d_model']}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {self.params['num_heads']}")
        
        try:
            self.model = self._build_model()
            print("âœ… TransformerRankeræ¨¡å‹æ„å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ TransformerRankeræ„å»ºå¤±è´¥: {e}")
            raise
        
    def _build_model(self):
        """æ„å»ºç¨³å®šçš„Transformeræ¨¡å‹"""
        print("ğŸ”¨ æ„å»ºTransformeræ¨¡å‹...")
        
        # è¾“å…¥å±‚
        inputs = keras.Input(shape=(self.input_dim,), name='input_features')
        
        # ç‰¹å¾é¢„å¤„ç†å’Œç»´åº¦è°ƒæ•´
        seq_len = self.params['max_seq_length']
        d_model = self.params['d_model']
        
        # æ–¹æ³•1: å¦‚æœè¾“å…¥ç»´åº¦å¯ä»¥æ•´é™¤åºåˆ—é•¿åº¦
        if self.input_dim % seq_len == 0:
            features_per_token = self.input_dim // seq_len
            x = layers.Reshape((seq_len, features_per_token), name='reshape_input')(inputs)
            
            # æŠ•å½±åˆ°d_modelç»´åº¦
            if features_per_token != d_model:
                x = layers.Dense(d_model, activation='relu', name='feature_projection')(x)
        else:
            # æ–¹æ³•2: ä½¿ç”¨çº¿æ€§å˜æ¢
            # å…ˆæŠ•å½±åˆ°å¯ä»¥æ•´é™¤çš„ç»´åº¦
            target_dim = seq_len * d_model
            x = layers.Dense(target_dim, activation='relu', name='linear_projection')(inputs)
            x = layers.Reshape((seq_len, d_model), name='reshape_projected')(x)
        
        print(f"âœ“ ç‰¹å¾é‡å¡‘å®Œæˆ: (batch, {seq_len}, {d_model})")
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = self._add_positional_encoding(x, seq_len, d_model)
        print("âœ“ ä½ç½®ç¼–ç å®Œæˆ")
        
        # Transformerå±‚
        for i in range(self.params['num_layers']):
            x = self._transformer_block(x, i)
            print(f"âœ“ Transformerå±‚ {i+1} å®Œæˆ")
        
        # å…¨å±€ç‰¹å¾æå–
        # ä½¿ç”¨å¤šç§æ± åŒ–æ–¹å¼
        avg_pool = layers.GlobalAveragePooling1D(name='global_avg_pool')(x)
        max_pool = layers.GlobalMaxPooling1D(name='global_max_pool')(x)
        
        # åˆå¹¶ä¸åŒçš„æ± åŒ–ç»“æœ
        pooled = layers.Concatenate(name='concat_pools')([avg_pool, max_pool])
        
        # åˆ†ç±»å¤´
        x = layers.Dense(128, activation='relu', name='dense_1')(pooled)
        x = layers.Dropout(self.params['dropout_rate'], name='dropout_1')(x)
        
        x = layers.Dense(64, activation='relu', name='dense_2')(x)
        x = layers.Dropout(self.params['dropout_rate'], name='dropout_2')(x)
        
        # è¾“å‡ºå±‚
        outputs = layers.Dense(1, activation='linear', name='ranking_output')(x)
        
        # åˆ›å»ºæ¨¡å‹
        model = keras.Model(inputs=inputs, outputs=outputs, name='TransformerRanker')
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=keras.optimizers.Adam(
                learning_rate=self.params['learning_rate'],
                clipnorm=1.0  # æ¢¯åº¦è£å‰ªï¼Œæé«˜ç¨³å®šæ€§
            ),
            loss='mse',
            metrics=['mae']
        )
        
        print("âœ“ æ¨¡å‹ç¼–è¯‘å®Œæˆ")
        return model
    
    def _add_positional_encoding(self, x, seq_len, d_model):
        """æ·»åŠ ç¨³å®šçš„ä½ç½®ç¼–ç """
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        positions = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]
        
        # ä½¿ç”¨ç®€å•ä½†æœ‰æ•ˆçš„ä½ç½®ç¼–ç 
        div_term = tf.pow(10000.0, tf.range(0, d_model, 2, dtype=tf.float32) / d_model)
        
        # è®¡ç®—sinå’Œcos
        sin_vals = tf.sin(positions / div_term)
        cos_vals = tf.cos(positions / div_term)
        
        # äº¤æ›¿æ’åˆ—sinå’Œcoså€¼
        if d_model % 2 == 0:
            # å¶æ•°ç»´åº¦
            pe_sin = tf.reshape(sin_vals, [seq_len, d_model // 2])
            pe_cos = tf.reshape(cos_vals, [seq_len, d_model // 2])
            
            # äº¤æ›¿æ’åˆ—
            pos_encoding = tf.stack([pe_sin, pe_cos], axis=2)
            pos_encoding = tf.reshape(pos_encoding, [seq_len, d_model])
        else:
            # å¥‡æ•°ç»´åº¦ï¼Œæœ€åä¸€ä¸ªä½¿ç”¨sin
            pe_sin = tf.reshape(sin_vals, [seq_len, d_model // 2])
            pe_cos = tf.reshape(cos_vals[:, :d_model//2], [seq_len, d_model // 2])
            
            pos_encoding = tf.stack([pe_sin, pe_cos], axis=2)
            pos_encoding = tf.reshape(pos_encoding, [seq_len, d_model - 1])
            
            # æ·»åŠ æœ€åä¸€ä¸ªç»´åº¦
            last_dim = tf.sin(positions[:, 0:1] / 10000.0)
            pos_encoding = tf.concat([pos_encoding, last_dim], axis=1)
        
        # æ‰©å±•batchç»´åº¦å¹¶æ·»åŠ åˆ°è¾“å…¥
        pos_encoding = pos_encoding[tf.newaxis, :, :]  # (1, seq_len, d_model)
        
        return x + pos_encoding
    
    def _transformer_block(self, x, block_idx):
        """ç¨³å®šçš„Transformerå—å®ç°"""
        # å¤šå¤´è‡ªæ³¨æ„åŠ›
        attention = layers.MultiHeadAttention(
            num_heads=self.params['num_heads'],
            key_dim=self.params['d_model'] // self.params['num_heads'],
            dropout=self.params['dropout_rate'],
            name=f'attention_{block_idx}'
        )
        
        # æ³¨æ„åŠ›è®¡ç®—
        attn_output = attention(x, x)
        
        # ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        x1 = layers.Add(name=f'add_1_{block_idx}')([x, attn_output])
        x1 = layers.LayerNormalization(
            epsilon=1e-6, 
            name=f'layer_norm_1_{block_idx}'
        )(x1)
        
        # å‰é¦ˆç½‘ç»œ
        ffn_output = layers.Dense(
            self.params['dff'], 
            activation='relu',
            name=f'ffn_1_{block_idx}'
        )(x1)
        ffn_output = layers.Dropout(
            self.params['dropout_rate'],
            name=f'ffn_dropout_{block_idx}'
        )(ffn_output)
        ffn_output = layers.Dense(
            self.params['d_model'],
            name=f'ffn_2_{block_idx}'
        )(ffn_output)
        
        # ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        x2 = layers.Add(name=f'add_2_{block_idx}')([x1, ffn_output])
        x2 = layers.LayerNormalization(
            epsilon=1e-6,
            name=f'layer_norm_2_{block_idx}'
        )(x2)
        
        return x2
    
    def fit(self, X, y, group, **kwargs):
        """è®­ç»ƒæ¨¡å‹ - å¸¦å®Œæ•´é”™è¯¯å¤„ç†"""
        epochs = kwargs.get('epochs', self.params['epochs'])
        batch_size = kwargs.get('batch_size', self.params['batch_size'])
        
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒTransformerRanker")
        print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {X.shape}")
        print(f"ğŸ“Š è®­ç»ƒå‚æ•°: epochs={epochs}, batch_size={batch_size}")
        
        try:
            # æ•°æ®é¢„å¤„ç†
            X_processed = X.astype(np.float32)
            y_processed = y.astype(np.float32)
            
            # æ£€æŸ¥æ•°æ®
            if np.any(np.isnan(X_processed)) or np.any(np.isinf(X_processed)):
                print("âš ï¸ å‘ç°NaNæˆ–Infå€¼ï¼Œè¿›è¡Œæ¸…ç†...")
                X_processed = np.nan_to_num(X_processed, nan=0.0, posinf=1.0, neginf=-1.0)
            
            if np.any(np.isnan(y_processed)) or np.any(np.isinf(y_processed)):
                print("âš ï¸ æ ‡ç­¾ä¸­å‘ç°NaNæˆ–Infå€¼ï¼Œè¿›è¡Œæ¸…ç†...")
                y_processed = np.nan_to_num(y_processed, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # æ•°æ®æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_processed = scaler.fit_transform(X_processed).astype(np.float32)
            
            print("âœ“ æ•°æ®é¢„å¤„ç†å®Œæˆ")
            
            # æ™ºèƒ½ç¡®å®šéªŒè¯é›†å¤§å°
            total_samples = len(X_processed)
            if total_samples > 100000:
                val_ratio = 0.02  # å¤§æ•°æ®é›†ç”¨2%
            elif total_samples > 10000:
                val_ratio = 0.05  # ä¸­ç­‰æ•°æ®é›†ç”¨5%
            else:
                val_ratio = 0.2   # å°æ•°æ®é›†ç”¨20%
            
            val_size = int(total_samples * val_ratio)
            train_size = total_samples - val_size
            
            print(f"ğŸ“Š æ•°æ®åˆ†å‰²: è®­ç»ƒ={train_size}, éªŒè¯={val_size} ({val_ratio*100:.1f}%)")
            
            # åˆ†å‰²æ•°æ®
            if val_size > 100:  # åªæœ‰å½“éªŒè¯é›†è¶³å¤Ÿå¤§æ—¶æ‰ä½¿ç”¨
                X_train = X_processed[:train_size]
                y_train = y_processed[:train_size]
                X_val = X_processed[train_size:]
                y_val = y_processed[train_size:]
                
                # è®¾ç½®å›è°ƒå‡½æ•°
                callbacks = [
                    keras.callbacks.EarlyStopping(
                        patience=5,
                        restore_best_weights=True,
                        verbose=1,
                        monitor='val_loss'
                    ),
                    keras.callbacks.ReduceLROnPlateau(
                        patience=3,
                        factor=0.5,
                        verbose=1,
                        monitor='val_loss'
                    )
                ]
                
                print("ğŸ”„ å¼€å§‹è®­ç»ƒï¼ˆå¸¦éªŒè¯é›†ï¼‰...")
                history = self.model.fit(
                    X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=epochs,
                    batch_size=batch_size,
                    callbacks=callbacks,
                    verbose=1
                )
            else:
                print("ğŸ”„ å¼€å§‹è®­ç»ƒï¼ˆæ— éªŒè¯é›†ï¼‰...")
                history = self.model.fit(
                    X_processed, y_processed,
                    epochs=epochs,
                    batch_size=batch_size,
                    verbose=1
                )
            
            print("âœ… TransformerRankerè®­ç»ƒå®Œæˆ")
            
            # è®¡ç®—ç‰¹å¾é‡è¦æ€§
            try:
                print("ğŸ” è®¡ç®—ç‰¹å¾é‡è¦æ€§...")
                sample_size = min(1000, len(X))
                self._compute_feature_importance(X[:sample_size])
                print("âœ… ç‰¹å¾é‡è¦æ€§è®¡ç®—å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ ç‰¹å¾é‡è¦æ€§è®¡ç®—å¤±è´¥: {e}")
                self._feature_importance = np.ones(self.input_dim) / self.input_dim
            
            return history
            
        except Exception as e:
            print(f"âŒ TransformerRankerè®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _compute_feature_importance(self, X_sample):
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§"""
        try:
            # æ–¹æ³•1: å°è¯•ä½¿ç”¨æ¢¯åº¦
            X_tensor = tf.convert_to_tensor(X_sample.astype(np.float32))
            
            with tf.GradientTape() as tape:
                tape.watch(X_tensor)
                predictions = self.model(X_tensor, training=False)
                loss = tf.reduce_mean(predictions)
            
            gradients = tape.gradient(loss, X_tensor)
            
            if gradients is not None:
                importance = np.mean(np.abs(gradients.numpy()), axis=0)
                importance = importance / (np.sum(importance) + 1e-8)
                self._feature_importance = importance
                print("âœ“ ä½¿ç”¨æ¢¯åº¦è®¡ç®—ç‰¹å¾é‡è¦æ€§")
            else:
                raise ValueError("æ— æ³•è®¡ç®—æ¢¯åº¦")
                
        except Exception as e:
            print(f"âš ï¸ æ¢¯åº¦æ–¹æ³•å¤±è´¥: {e}ï¼Œä½¿ç”¨æ–¹å·®æ–¹æ³•")
            # æ–¹æ³•2: ä½¿ç”¨è¾“å…¥ç‰¹å¾çš„æ–¹å·®
            try:
                feature_variance = np.var(X_sample, axis=0)
                importance = feature_variance / (np.sum(feature_variance) + 1e-8)
                self._feature_importance = importance
                print("âœ“ ä½¿ç”¨æ–¹å·®è®¡ç®—ç‰¹å¾é‡è¦æ€§")
            except:
                # æ–¹æ³•3: å‡åŒ€åˆ†å¸ƒ
                self._feature_importance = np.ones(self.input_dim) / self.input_dim
                print("âœ“ ä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºç‰¹å¾é‡è¦æ€§")
    
    def predict(self, X):
        """é¢„æµ‹åˆ†æ•°"""
        try:
            print(f"ğŸ”® TransformerRankeré¢„æµ‹ï¼Œæ•°æ®å½¢çŠ¶: {X.shape}")
            
            # æ•°æ®é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            X_processed = X.astype(np.float32)
            
            # æ¸…ç†å¼‚å¸¸å€¼
            if np.any(np.isnan(X_processed)) or np.any(np.isinf(X_processed)):
                X_processed = np.nan_to_num(X_processed, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # æ ‡å‡†åŒ–ï¼ˆæ³¨æ„ï¼šå®é™…åº”ç”¨ä¸­åº”è¯¥ä¿å­˜è®­ç»ƒæ—¶çš„scalerï¼‰
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_processed = scaler.fit_transform(X_processed).astype(np.float32)
            
            # æ‰¹é‡é¢„æµ‹ä»¥é¿å…å†…å­˜é—®é¢˜
            batch_size = 1000
            predictions = []
            
            for i in range(0, len(X_processed), batch_size):
                batch = X_processed[i:i+batch_size]
                batch_pred = self.model.predict(batch, verbose=0)
                predictions.append(batch_pred.flatten())
            
            result = np.concatenate(predictions)
            print(f"âœ… TransformerRankeré¢„æµ‹å®Œæˆï¼Œç»“æœå½¢çŠ¶: {result.shape}")
            return result
            
        except Exception as e:
            print(f"âŒ TransformerRankeré¢„æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›éšæœºå€¼ä½œä¸ºåå¤‡
            return np.random.random(len(X)).astype(np.float32)
    
    def get_model_name(self):
        return "TransformerRanker"
    
    @property
    def feature_importances_(self):
        if self._feature_importance is None:
            return np.ones(self.input_dim) / self.input_dim
        return self._feature_importance


class BM25Ranker(BaseRanker):
    """BM25æ’åºæ¨¡å‹"""
    
    def __init__(self, **params):
        self.params = params
        self.model = None
        self.tokenized_corpus = None
        self.feature_names = None
        
    def fit(self, X, y, group, **kwargs):
        # å°†ç‰¹å¾è½¬æ¢ä¸º"æ–‡æ¡£"å½¢å¼
        self.feature_names = [f"feature_{i}" for i in range(X.shape[1])]
        self.tokenized_corpus = []
        
        for i in range(X.shape[0]):
            # å°†ç‰¹å¾å€¼å¤§äºé˜ˆå€¼çš„ç‰¹å¾åä½œä¸º"è¯"
            threshold = kwargs.get('threshold', 0.5)
            doc = [self.feature_names[j] for j in np.where(X[i] > threshold)[0]]
            if not doc:  # å¦‚æœæ²¡æœ‰ç‰¹å¾è¶…è¿‡é˜ˆå€¼ï¼Œä½¿ç”¨æ‰€æœ‰éé›¶ç‰¹å¾
                doc = [self.feature_names[j] for j in np.where(X[i] != 0)[0]]
            self.tokenized_corpus.append(doc)
        
        self.model = BM25Okapi(self.tokenized_corpus)
    
    def predict(self, X):
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
        
        scores = []
        for i in range(X.shape[0]):
            query = [self.feature_names[j] for j in np.where(X[i] > 0.5)[0]]
            if not query:
                query = [self.feature_names[j] for j in np.where(X[i] != 0)[0]]
            
            if query:
                doc_scores = self.model.get_scores(query)
                scores.append(doc_scores[i] if i < len(doc_scores) else 0.0)
            else:
                scores.append(0.0)
        
        return np.array(scores)
    
    def get_model_name(self):
        return "BM25Ranker"


class ModelFactory:
    """æ¨¡å‹å·¥å‚ç±»"""
    
    @staticmethod
    def create_model(model_name: str, use_gpu: bool = True, input_dim: Optional[int] = None, **params) -> BaseRanker:
        """
        åˆ›å»ºæŒ‡å®šçš„æ¨¡å‹å®ä¾‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            use_gpu: æ˜¯å¦ä½¿ç”¨GPU
            input_dim: è¾“å…¥ç»´åº¦ï¼ˆç¥ç»ç½‘ç»œæ¨¡å‹éœ€è¦ï¼‰
            **params: æ¨¡å‹å‚æ•°
            
        Returns:
            BaseRanker: æ¨¡å‹å®ä¾‹
        """
        if model_name == 'XGBRanker':
            return XGBRankerModel(use_gpu=use_gpu, **params)
        elif model_name == 'LGBMRanker':
            return LGBMRankerModel(use_gpu=use_gpu, **params)
        elif model_name == 'LambdaMART':
            return LambdaMART(use_gpu=use_gpu, **params)
        elif model_name == 'ListNet':
            return ListNetModel(use_gpu=use_gpu, **params)
        elif model_name == 'NeuralRanker':
            if input_dim is None:
                raise ValueError("NeuralRankeréœ€è¦æŒ‡å®šinput_dimå‚æ•°")
            return NeuralRanker(input_dim=input_dim, **params)
        elif model_name == 'RankNet':
            if input_dim is None:
                raise ValueError("RankNetéœ€è¦æŒ‡å®šinput_dimå‚æ•°")
            return RankNet(input_dim=input_dim, **params)
        elif model_name == 'TransformerRanker':
            if input_dim is None:
                raise ValueError("TransformerRankeréœ€è¦æŒ‡å®šinput_dimå‚æ•°")
            return TransformerRanker(input_dim=input_dim, **params)
        elif model_name == 'BM25Ranker':
            return BM25Ranker(**params)
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_name}")
    
    @staticmethod
    def get_available_models() -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åç§°"""
        return [
            'XGBRanker', 'LGBMRanker', 'LambdaMART', 'ListNet', 
            'NeuralRanker', 'RankNet', 'TransformerRanker', 'BM25Ranker'
        ]
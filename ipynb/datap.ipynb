{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f52dbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Processing chunk 2\n",
      "Processing chunk 3\n",
      "Processing chunk 4\n",
      "Processing chunk 5\n",
      "Processing chunk 6\n",
      "Processing chunk 7\n",
      "Processing chunk 8\n",
      "Processing chunk 9\n",
      "Processing chunk 10\n",
      "Processing chunk 11\n",
      "Processing chunk 12\n",
      "Processing chunk 13\n",
      "Processing chunk 14\n",
      "Processing chunk 15\n",
      "Processing chunk 16\n",
      "Processing chunk 17\n",
      "Processing chunk 18\n",
      "Processing chunk 19\n",
      "Processing chunk 20\n",
      "Processing chunk 21\n",
      "Processing chunk 22\n",
      "Processing chunk 23\n",
      "Processing chunk 24\n",
      "Processing chunk 25\n",
      "Processing chunk 26\n",
      "Processing chunk 27\n",
      "Processing chunk 28\n",
      "Processing chunk 29\n",
      "Processing chunk 30\n",
      "Processing chunk 31\n",
      "Processing chunk 32\n",
      "Processing chunk 33\n",
      "Processing chunk 34\n",
      "Processing chunk 35\n",
      "Processing chunk 36\n",
      "Processing chunk 37\n",
      "Processing chunk 38\n",
      "Processing chunk 39\n",
      "Processing chunk 40\n",
      "Processing chunk 41\n",
      "Processing chunk 42\n",
      "Processing chunk 43\n",
      "Processing chunk 44\n",
      "Processing chunk 45\n",
      "Processing chunk 46\n",
      "Processing chunk 47\n",
      "Processing chunk 48\n",
      "Processing chunk 49\n",
      "Processing chunk 50\n",
      "Processing chunk 51\n",
      "Processing chunk 52\n",
      "Processing chunk 53\n",
      "Processing chunk 54\n",
      "Processing chunk 55\n",
      "Processing chunk 56\n",
      "Processing chunk 57\n",
      "Processing chunk 58\n",
      "Processing chunk 59\n",
      "Processing chunk 60\n",
      "Processing chunk 61\n",
      "Processing chunk 62\n",
      "Processing chunk 63\n",
      "Processing chunk 64\n",
      "Processing chunk 65\n",
      "Processing chunk 66\n",
      "Processing chunk 67\n",
      "Processing chunk 68\n",
      "Processing chunk 69\n",
      "Processing chunk 70\n",
      "Processing chunk 71\n",
      "Processing chunk 72\n",
      "Processing chunk 73\n",
      "Processing chunk 74\n",
      "Processing chunk 75\n",
      "Processing chunk 76\n",
      "Processing chunk 77\n",
      "Processing chunk 78\n",
      "Processing chunk 79\n",
      "Processing chunk 80\n",
      "Processing chunk 81\n",
      "Processing chunk 82\n",
      "Processing chunk 83\n",
      "Processing chunk 84\n",
      "Processing chunk 85\n",
      "Processing chunk 86\n",
      "Processing chunk 87\n",
      "Processing chunk 88\n",
      "Processing chunk 89\n",
      "Processing chunk 90\n",
      "Processing chunk 91\n",
      "Processing chunk 92\n",
      "Processing chunk 93\n",
      "Processing chunk 94\n",
      "Processing chunk 95\n",
      "Processing chunk 96\n",
      "Processing chunk 97\n",
      "Processing chunk 98\n",
      "Processing chunk 99\n",
      "Processing chunk 100\n",
      "Processing chunk 101\n",
      "Processing chunk 102\n",
      "Processing chunk 103\n",
      "Processing chunk 104\n",
      "Processing chunk 105\n",
      "Processing chunk 106\n",
      "Processing chunk 107\n",
      "Processing chunk 108\n",
      "Processing chunk 109\n",
      "Processing chunk 110\n",
      "Processing chunk 111\n",
      "Processing chunk 112\n",
      "Processing chunk 113\n",
      "Processing chunk 114\n",
      "Processing chunk 115\n",
      "Processing chunk 116\n",
      "Processing chunk 117\n",
      "Processing chunk 118\n",
      "Processing chunk 119\n",
      "Processing chunk 120\n",
      "Processing chunk 121\n",
      "Processing chunk 122\n",
      "Processing chunk 123\n",
      "Processing chunk 124\n",
      "Processing chunk 125\n",
      "Processing chunk 126\n",
      "Processing chunk 127\n",
      "Processing chunk 128\n",
      "Processing chunk 129\n",
      "Processing chunk 130\n",
      "Processing chunk 131\n",
      "Processing chunk 132\n",
      "Processing chunk 133\n",
      "Processing chunk 134\n",
      "Processing chunk 135\n",
      "Processing chunk 136\n",
      "Processing chunk 137\n",
      "Processing chunk 138\n",
      "Processing chunk 139\n",
      "Processing chunk 140\n",
      "Processing chunk 141\n",
      "Processing chunk 142\n",
      "Processing chunk 143\n",
      "Processing chunk 144\n",
      "Processing chunk 145\n",
      "Processing chunk 146\n",
      "Processing chunk 147\n",
      "Processing chunk 148\n",
      "Processing chunk 149\n",
      "Processing chunk 150\n",
      "Processing chunk 151\n",
      "Processing chunk 152\n",
      "Processing chunk 153\n",
      "Processing chunk 154\n",
      "Processing chunk 155\n",
      "Processing chunk 156\n",
      "Processing chunk 157\n",
      "Processing chunk 158\n",
      "Processing chunk 159\n",
      "Processing chunk 160\n",
      "Processing chunk 161\n",
      "Processing chunk 162\n",
      "Processing chunk 163\n",
      "Processing chunk 164\n",
      "Processing chunk 165\n",
      "Processing chunk 166\n",
      "Processing chunk 167\n",
      "Processing chunk 168\n",
      "Processing chunk 169\n",
      "Processing chunk 170\n",
      "Processing chunk 171\n",
      "Processing chunk 172\n",
      "Processing chunk 173\n",
      "Processing chunk 174\n",
      "Processing chunk 175\n",
      "Processing chunk 176\n",
      "Processing chunk 177\n",
      "Processing chunk 178\n",
      "Processing chunk 179\n",
      "Processing chunk 180\n",
      "Processing chunk 181\n",
      "Processing chunk 182\n",
      "All chunks processed. Now merging...\n",
      "Processing completed. Optimized data saved to: E:/GIT PROJECT/FR/kaggle/input/data/aeroclub-recsys-2025\\processed_train.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_duration(duration_str):\n",
    "    \"\"\"更健壮的持续时间解析函数，处理多种格式\"\"\"\n",
    "    if pd.isna(duration_str):\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        # 尝试处理\"HH:MM:SS\"格式\n",
    "        if ':' in duration_str:\n",
    "            parts = duration_str.split(':')\n",
    "            if len(parts) == 3:  # HH:MM:SS\n",
    "                return int(parts[0])*3600 + int(parts[1])*60 + int(parts[2])\n",
    "            elif len(parts) == 2:  # MM:SS\n",
    "                return int(parts[0])*60 + int(parts[1])\n",
    "        \n",
    "        # 尝试处理小数格式（可能是小时）\n",
    "        try:\n",
    "            hours = float(duration_str)\n",
    "            return int(hours * 3600)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # 其他无法识别的格式返回0\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def optimize_data_types(df):\n",
    "    \"\"\"\n",
    "    根据数据类型优化表转换数据类型\n",
    "    \"\"\"\n",
    "    # 删除所有名称中含有segments3的列（不区分大小写）\n",
    "    seg3_cols = [col for col in df.columns if 'segments3' in col.lower()]\n",
    "    df.drop(columns=seg3_cols, inplace=True, errors='ignore')\n",
    "    \n",
    "    # 数据类型转换\n",
    "    # 1. 将double类型转换为int32/int8\n",
    "    double_to_int_cols = [\n",
    "        'legs1_segments2_baggageAllowance_weightMeasurementType',\n",
    "        'legs1_segments2_baggageAllowance_quantity',\n",
    "        'legs1_segments2_cabinClass',\n",
    "        'legs1_segments2_seatsAvailable',\n",
    "        'legs0_segments2_baggageAllowance_quantity',\n",
    "        'legs0_segments2_baggageAllowance_weightMeasurementType',\n",
    "        'legs0_segments2_cabinClass',\n",
    "        'legs0_segments2_seatsAvailable',\n",
    "        'miniRules1_percentage',\n",
    "        'miniRules0_percentage',\n",
    "        'legs1_segments1_seatsAvailable',\n",
    "        'legs1_segments1_baggageAllowance_weightMeasurementType',\n",
    "        'legs1_segments1_baggageAllowance_quantity',\n",
    "        'legs1_segments1_cabinClass',\n",
    "        'legs0_segments1_seatsAvailable',\n",
    "        'legs0_segments1_baggageAllowance_weightMeasurementType',\n",
    "        'legs0_segments1_baggageAllowance_quantity',\n",
    "        'legs0_segments1_cabinClass',\n",
    "        'corporateTariffCode',\n",
    "        'legs1_segments0_seatsAvailable',\n",
    "        'legs1_segments0_baggageAllowance_weightMeasurementType',\n",
    "        'legs1_segments0_baggageAllowance_quantity',\n",
    "        'legs1_segments0_cabinClass',\n",
    "        'miniRules1_statusInfos',\n",
    "        'miniRules0_statusInfos',\n",
    "        'miniRules1_monetaryAmount',\n",
    "        'miniRules0_monetaryAmount',\n",
    "        'pricingInfo_isAccessTP',\n",
    "        'legs0_segments0_seatsAvailable',\n",
    "        'legs0_segments0_baggageAllowance_weightMeasurementType',\n",
    "        'legs0_segments0_baggageAllowance_quantity',\n",
    "        'legs0_segments0_cabinClass',\n",
    "        'nationality',\n",
    "        'Id',\n",
    "        'pricingInfo_passengerCount'\n",
    "    ]\n",
    "    \n",
    "    for col in double_to_int_cols:\n",
    "        if col in df.columns:\n",
    "            # 先填充NaN为0（根据业务逻辑决定，可能需要其他填充方式）\n",
    "            df[col] = df[col].fillna(0)\n",
    "            # 转换为int32以节省内存\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "            except:\n",
    "                # 如果转换失败，尝试先转换为float再转换为int\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('int32')\n",
    "    \n",
    "    # 2. 时间类型转换\n",
    "    time_cols = [\n",
    "        'legs1_departureAt',\n",
    "        'legs1_arrivalAt',\n",
    "        'legs0_departureAt',\n",
    "        'legs0_arrivalAt',\n",
    "        'requestDate'\n",
    "    ]\n",
    "    \n",
    "    duration_cols = [\n",
    "        'legs1_duration',\n",
    "        'legs1_segments0_duration',\n",
    "        'legs0_segments0_duration',\n",
    "        'legs0_duration',\n",
    "        'legs1_segments2_duration',\n",
    "        'legs0_segments2_duration',\n",
    "        'legs1_segments1_duration',\n",
    "        'legs0_segments1_duration'\n",
    "    ]\n",
    "    \n",
    "    for col in time_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                # 处理时间戳\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                df[col] = (df[col] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "                df[col] = pd.to_numeric(df[col], downcast='integer').fillna(0)\n",
    "            except:\n",
    "                df[col] = 0\n",
    "    \n",
    "    for col in duration_cols:\n",
    "        if col in df.columns:\n",
    "            # 使用更健壮的持续时间解析函数\n",
    "            df[col] = df[col].apply(parse_duration)\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    # 3. 布尔类型转换\n",
    "    bool_cols = [\n",
    "        'isAccess3D',\n",
    "        'isVip',\n",
    "        'bySelf',\n",
    "        'sex'\n",
    "    ]\n",
    "    \n",
    "    for col in bool_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('bool')\n",
    "    \n",
    "    # 4. 分类变量编码\n",
    "    # 对于searchRoute，检查是否包含\"/\"表示往返票\n",
    "    if 'searchRoute' in df.columns:\n",
    "        df['is_round_trip'] = df['searchRoute'].str.contains('/').fillna(False).astype('int8')\n",
    "        df.drop(columns=['searchRoute'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # 5. 分箱编码\n",
    "    if 'taxes' in df.columns:\n",
    "        try:\n",
    "            df['taxes_bin'] = pd.qcut(df['taxes'], q=10, labels=False, duplicates='drop').astype('int8')\n",
    "            df.drop(columns=['taxes'], inplace=True, errors='ignore')\n",
    "        except:\n",
    "            df['taxes_bin'] = 0\n",
    "    \n",
    "    if 'totalPrice' in df.columns:\n",
    "        try:\n",
    "            df['totalPrice_bin'] = pd.qcut(df['totalPrice'], q=10, labels=False, duplicates='drop').astype('int8')\n",
    "            df.drop(columns=['totalPrice'], inplace=True, errors='ignore')\n",
    "        except:\n",
    "            df['totalPrice_bin'] = 0\n",
    "    \n",
    "    # 6. 字符串分类变量\n",
    "    str_cat_cols = [\n",
    "        'frequentFlyer',\n",
    "        'legs1_segments2_marketingCarrier_code',\n",
    "        'legs1_segments2_operatingCarrier_code',\n",
    "        'legs1_segments2_flightNumber',\n",
    "        'legs1_segments2_arrivalTo_airport_iata',\n",
    "        'legs1_segments2_departureFrom_airport_iata',\n",
    "        'legs1_segments2_aircraft_code',\n",
    "        'legs1_segments2_arrivalTo_airport_city_iata',\n",
    "        'legs0_segments2_aircraft_code',\n",
    "        'legs0_segments2_marketingCarrier_code',\n",
    "        'legs0_segments2_flightNumber',\n",
    "        'legs0_segments2_arrivalTo_airport_iata',\n",
    "        'legs0_segments2_departureFrom_airport_iata',\n",
    "        'legs0_segments2_arrivalTo_airport_city_iata',\n",
    "        'legs0_segments2_operatingCarrier_code',\n",
    "        'legs1_segments1_arrivalTo_airport_iata',\n",
    "        'legs1_segments1_arrivalTo_airport_city_iata',\n",
    "        'legs1_segments1_operatingCarrier_code',\n",
    "        'legs1_segments1_departureFrom_airport_iata',\n",
    "        'legs1_segments1_marketingCarrier_code',\n",
    "        'legs1_segments1_flightNumber',\n",
    "        'legs1_segments1_aircraft_code',\n",
    "        'legs0_segments1_aircraft_code',\n",
    "        'legs0_segments1_arrivalTo_airport_city_iata',\n",
    "        'legs0_segments1_flightNumber',\n",
    "        'legs0_segments1_arrivalTo_airport_iata',\n",
    "        'legs0_segments1_operatingCarrier_code',\n",
    "        'legs0_segments1_marketingCarrier_code',\n",
    "        'legs0_segments1_departureFrom_airport_iata',\n",
    "        'legs1_segments0_arrivalTo_airport_city_iata',\n",
    "        'legs1_segments0_departureFrom_airport_iata',\n",
    "        'legs1_segments0_flightNumber',\n",
    "        'legs1_segments0_operatingCarrier_code',\n",
    "        'legs1_segments0_aircraft_code',\n",
    "        'legs1_segments0_marketingCarrier_code',\n",
    "        'legs1_segments0_arrivalTo_airport_iata',\n",
    "        'legs0_segments0_arrivalTo_airport_city_iata',\n",
    "        'legs0_segments0_aircraft_code',\n",
    "        'legs0_segments0_arrivalTo_airport_iata',\n",
    "        'legs0_segments0_departureFrom_airport_iata',\n",
    "        'legs0_segments0_marketingCarrier_code',\n",
    "        'legs0_segments0_operatingCarrier_code',\n",
    "        'legs0_segments0_flightNumber'\n",
    "    ]\n",
    "    \n",
    "    for col in str_cat_cols:\n",
    "        if col in df.columns:\n",
    "            # 对于高基数的分类变量，使用哈希编码减少内存\n",
    "            df[col] = df[col].astype('str')\n",
    "            df[col] = df[col].apply(lambda x: hash(x) % 65535 if pd.notna(x) else 0).astype('int32')\n",
    "    \n",
    "    # 7. 处理ranker_id和profileId\n",
    "    if 'ranker_id' in df.columns:\n",
    "        df['ranker_id'] = df['ranker_id'].apply(lambda x: hash(str(x)) % 65535).astype('int32')\n",
    "    \n",
    "    if 'profileId' in df.columns:\n",
    "        df['profileId'] = df['profileId'].astype('int32')\n",
    "    \n",
    "    if 'companyID' in df.columns:\n",
    "        df['companyID'] = df['companyID'].astype('int32')\n",
    "    \n",
    "    # 8. 目标变量\n",
    "    if 'selected' in df.columns:\n",
    "        df['selected'] = df['selected'].astype('int8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_large_parquet(file_path, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    分块处理大型Parquet文件\n",
    "    \"\"\"\n",
    "    # 首先读取schema\n",
    "    pf = pq.ParquetFile(file_path)\n",
    "    \n",
    "    # 获取所有列名\n",
    "    all_columns = pf.schema.names\n",
    "    \n",
    "    # 预先删除segments3的列\n",
    "    columns_to_read = [col for col in all_columns if 'segments3' not in col.lower()]\n",
    "    \n",
    "    # 创建输出目录\n",
    "    output_dir = os.path.join(os.path.dirname(file_path), 'processed')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 分块处理\n",
    "    for i, batch in enumerate(pf.iter_batches(batch_size=chunk_size, columns=columns_to_read)):\n",
    "        print(f\"Processing chunk {i+1}\")\n",
    "        df = batch.to_pandas()\n",
    "        \n",
    "        # 优化数据类型\n",
    "        df = optimize_data_types(df)\n",
    "        \n",
    "        # 保存处理后的数据\n",
    "        output_path = os.path.join(output_dir, f'chunk_{i}.parquet')\n",
    "        df.to_parquet(output_path, index=False)\n",
    "        \n",
    "        # 手动清理内存\n",
    "        del df\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"All chunks processed. Now merging...\")\n",
    "    \n",
    "    # 合并所有分块\n",
    "    chunk_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.startswith('chunk_')]\n",
    "    df_list = []\n",
    "    \n",
    "    for chunk_file in sorted(chunk_files, key=lambda x: int(x.split('_')[-1].split('.')[0])):\n",
    "        df_chunk = pd.read_parquet(chunk_file)\n",
    "        df_list.append(df_chunk)\n",
    "    \n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # 保存最终文件\n",
    "    final_output_path = os.path.join(os.path.dirname(file_path), 'processed_train.parquet')\n",
    "    final_df.to_parquet(final_output_path, index=False)\n",
    "    \n",
    "    # 清理临时文件\n",
    "    for chunk_file in chunk_files:\n",
    "        os.remove(chunk_file)\n",
    "    os.rmdir(output_dir)\n",
    "    \n",
    "    return final_output_path\n",
    "\n",
    "# 处理训练数据\n",
    "train_file_path = \"E:/GIT PROJECT/FR/kaggle/input/data/aeroclub-recsys-2025/train.parquet\"\n",
    "processed_path = process_large_parquet(train_file_path)\n",
    "\n",
    "print(f\"Processing completed. Optimized data saved to: {processed_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

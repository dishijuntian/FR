{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 航班排名竞赛 - 业务逻辑理解与特征分析\n",
    "## Flight Ranking Competition - Business Logic Understanding & Feature Analysis\n",
    "\n",
    "#### 1. **智能缺失值处理**\n",
    "- 自动移除缺失值超过80%的特征\n",
    "- 按`ranker_id`分组分析，只处理有足够数据的搜索会话\n",
    "- 对保留特征进行合理填充策略\n",
    "\n",
    "#### 2. **分析维度**\n",
    "- 商务vs休闲旅行者深度对比\n",
    "- 价格敏感性和支付意愿分析\n",
    "- 时间偏好模式（出发时间、预订时间）\n",
    "- 航线复杂性偏好（直飞vs中转）\n",
    "- 航空公司和机场偏好\n",
    "- 用户聚类和忠诚度分析\n",
    "- 企业差旅政策影响\n",
    "- 舱位等级偏好\n",
    "- 提前预订行为分析\n",
    "- 往返vs单程偏好\n",
    "- 用户转化率分析\n",
    "- 拓展...\n",
    "\n",
    "#### 3. **功能**\n",
    "- **用户画像系统**：为每个用户创建详细的偏好档案\n",
    "- **智能聚类**：将用户分为4个主要群体\n",
    "- **交互式可视化**：使用Plotly创建动态图表\n",
    "- **商业洞察**：生成可执行的商业建议\n",
    "\n",
    "### 📊 系统架构\n",
    "\n",
    "```\n",
    "EnhancedFlightDataAnalyzer\n",
    "├── 数据加载与预处理\n",
    "├── 分组数据创建 (按ranker_id)\n",
    "├── 用户画像生成\n",
    "├── 多维度分析模块\n",
    "│   ├── 商务vs休闲分析\n",
    "│   ├── 价格敏感性分析\n",
    "│   ├── 时间偏好分析\n",
    "│   ├── 航线偏好分析\n",
    "│   └── 用户聚类分析\n",
    "└── 综合报告生成\n",
    "```\n",
    "\n",
    "### 🚀 使用方法\n",
    "\n",
    "```python\n",
    "# 初始化分析器\n",
    "analyzer = EnhancedFlightDataAnalyzer(\n",
    "    train_path='data/train.parquet',\n",
    "    test_path='data/test.parquet'\n",
    ")\n",
    "\n",
    "# 运行完整分析\n",
    "analyzer.run_full_analysis()\n",
    "\n",
    "# 或者运行单个分析模块\n",
    "analyzer.load_and_preprocess_data()\n",
    "analyzer.analyze_business_vs_leisure_detailed()\n",
    "analyzer.analyze_price_sensitivity()\n",
    "```\n",
    "\n",
    "### 🎯 关键洞察能力\n",
    "\n",
    "1. **用户分群**：自动识别高价值商务用户和价格敏感休闲用户\n",
    "2. **偏好预测**：基于历史行为预测用户选择倾向\n",
    "3. **个性化推荐**：为不同用户群体提供定制化的航班推荐策略\n",
    "4. **转化优化**：识别影响用户选择的关键因素\n",
    "\n",
    "### 📈 输出报告\n",
    "\n",
    "系统会生成：\n",
    "- 多维度可视化图表\n",
    "- 用户聚类分析结果\n",
    "- 商业洞察和建议\n",
    "- 详细的统计报告文件\n",
    "\n",
    "这个系统特别适合：\n",
    "- 航班推荐系统优化\n",
    "- 用户行为分析\n",
    "- 个性化营销策略制定\n",
    "- 产品功能改进决策"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "测试数据文件\n",
      "============================================================\n",
      "============================================================\n",
      "分析测试数据\n",
      "============================================================\n",
      "航班数据分析器已初始化\n",
      "- 数据文件: ../data/test.parquet\n",
      "- 输出目录: ../data/test_flight_analysis_results\n",
      "- 最大处理行数: 500,000\n",
      "============================================================\n",
      "开始航班数据分析...\n",
      "============================================================\n",
      "开始加载数据...\n",
      "文件大小: 137.5 MB\n",
      "直接加载文件...\n",
      "数据量 (6,897,776) 超过限制 (500,000)，进行随机采样...\n",
      "采样后数据量: 500,000\n",
      "数据加载完成 | 总行数: 500,000 | 列数: 125\n",
      "数据列: ['Id', 'bySelf', 'companyID', 'corporateTariffCode', 'frequentFlyer', 'nationality', 'isAccess3D', 'isVip', 'legs0_arrivalAt', 'legs0_departureAt', 'legs0_duration', 'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata', 'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_baggageAllowance_quantity', 'legs0_segments0_baggageAllowance_weightMeasurementType', 'legs0_segments0_cabinClass', 'legs0_segments0_departureFrom_airport_iata', 'legs0_segments0_duration', 'legs0_segments0_flightNumber', 'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code', 'legs0_segments0_seatsAvailable', 'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata', 'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_baggageAllowance_quantity', 'legs0_segments1_baggageAllowance_weightMeasurementType', 'legs0_segments1_cabinClass', 'legs0_segments1_departureFrom_airport_iata', 'legs0_segments1_duration', 'legs0_segments1_flightNumber', 'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code', 'legs0_segments1_seatsAvailable', 'legs0_segments2_aircraft_code', 'legs0_segments2_arrivalTo_airport_city_iata', 'legs0_segments2_arrivalTo_airport_iata', 'legs0_segments2_baggageAllowance_quantity', 'legs0_segments2_baggageAllowance_weightMeasurementType', 'legs0_segments2_cabinClass', 'legs0_segments2_departureFrom_airport_iata', 'legs0_segments2_duration', 'legs0_segments2_flightNumber', 'legs0_segments2_marketingCarrier_code', 'legs0_segments2_operatingCarrier_code', 'legs0_segments2_seatsAvailable', 'legs0_segments3_aircraft_code', 'legs0_segments3_arrivalTo_airport_city_iata', 'legs0_segments3_arrivalTo_airport_iata', 'legs0_segments3_baggageAllowance_quantity', 'legs0_segments3_baggageAllowance_weightMeasurementType', 'legs0_segments3_cabinClass', 'legs0_segments3_departureFrom_airport_iata', 'legs0_segments3_duration', 'legs0_segments3_flightNumber', 'legs0_segments3_marketingCarrier_code', 'legs0_segments3_operatingCarrier_code', 'legs0_segments3_seatsAvailable', 'legs1_arrivalAt', 'legs1_departureAt', 'legs1_duration', 'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata', 'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_baggageAllowance_quantity', 'legs1_segments0_baggageAllowance_weightMeasurementType', 'legs1_segments0_cabinClass', 'legs1_segments0_departureFrom_airport_iata', 'legs1_segments0_duration', 'legs1_segments0_flightNumber', 'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code', 'legs1_segments0_seatsAvailable', 'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata', 'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_baggageAllowance_quantity', 'legs1_segments1_baggageAllowance_weightMeasurementType', 'legs1_segments1_cabinClass', 'legs1_segments1_departureFrom_airport_iata', 'legs1_segments1_duration', 'legs1_segments1_flightNumber', 'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code', 'legs1_segments1_seatsAvailable', 'legs1_segments2_aircraft_code', 'legs1_segments2_arrivalTo_airport_city_iata', 'legs1_segments2_arrivalTo_airport_iata', 'legs1_segments2_baggageAllowance_quantity', 'legs1_segments2_baggageAllowance_weightMeasurementType', 'legs1_segments2_cabinClass', 'legs1_segments2_departureFrom_airport_iata', 'legs1_segments2_duration', 'legs1_segments2_flightNumber', 'legs1_segments2_marketingCarrier_code', 'legs1_segments2_operatingCarrier_code', 'legs1_segments2_seatsAvailable', 'legs1_segments3_aircraft_code', 'legs1_segments3_arrivalTo_airport_city_iata', 'legs1_segments3_arrivalTo_airport_iata', 'legs1_segments3_baggageAllowance_quantity', 'legs1_segments3_baggageAllowance_weightMeasurementType', 'legs1_segments3_cabinClass', 'legs1_segments3_departureFrom_airport_iata', 'legs1_segments3_duration', 'legs1_segments3_flightNumber', 'legs1_segments3_marketingCarrier_code', 'legs1_segments3_operatingCarrier_code', 'legs1_segments3_seatsAvailable', 'miniRules0_monetaryAmount', 'miniRules0_percentage', 'miniRules0_statusInfos', 'miniRules1_monetaryAmount', 'miniRules1_percentage', 'miniRules1_statusInfos', 'pricingInfo_isAccessTP', 'pricingInfo_passengerCount', 'profileId', 'ranker_id', 'requestDate', 'searchRoute', 'sex', 'taxes', 'totalPrice']\n",
      "分析数据结构...\n",
      "数据形状: (500000, 125)\n",
      "内存使用: 1519.8 MB\n",
      "列数: 125\n",
      "可用关键列: ['ranker_id']\n",
      "缺失关键列: ['user_id', 'departure_datetime', 'booking_datetime', 'price', 'num_stops']\n",
      "预处理数据...\n",
      "数据预处理完成\n",
      "使用用户标识列: ranker_id\n",
      "创建用户分组数据...\n",
      "创建用户分组数据完成 | 用户数: 23,200\n",
      "[load_and_preprocess_data] 耗时: 18.94s | 内存: 6738.3MB (+5913.9MB)\n",
      "分析整体模式...\n",
      "整体模式分析完成\n",
      "[analyze_overall_patterns] 耗时: 0.04s | 内存: 6738.4MB (+0.1MB)\n",
      "分析用户行为...\n",
      "用户行为分析完成\n",
      "[analyze_user_behavior] 耗时: 0.06s | 内存: 6742.7MB (+4.3MB)\n",
      "生成综合分析报告...\n",
      "综合报告已保存到: ../data/test_flight_analysis_results/reports/comprehensive_analysis.md\n",
      "[generate_comprehensive_report] 耗时: 0.00s | 内存: 6742.7MB (+0.0MB)\n",
      "============================================================\n",
      "分析完成！\n",
      "============================================================\n",
      "查看结果:\n",
      "- 报告: ../data/test_flight_analysis_results/reports/\n",
      "- 图表: ../data/test_flight_analysis_results/plots/\n",
      "内存清理完成\n",
      "✅ 测试数据分析完成！\n",
      "\n",
      "============================================================\n",
      "分析训练数据\n",
      "============================================================\n",
      "航班数据分析器已初始化\n",
      "- 数据文件: ../data/train.parquet\n",
      "- 输出目录: ../data/train_flight_analysis_results\n",
      "- 最大处理行数: 500,000\n",
      "============================================================\n",
      "开始航班数据分析...\n",
      "============================================================\n",
      "开始加载数据...\n",
      "文件大小: 376.6 MB\n",
      "直接加载文件...\n",
      "数据量 (18,145,372) 超过限制 (500,000)，进行随机采样...\n",
      "采样后数据量: 500,000\n",
      "数据加载完成 | 总行数: 500,000 | 列数: 126\n",
      "数据列: ['Id', 'bySelf', 'companyID', 'corporateTariffCode', 'frequentFlyer', 'nationality', 'isAccess3D', 'isVip', 'legs0_arrivalAt', 'legs0_departureAt', 'legs0_duration', 'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata', 'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_baggageAllowance_quantity', 'legs0_segments0_baggageAllowance_weightMeasurementType', 'legs0_segments0_cabinClass', 'legs0_segments0_departureFrom_airport_iata', 'legs0_segments0_duration', 'legs0_segments0_flightNumber', 'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code', 'legs0_segments0_seatsAvailable', 'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata', 'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_baggageAllowance_quantity', 'legs0_segments1_baggageAllowance_weightMeasurementType', 'legs0_segments1_cabinClass', 'legs0_segments1_departureFrom_airport_iata', 'legs0_segments1_duration', 'legs0_segments1_flightNumber', 'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code', 'legs0_segments1_seatsAvailable', 'legs0_segments2_aircraft_code', 'legs0_segments2_arrivalTo_airport_city_iata', 'legs0_segments2_arrivalTo_airport_iata', 'legs0_segments2_baggageAllowance_quantity', 'legs0_segments2_baggageAllowance_weightMeasurementType', 'legs0_segments2_cabinClass', 'legs0_segments2_departureFrom_airport_iata', 'legs0_segments2_duration', 'legs0_segments2_flightNumber', 'legs0_segments2_marketingCarrier_code', 'legs0_segments2_operatingCarrier_code', 'legs0_segments2_seatsAvailable', 'legs0_segments3_aircraft_code', 'legs0_segments3_arrivalTo_airport_city_iata', 'legs0_segments3_arrivalTo_airport_iata', 'legs0_segments3_baggageAllowance_quantity', 'legs0_segments3_baggageAllowance_weightMeasurementType', 'legs0_segments3_cabinClass', 'legs0_segments3_departureFrom_airport_iata', 'legs0_segments3_duration', 'legs0_segments3_flightNumber', 'legs0_segments3_marketingCarrier_code', 'legs0_segments3_operatingCarrier_code', 'legs0_segments3_seatsAvailable', 'legs1_arrivalAt', 'legs1_departureAt', 'legs1_duration', 'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata', 'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_baggageAllowance_quantity', 'legs1_segments0_baggageAllowance_weightMeasurementType', 'legs1_segments0_cabinClass', 'legs1_segments0_departureFrom_airport_iata', 'legs1_segments0_duration', 'legs1_segments0_flightNumber', 'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code', 'legs1_segments0_seatsAvailable', 'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata', 'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_baggageAllowance_quantity', 'legs1_segments1_baggageAllowance_weightMeasurementType', 'legs1_segments1_cabinClass', 'legs1_segments1_departureFrom_airport_iata', 'legs1_segments1_duration', 'legs1_segments1_flightNumber', 'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code', 'legs1_segments1_seatsAvailable', 'legs1_segments2_aircraft_code', 'legs1_segments2_arrivalTo_airport_city_iata', 'legs1_segments2_arrivalTo_airport_iata', 'legs1_segments2_baggageAllowance_quantity', 'legs1_segments2_baggageAllowance_weightMeasurementType', 'legs1_segments2_cabinClass', 'legs1_segments2_departureFrom_airport_iata', 'legs1_segments2_duration', 'legs1_segments2_flightNumber', 'legs1_segments2_marketingCarrier_code', 'legs1_segments2_operatingCarrier_code', 'legs1_segments2_seatsAvailable', 'legs1_segments3_aircraft_code', 'legs1_segments3_arrivalTo_airport_city_iata', 'legs1_segments3_arrivalTo_airport_iata', 'legs1_segments3_baggageAllowance_quantity', 'legs1_segments3_baggageAllowance_weightMeasurementType', 'legs1_segments3_cabinClass', 'legs1_segments3_departureFrom_airport_iata', 'legs1_segments3_duration', 'legs1_segments3_flightNumber', 'legs1_segments3_marketingCarrier_code', 'legs1_segments3_operatingCarrier_code', 'legs1_segments3_seatsAvailable', 'miniRules0_monetaryAmount', 'miniRules0_percentage', 'miniRules0_statusInfos', 'miniRules1_monetaryAmount', 'miniRules1_percentage', 'miniRules1_statusInfos', 'pricingInfo_isAccessTP', 'pricingInfo_passengerCount', 'profileId', 'ranker_id', 'requestDate', 'searchRoute', 'sex', 'taxes', 'totalPrice', 'selected']\n",
      "分析数据结构...\n",
      "数据形状: (500000, 126)\n",
      "内存使用: 1507.1 MB\n",
      "列数: 126\n",
      "可用关键列: ['ranker_id']\n",
      "缺失关键列: ['user_id', 'departure_datetime', 'booking_datetime', 'price', 'num_stops']\n",
      "预处理数据...\n",
      "数据预处理完成\n",
      "使用用户标识列: ranker_id\n",
      "创建用户分组数据...\n",
      "创建用户分组数据完成 | 用户数: 38,675\n",
      "[load_and_preprocess_data] 耗时: 53.17s | 内存: 3117.8MB (-3156.2MB)\n",
      "分析整体模式...\n",
      "整体模式分析完成\n",
      "[analyze_overall_patterns] 耗时: 0.05s | 内存: 3118.7MB (+0.9MB)\n",
      "分析用户行为...\n",
      "用户行为分析完成\n",
      "[analyze_user_behavior] 耗时: 0.06s | 内存: 3123.7MB (+5.0MB)\n",
      "生成综合分析报告...\n",
      "综合报告已保存到: ../data/train_flight_analysis_results/reports/comprehensive_analysis.md\n",
      "[generate_comprehensive_report] 耗时: 0.00s | 内存: 3123.8MB (+0.1MB)\n",
      "============================================================\n",
      "分析完成！\n",
      "============================================================\n",
      "查看结果:\n",
      "- 报告: ../data/train_flight_analysis_results/reports/\n",
      "- 图表: ../data/train_flight_analysis_results/plots/\n",
      "内存清理完成\n",
      "✅ 训练数据分析完成！\n",
      "\n",
      "============================================================\n",
      "分析完成，请查看输出目录中的结果\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "from functools import wraps\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def memory_monitor(func):\n",
    "    \"\"\"内存监控装饰器\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        process = psutil.Process()\n",
    "        mem_before = process.memory_info().rss / 1024**2\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        mem_after = process.memory_info().rss / 1024**2\n",
    "        print(f\"[{func.__name__}] 耗时: {end_time-start_time:.2f}s | 内存: {mem_after:.1f}MB ({mem_after-mem_before:+.1f}MB)\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "class EnhancedFlightDataAnalyzer:\n",
    "    \"\"\"航班数据分析器 - 支持单一数据集分析\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, output_dir: str = \"flight_analysis\", max_rows: int = 1000000):\n",
    "        \"\"\"\n",
    "        初始化分析器\n",
    "        \n",
    "        Args:\n",
    "            data_path: 数据文件路径\n",
    "            output_dir: 输出目录\n",
    "            max_rows: 最大处理行数（用于内存控制）\n",
    "        \"\"\"\n",
    "        if not data_path or not os.path.exists(data_path):\n",
    "            raise ValueError(f\"数据文件路径无效: {data_path}\")\n",
    "            \n",
    "        self.data_path = data_path\n",
    "        self.output_dir = output_dir\n",
    "        self.max_rows = max_rows\n",
    "        self.chunk_size = 100000\n",
    "        \n",
    "        # 初始化目录\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        for subdir in ['reports', 'plots', 'processed_data']:\n",
    "            Path(f\"{output_dir}/{subdir}\").mkdir(exist_ok=True)\n",
    "        \n",
    "        # 数据存储\n",
    "        self.data = None\n",
    "        self.grouped_data = None\n",
    "        self.analysis_results = {}\n",
    "        self.data_info = {}\n",
    "        \n",
    "        print(f\"航班数据分析器已初始化\")\n",
    "        print(f\"- 数据文件: {data_path}\")\n",
    "        print(f\"- 输出目录: {output_dir}\")\n",
    "        print(f\"- 最大处理行数: {max_rows:,}\")\n",
    "    \n",
    "    @memory_monitor\n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"加载和预处理数据\"\"\"\n",
    "        print(\"开始加载数据...\")\n",
    "        \n",
    "        # 获取文件信息\n",
    "        file_size_mb = os.path.getsize(self.data_path) / (1024**2)\n",
    "        print(f\"文件大小: {file_size_mb:.1f} MB\")\n",
    "        \n",
    "        # 根据文件大小决定加载策略\n",
    "        if file_size_mb > 500:  # 大于500MB进行采样\n",
    "            print(\"文件较大，进行采样加载...\")\n",
    "            self.data = self._load_large_file_sampled()\n",
    "        else:\n",
    "            print(\"直接加载文件...\")\n",
    "            self.data = self._load_file_direct()\n",
    "        \n",
    "        if self.data is None or len(self.data) == 0:\n",
    "            raise ValueError(\"数据加载失败，请检查文件格式和路径\")\n",
    "        \n",
    "        print(f\"数据加载完成 | 总行数: {len(self.data):,} | 列数: {len(self.data.columns)}\")\n",
    "        print(f\"数据列: {list(self.data.columns)}\")\n",
    "        \n",
    "        # 数据信息统计\n",
    "        self._analyze_data_structure()\n",
    "        \n",
    "        # 预处理\n",
    "        self._preprocess_data()\n",
    "        \n",
    "        # 创建用户分组数据（如果有用户标识）\n",
    "        self._create_grouped_data()\n",
    "    \n",
    "    def _load_large_file_sampled(self) -> pd.DataFrame:\n",
    "        \"\"\"采样加载大文件\"\"\"\n",
    "        try:\n",
    "            # 先读取小样本了解数据结构\n",
    "            sample_data = pd.read_parquet(self.data_path, nrows=10000)\n",
    "            print(f\"样本数据结构: {sample_data.shape}\")\n",
    "            print(f\"列名: {list(sample_data.columns)}\")\n",
    "            \n",
    "            # 计算总行数\n",
    "            try:\n",
    "                import pyarrow.parquet as pq\n",
    "                parquet_file = pq.ParquetFile(self.data_path)\n",
    "                total_rows = parquet_file.metadata.num_rows\n",
    "                print(f\"文件总行数: {total_rows:,}\")\n",
    "            except:\n",
    "                total_rows = None\n",
    "            \n",
    "            # 根据内存限制决定采样策略\n",
    "            if total_rows and total_rows > self.max_rows:\n",
    "                # 随机采样\n",
    "                sample_fraction = min(self.max_rows / total_rows, 1.0)\n",
    "                print(f\"采样比例: {sample_fraction:.2%}\")\n",
    "     \n",
    "        except Exception as e:\n",
    "            print(f\"读取文件失败: {e}\")\n",
    "            return None\n",
    "                \n",
    "    def _load_large_file_sampled(self) -> pd.DataFrame:\n",
    "        \"\"\"采样加载大文件\"\"\"\n",
    "        try:\n",
    "            # 先读取小样本了解数据结构\n",
    "            print(\"读取数据样本以了解结构...\")\n",
    "            \n",
    "            # 使用pyarrow尝试获取文件信息\n",
    "            try:\n",
    "                import pyarrow.parquet as pq\n",
    "                parquet_file = pq.ParquetFile(self.data_path)\n",
    "                total_rows = parquet_file.metadata.num_rows\n",
    "                print(f\"文件总行数: {total_rows:,}\")\n",
    "                \n",
    "                # 如果行数不大，直接读取\n",
    "                if total_rows <= self.max_rows:\n",
    "                    table = parquet_file.read()\n",
    "                    return table.to_pandas()\n",
    "                \n",
    "                # 否则进行采样\n",
    "                return self._sample_large_parquet(parquet_file, total_rows)\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"pyarrow未安装，使用pandas直接读取...\")\n",
    "                return self._load_file_direct()\n",
    "            except Exception as e:\n",
    "                print(f\"pyarrow读取失败: {e}\")\n",
    "                return self._load_file_direct()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"采样加载失败: {e}\")\n",
    "            return self._load_file_direct()\n",
    "    \n",
    "    def _sample_large_parquet(self, parquet_file, total_rows: int) -> pd.DataFrame:\n",
    "        \"\"\"对大型parquet文件进行采样\"\"\"\n",
    "        print(f\"对大文件进行采样 ({total_rows:,} -> {self.max_rows:,})\")\n",
    "        \n",
    "        try:\n",
    "            # 计算采样比例\n",
    "            sample_ratio = self.max_rows / total_rows\n",
    "            batch_size = min(self.chunk_size, total_rows // 10)  # 分成10批处理\n",
    "            \n",
    "            sampled_chunks = []\n",
    "            processed_rows = 0\n",
    "            \n",
    "            for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "                batch_df = batch.to_pandas()\n",
    "                processed_rows += len(batch_df)\n",
    "                \n",
    "                # 对每个批次进行采样\n",
    "                if len(batch_df) > 0:\n",
    "                    sample_size = max(1, int(len(batch_df) * sample_ratio))\n",
    "                    sampled_batch = batch_df.sample(n=sample_size, random_state=42)\n",
    "                    sampled_chunks.append(sampled_batch)\n",
    "                \n",
    "                # 显示进度\n",
    "                if processed_rows % (batch_size * 5) == 0:\n",
    "                    print(f\"已处理: {processed_rows:,}/{total_rows:,} ({processed_rows/total_rows:.1%})\")\n",
    "                \n",
    "                # 如果已经采样到足够数据，停止\n",
    "                current_sampled = sum(len(chunk) for chunk in sampled_chunks)\n",
    "                if current_sampled >= self.max_rows:\n",
    "                    break\n",
    "            \n",
    "            if sampled_chunks:\n",
    "                result = pd.concat(sampled_chunks, ignore_index=True)\n",
    "                # 确保不超过最大行数\n",
    "                if len(result) > self.max_rows:\n",
    "                    result = result.sample(n=self.max_rows, random_state=42)\n",
    "                print(f\"采样完成，最终数据量: {len(result):,}\")\n",
    "                return result\n",
    "            else:\n",
    "                print(\"采样失败，返回空DataFrame\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"采样过程出错: {e}\")\n",
    "            # 尝试直接读取一部分数据\n",
    "            try:\n",
    "                table = parquet_file.read(use_threads=False)\n",
    "                data = table.to_pandas()\n",
    "                if len(data) > self.max_rows:\n",
    "                    data = data.sample(n=self.max_rows, random_state=42)\n",
    "                return data\n",
    "            except:\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"采样加载失败: {e}\")\n",
    "            return self._load_file_direct()\n",
    "    \n",
    "    def _load_file_direct(self) -> pd.DataFrame:\n",
    "        \"\"\"直接加载文件\"\"\"\n",
    "        try:\n",
    "            # 先尝试读取全部数据\n",
    "            data = pd.read_parquet(self.data_path)\n",
    "            \n",
    "            # 如果数据量超过限制，进行采样\n",
    "            if len(data) > self.max_rows:\n",
    "                print(f\"数据量 ({len(data):,}) 超过限制 ({self.max_rows:,})，进行随机采样...\")\n",
    "                data = data.sample(n=self.max_rows, random_state=42)\n",
    "                print(f\"采样后数据量: {len(data):,}\")\n",
    "            \n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"直接加载失败: {e}\")\n",
    "    def _load_parquet_with_pyarrow(self) -> pd.DataFrame:\n",
    "        \"\"\"使用pyarrow加载parquet文件\"\"\"\n",
    "        try:\n",
    "            import pyarrow.parquet as pq\n",
    "            \n",
    "            # 读取parquet文件\n",
    "            parquet_file = pq.ParquetFile(self.data_path)\n",
    "            total_rows = parquet_file.metadata.num_rows\n",
    "            \n",
    "            print(f\"使用pyarrow读取，总行数: {total_rows:,}\")\n",
    "            \n",
    "            # 如果行数超过限制，按批次读取并采样\n",
    "            if total_rows > self.max_rows:\n",
    "                print(f\"分批读取并采样到 {self.max_rows:,} 行...\")\n",
    "                \n",
    "                # 计算采样比例\n",
    "                sample_ratio = self.max_rows / total_rows\n",
    "                batch_size = min(self.chunk_size, self.max_rows)\n",
    "                \n",
    "                sampled_data = []\n",
    "                for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "                    batch_df = batch.to_pandas()\n",
    "                    \n",
    "                    # 随机采样\n",
    "                    if len(batch_df) > 0:\n",
    "                        sample_size = max(1, int(len(batch_df) * sample_ratio))\n",
    "                        sampled_batch = batch_df.sample(n=sample_size, random_state=42)\n",
    "                        sampled_data.append(sampled_batch)\n",
    "                    \n",
    "                    # 如果已经采样到足够数据，停止\n",
    "                    if sum(len(df) for df in sampled_data) >= self.max_rows:\n",
    "                        break\n",
    "                \n",
    "                if sampled_data:\n",
    "                    result = pd.concat(sampled_data, ignore_index=True)\n",
    "                    # 确保不超过最大行数\n",
    "                    if len(result) > self.max_rows:\n",
    "                        result = result.sample(n=self.max_rows, random_state=42)\n",
    "                    return result\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "            else:\n",
    "                # 直接读取全部数据\n",
    "                table = parquet_file.read()\n",
    "                return table.to_pandas()\n",
    "                \n",
    "        except ImportError:\n",
    "            print(\"pyarrow未安装，尝试其他方法...\")\n",
    "            return self._load_parquet_fallback()\n",
    "        except Exception as e:\n",
    "            print(f\"pyarrow加载失败: {e}\")\n",
    "            return self._load_parquet_fallback()\n",
    "    \n",
    "    def _load_parquet_fallback(self) -> pd.DataFrame:\n",
    "        \"\"\"parquet文件最后的备用方法\"\"\"\n",
    "        try:\n",
    "            # 尝试读取一小部分数据\n",
    "            print(\"尝试读取数据样本...\")\n",
    "            \n",
    "            # 先读取很小的样本来了解数据结构\n",
    "            import tempfile\n",
    "            import os\n",
    "            \n",
    "            # 创建临时文件进行测试读取\n",
    "            data = pd.read_parquet(self.data_path)\n",
    "            \n",
    "            # 如果成功读取但数据量太大，进行采样\n",
    "            if len(data) > self.max_rows:\n",
    "                print(f\"数据量过大，从 {len(data):,} 行采样到 {self.max_rows:,} 行\")\n",
    "                data = data.sample(n=self.max_rows, random_state=42)\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"所有加载方法都失败了: {e}\")\n",
    "            print(\"请检查文件格式和完整性\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _analyze_data_structure(self):\n",
    "        \"\"\"分析数据结构\"\"\"\n",
    "        print(\"分析数据结构...\")\n",
    "        \n",
    "        self.data_info = {\n",
    "            'shape': self.data.shape,\n",
    "            'columns': list(self.data.columns),\n",
    "            'dtypes': self.data.dtypes.to_dict(),\n",
    "            'missing_values': self.data.isnull().sum().to_dict(),\n",
    "            'memory_usage_mb': self.data.memory_usage(deep=True).sum() / 1024**2\n",
    "        }\n",
    "        \n",
    "        print(f\"数据形状: {self.data_info['shape']}\")\n",
    "        print(f\"内存使用: {self.data_info['memory_usage_mb']:.1f} MB\")\n",
    "        print(f\"列数: {len(self.data_info['columns'])}\")\n",
    "        \n",
    "        # 检查关键列\n",
    "        key_columns = ['ranker_id', 'user_id', 'departure_datetime', 'booking_datetime', 'price', 'num_stops']\n",
    "        available_columns = [col for col in key_columns if col in self.data.columns]\n",
    "        missing_columns = [col for col in key_columns if col not in self.data.columns]\n",
    "        \n",
    "        print(f\"可用关键列: {available_columns}\")\n",
    "        if missing_columns:\n",
    "            print(f\"缺失关键列: {missing_columns}\")\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"预处理数据\"\"\"\n",
    "        print(\"预处理数据...\")\n",
    "        \n",
    "        # 数据类型优化\n",
    "        for col in self.data.columns:\n",
    "            if self.data[col].dtype == 'object':\n",
    "                try:\n",
    "                    # 尝试转换为数值\n",
    "                    self.data[col] = pd.to_numeric(self.data[col], errors='ignore')\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # 处理时间列\n",
    "        datetime_columns = ['departure_datetime', 'booking_datetime', 'arrival_datetime']\n",
    "        for col in datetime_columns:\n",
    "            if col in self.data.columns:\n",
    "                try:\n",
    "                    self.data[col] = pd.to_datetime(self.data[col], errors='coerce')\n",
    "                    print(f\"处理时间列: {col}\")\n",
    "                except:\n",
    "                    print(f\"时间列处理失败: {col}\")\n",
    "        \n",
    "        # 特征工程\n",
    "        if 'departure_datetime' in self.data.columns:\n",
    "            self.data['departure_hour'] = self.data['departure_datetime'].dt.hour\n",
    "            self.data['departure_day'] = self.data['departure_datetime'].dt.day_name()\n",
    "            self.data['departure_month'] = self.data['departure_datetime'].dt.month\n",
    "            self.data['is_weekend'] = self.data['departure_datetime'].dt.weekday >= 5\n",
    "        \n",
    "        if 'booking_datetime' in self.data.columns and 'departure_datetime' in self.data.columns:\n",
    "            self.data['advance_booking_days'] = (\n",
    "                self.data['departure_datetime'] - self.data['booking_datetime']\n",
    "            ).dt.days\n",
    "        \n",
    "        # 价格相关特征\n",
    "        if 'price' in self.data.columns:\n",
    "            self.data['price_level'] = pd.cut(\n",
    "                self.data['price'], \n",
    "                bins=5, \n",
    "                labels=['极低价', '低价', '中价', '高价', '极高价']\n",
    "            )\n",
    "        \n",
    "        # 航线复杂性\n",
    "        if 'num_stops' in self.data.columns:\n",
    "            self.data['flight_type'] = self.data['num_stops'].apply(\n",
    "                lambda x: '直飞' if x == 0 else ('一次中转' if x == 1 else '多次中转')\n",
    "            )\n",
    "        \n",
    "        print(\"数据预处理完成\")\n",
    "    \n",
    "    def _create_grouped_data(self):\n",
    "        \"\"\"创建用户分组数据\"\"\"\n",
    "        # 寻找用户标识列\n",
    "        user_id_col = 'ranker_id'\n",
    "        \n",
    "        if not user_id_col in self.data.columns:\n",
    "            print(\"警告: 未找到用户标识列，将进行整体分析\")\n",
    "            self.grouped_data = None\n",
    "            return\n",
    "        \n",
    "        print(f\"使用用户标识列: {user_id_col}\")\n",
    "        print(\"创建用户分组数据...\")\n",
    "        \n",
    "        # 按用户分组统计\n",
    "        user_stats = []\n",
    "        user_groups = self.data.groupby(user_id_col)\n",
    "        \n",
    "        for user_id, group in user_groups:\n",
    "            if len(group) >= 3:  # 只处理有足够数据的用户\n",
    "                stats = self._calculate_user_stats(group)\n",
    "                stats[user_id_col] = user_id\n",
    "                user_stats.append(stats)\n",
    "        \n",
    "        if user_stats:\n",
    "            self.grouped_data = pd.DataFrame(user_stats)\n",
    "            print(f\"创建用户分组数据完成 | 用户数: {len(self.grouped_data):,}\")\n",
    "        else:\n",
    "            print(\"警告: 没有足够的用户数据进行分组分析\")\n",
    "            self.grouped_data = None\n",
    "    \n",
    "    def _calculate_user_stats(self, group: pd.DataFrame) -> Dict:\n",
    "        \"\"\"计算用户统计信息\"\"\"\n",
    "        stats = {\n",
    "            'total_searches': len(group),\n",
    "            'avg_price': group['price'].mean() if 'price' in group.columns and not group['price'].isna().all() else 0,\n",
    "            'price_std': group['price'].std() if 'price' in group.columns and not group['price'].isna().all() else 0,\n",
    "            'price_sensitivity': self._calculate_price_sensitivity(group),\n",
    "            'preferred_departure_hour': group['departure_hour'].mode().iloc[0] if 'departure_hour' in group.columns and len(group['departure_hour'].dropna()) > 0 else 12,\n",
    "            'weekend_ratio': group['is_weekend'].mean() if 'is_weekend' in group.columns else 0,\n",
    "            'avg_advance_booking': group['advance_booking_days'].mean() if 'advance_booking_days' in group.columns and not group['advance_booking_days'].isna().all() else 0,\n",
    "            'direct_flight_ratio': (group['num_stops'] == 0).mean() if 'num_stops' in group.columns else 0,\n",
    "            'business_indicator': self._identify_business_traveler(group)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _calculate_price_sensitivity(self, group: pd.DataFrame) -> float:\n",
    "        \"\"\"计算价格敏感性\"\"\"\n",
    "        if 'price' not in group.columns or len(group) < 3 or group['price'].isna().all():\n",
    "            return 0.5\n",
    "        \n",
    "        price_data = group['price'].dropna()\n",
    "        if len(price_data) < 3:\n",
    "            return 0.5\n",
    "        \n",
    "        price_ranges = price_data.quantile([0.25, 0.75])\n",
    "        low_price_selections = len(price_data[price_data <= price_ranges[0.25]])\n",
    "        high_price_selections = len(price_data[price_data >= price_ranges[0.75]])\n",
    "        \n",
    "        if low_price_selections + high_price_selections == 0:\n",
    "            return 0.5\n",
    "        \n",
    "        return low_price_selections / (low_price_selections + high_price_selections)\n",
    "    \n",
    "    def _identify_business_traveler(self, group: pd.DataFrame) -> float:\n",
    "        \"\"\"识别商务旅行者\"\"\"\n",
    "        business_indicators = []\n",
    "        \n",
    "        # 工作日出行比例\n",
    "        if 'is_weekend' in group.columns:\n",
    "            weekday_ratio = 1 - group['is_weekend'].mean()\n",
    "            business_indicators.append(weekday_ratio)\n",
    "        \n",
    "        # 早班机偏好\n",
    "        if 'departure_hour' in group.columns:\n",
    "            early_flight_ratio = (group['departure_hour'] <= 8).mean()\n",
    "            business_indicators.append(early_flight_ratio)\n",
    "        \n",
    "        # 短期预订\n",
    "        if 'advance_booking_days' in group.columns:\n",
    "            short_booking_data = group['advance_booking_days'].dropna()\n",
    "            if len(short_booking_data) > 0:\n",
    "                short_booking_ratio = (short_booking_data <= 7).mean()\n",
    "                business_indicators.append(short_booking_ratio)\n",
    "        \n",
    "        # 直飞偏好\n",
    "        if 'num_stops' in group.columns:\n",
    "            direct_ratio = (group['num_stops'] == 0).mean()\n",
    "            business_indicators.append(direct_ratio)\n",
    "        \n",
    "        return np.mean(business_indicators) if business_indicators else 0.5\n",
    "    \n",
    "    @memory_monitor\n",
    "    def analyze_overall_patterns(self):\n",
    "        \"\"\"分析整体模式\"\"\"\n",
    "        print(\"分析整体模式...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # 价格分析\n",
    "        if 'price' in self.data.columns:\n",
    "            price_data = self.data['price'].dropna()\n",
    "            patterns['price_analysis'] = {\n",
    "                'mean': price_data.mean(),\n",
    "                'median': price_data.median(),\n",
    "                'std': price_data.std(),\n",
    "                'min': price_data.min(),\n",
    "                'max': price_data.max(),\n",
    "                'q25': price_data.quantile(0.25),\n",
    "                'q75': price_data.quantile(0.75)\n",
    "            }\n",
    "        \n",
    "        # 时间模式分析\n",
    "        if 'departure_hour' in self.data.columns:\n",
    "            patterns['time_patterns'] = {\n",
    "                'peak_hours': self.data['departure_hour'].value_counts().head(5).to_dict(),\n",
    "                'weekend_vs_weekday': self.data['is_weekend'].value_counts().to_dict() if 'is_weekend' in self.data.columns else {}\n",
    "            }\n",
    "        \n",
    "        # 航线类型分析\n",
    "        if 'num_stops' in self.data.columns:\n",
    "            patterns['flight_type_analysis'] = {\n",
    "                'direct_flights_ratio': (self.data['num_stops'] == 0).mean(),\n",
    "                'stops_distribution': self.data['num_stops'].value_counts().to_dict()\n",
    "            }\n",
    "        \n",
    "        # 预订模式分析\n",
    "        if 'advance_booking_days' in self.data.columns:\n",
    "            booking_data = self.data['advance_booking_days'].dropna()\n",
    "            patterns['booking_patterns'] = {\n",
    "                'avg_advance_days': booking_data.mean(),\n",
    "                'median_advance_days': booking_data.median(),\n",
    "                'last_minute_ratio': (booking_data <= 1).mean(),\n",
    "                'planned_booking_ratio': (booking_data >= 14).mean()\n",
    "            }\n",
    "        \n",
    "        # 可视化\n",
    "        self._create_overall_pattern_plots(patterns)\n",
    "        \n",
    "        self.analysis_results['overall_patterns'] = patterns\n",
    "        print(\"整体模式分析完成\")\n",
    "    \n",
    "    def _create_overall_pattern_plots(self, patterns: Dict):\n",
    "        \"\"\"创建整体模式图表\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('价格分布', '出发时间分布', '航班类型分布', '预订提前天数分布'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # 价格分布\n",
    "        if 'price' in self.data.columns:\n",
    "            price_data = self.data['price'].dropna()\n",
    "            fig.add_trace(go.Histogram(x=price_data, name='价格分布', nbinsx=50), row=1, col=1)\n",
    "        \n",
    "        # 出发时间分布\n",
    "        if 'departure_hour' in self.data.columns:\n",
    "            hour_counts = self.data['departure_hour'].value_counts().sort_index()\n",
    "            fig.add_trace(go.Bar(x=hour_counts.index, y=hour_counts.values, name='出发时间'), row=1, col=2)\n",
    "        \n",
    "        # 航班类型分布\n",
    "        if 'num_stops' in self.data.columns:\n",
    "            stops_counts = self.data['num_stops'].value_counts().sort_index()\n",
    "            fig.add_trace(go.Bar(x=stops_counts.index, y=stops_counts.values, name='中转次数'), row=2, col=1)\n",
    "        \n",
    "        # 预订提前天数分布\n",
    "        if 'advance_booking_days' in self.data.columns:\n",
    "            booking_data = self.data['advance_booking_days'].dropna()\n",
    "            fig.add_trace(go.Histogram(x=booking_data, name='预订提前天数', nbinsx=50), row=2, col=2)\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"整体模式分析\")\n",
    "        fig.write_html(f\"{self.output_dir}/plots/overall_patterns.html\")\n",
    "    \n",
    "    @memory_monitor\n",
    "    def analyze_user_behavior(self):\n",
    "        \"\"\"分析用户行为（如果有用户数据）\"\"\"\n",
    "        if self.grouped_data is None:\n",
    "            print(\"跳过用户行为分析 - 无用户分组数据\")\n",
    "            return\n",
    "        \n",
    "        print(\"分析用户行为...\")\n",
    "        \n",
    "        # 用户分类\n",
    "        if 'business_indicator' in self.grouped_data.columns:\n",
    "            threshold = self.grouped_data['business_indicator'].median()\n",
    "            business_users = self.grouped_data[self.grouped_data['business_indicator'] >= threshold]\n",
    "            leisure_users = self.grouped_data[self.grouped_data['business_indicator'] < threshold]\n",
    "            \n",
    "            behavior_analysis = {\n",
    "                'business_users': {\n",
    "                    'count': len(business_users),\n",
    "                    'avg_price': business_users['avg_price'].mean(),\n",
    "                    'price_sensitivity': business_users['price_sensitivity'].mean(),\n",
    "                    'advance_booking': business_users['avg_advance_booking'].mean(),\n",
    "                    'direct_flight_ratio': business_users['direct_flight_ratio'].mean(),\n",
    "                    'weekend_ratio': business_users['weekend_ratio'].mean()\n",
    "                },\n",
    "                'leisure_users': {\n",
    "                    'count': len(leisure_users),\n",
    "                    'avg_price': leisure_users['avg_price'].mean(),\n",
    "                    'price_sensitivity': leisure_users['price_sensitivity'].mean(),\n",
    "                    'advance_booking': leisure_users['avg_advance_booking'].mean(),\n",
    "                    'direct_flight_ratio': leisure_users['direct_flight_ratio'].mean(),\n",
    "                    'weekend_ratio': leisure_users['weekend_ratio'].mean()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # 可视化\n",
    "            self._create_user_behavior_plots(business_users, leisure_users)\n",
    "            \n",
    "            self.analysis_results['user_behavior'] = behavior_analysis\n",
    "            \n",
    "        print(\"用户行为分析完成\")\n",
    "    \n",
    "    def _create_user_behavior_plots(self, business_users: pd.DataFrame, leisure_users: pd.DataFrame):\n",
    "        \"\"\"创建用户行为图表\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('价格敏感性对比', '平均价格对比', '提前预订天数对比', '直飞偏好对比'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # 价格敏感性\n",
    "        fig.add_trace(go.Histogram(x=business_users['price_sensitivity'], name='商务用户', \n",
    "                                 opacity=0.7, nbinsx=20), row=1, col=1)\n",
    "        fig.add_trace(go.Histogram(x=leisure_users['price_sensitivity'], name='休闲用户', \n",
    "                                 opacity=0.7, nbinsx=20), row=1, col=1)\n",
    "        \n",
    "        # 平均价格\n",
    "        categories = ['商务用户', '休闲用户']\n",
    "        avg_prices = [business_users['avg_price'].mean(), leisure_users['avg_price'].mean()]\n",
    "        fig.add_trace(go.Bar(x=categories, y=avg_prices, name='平均价格'), row=1, col=2)\n",
    "        \n",
    "        # 提前预订\n",
    "        fig.add_trace(go.Histogram(x=business_users['avg_advance_booking'], name='商务用户', \n",
    "                                 opacity=0.7, nbinsx=20), row=2, col=1)\n",
    "        fig.add_trace(go.Histogram(x=leisure_users['avg_advance_booking'], name='休闲用户', \n",
    "                                 opacity=0.7, nbinsx=20), row=2, col=1)\n",
    "        \n",
    "        # 直飞偏好\n",
    "        direct_ratios = [business_users['direct_flight_ratio'].mean(), leisure_users['direct_flight_ratio'].mean()]\n",
    "        fig.add_trace(go.Bar(x=categories, y=direct_ratios, name='直飞比例'), row=2, col=2)\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"用户行为对比分析\")\n",
    "        fig.write_html(f\"{self.output_dir}/plots/user_behavior_analysis.html\")\n",
    "    \n",
    "    @memory_monitor\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"生成综合分析报告\"\"\"\n",
    "        print(\"生成综合分析报告...\")\n",
    "        \n",
    "        report = f\"\"\"# 航班数据分析报告\n",
    "\n",
    "## 数据概况\n",
    "- 数据文件: {self.data_path}\n",
    "- 记录数: {len(self.data):,}\n",
    "- 内存使用: {self.data_info['memory_usage_mb']:.1f} MB\n",
    "- 分析时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## 数据结构\n",
    "- 数据维度: {self.data_info['shape']}\n",
    "- 列数: {len(self.data_info['columns'])}\n",
    "- 主要列: {', '.join(self.data_info['columns'][:10])}\n",
    "\n",
    "## 主要发现\n",
    "\n",
    "### 1. 整体模式分析\n",
    "\"\"\"\n",
    "        \n",
    "        if 'overall_patterns' in self.analysis_results:\n",
    "            patterns = self.analysis_results['overall_patterns']\n",
    "            \n",
    "            if 'price_analysis' in patterns:\n",
    "                price_info = patterns['price_analysis']\n",
    "                report += f\"\"\"\n",
    "**价格分析:**\n",
    "- 平均价格: {price_info['mean']:.2f}\n",
    "- 中位价格: {price_info['median']:.2f}\n",
    "- 价格标准差: {price_info['std']:.2f}\n",
    "- 价格范围: {price_info['min']:.2f} - {price_info['max']:.2f}\n",
    "\"\"\"\n",
    "            \n",
    "            if 'flight_type_analysis' in patterns:\n",
    "                flight_info = patterns['flight_type_analysis']\n",
    "                report += f\"\"\"\n",
    "**航班类型分析:**\n",
    "- 直飞比例: {flight_info['direct_flights_ratio']:.2%}\n",
    "- 中转分布: {flight_info['stops_distribution']}\n",
    "\"\"\"\n",
    "            \n",
    "            if 'booking_patterns' in patterns:\n",
    "                booking_info = patterns['booking_patterns']\n",
    "                report += f\"\"\"\n",
    "**预订模式分析:**\n",
    "- 平均提前预订: {booking_info['avg_advance_days']:.1f} 天\n",
    "- 临时预订比例: {booking_info['last_minute_ratio']:.2%}\n",
    "- 计划预订比例: {booking_info['planned_booking_ratio']:.2%}\n",
    "\"\"\"\n",
    "        \n",
    "        if 'user_behavior' in self.analysis_results:\n",
    "            behavior = self.analysis_results['user_behavior']\n",
    "            report += f\"\"\"\n",
    "### 2. 用户行为分析\n",
    "\n",
    "**商务用户 ({behavior['business_users']['count']}人):**\n",
    "- 平均价格: {behavior['business_users']['avg_price']:.2f}\n",
    "- 价格敏感性: {behavior['business_users']['price_sensitivity']:.2f}\n",
    "- 提前预订: {behavior['business_users']['advance_booking']:.1f} 天\n",
    "- 直飞偏好: {behavior['business_users']['direct_flight_ratio']:.2%}\n",
    "\n",
    "**休闲用户 ({behavior['leisure_users']['count']}人):**\n",
    "- 平均价格: {behavior['leisure_users']['avg_price']:.2f}\n",
    "- 价格敏感性: {behavior['leisure_users']['price_sensitivity']:.2f}\n",
    "- 提前预订: {behavior['leisure_users']['advance_booking']:.1f} 天\n",
    "- 直飞偏好: {behavior['leisure_users']['direct_flight_ratio']:.2%}\n",
    "\"\"\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "## 商业建议\n",
    "\n",
    "### 整体策略\n",
    "1. 优化价格策略，关注价格敏感用户群体\n",
    "2. 提供多样化的航班选择（直飞/中转）\n",
    "3. 针对不同预订习惯制定差异化服务\n",
    "\n",
    "### 技术建议\n",
    "1. 改进搜索排序算法，考虑用户历史行为\n",
    "2. 实施个性化推荐系统\n",
    "3. 优化移动端预订流程\n",
    "\n",
    "---\n",
    "*报告生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "*数据来源: {self.data_path}*\n",
    "\"\"\"\n",
    "        \n",
    "        # 保存报告\n",
    "        report_path = f\"{self.output_dir}/reports/comprehensive_analysis.md\"\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"综合报告已保存到: {report_path}\")\n",
    "    \n",
    "    def run_analysis(self):\n",
    "        \"\"\"运行完整分析流程\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"开始航班数据分析...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # 1. 数据加载和预处理\n",
    "            self.load_and_preprocess_data()\n",
    "            \n",
    "            # 2. 整体模式分析\n",
    "            self.analyze_overall_patterns()\n",
    "            \n",
    "            # 3. 用户行为分析（如果有用户数据）\n",
    "            self.analyze_user_behavior()\n",
    "            \n",
    "            # 4. 生成综合报告\n",
    "            self.generate_comprehensive_report()\n",
    "            \n",
    "            print(\"=\" * 60)\n",
    "            print(\"分析完成！\")\n",
    "            print(\"=\" * 60)\n",
    "            print(\"查看结果:\")\n",
    "            print(f\"- 报告: {self.output_dir}/reports/\")\n",
    "            print(f\"- 图表: {self.output_dir}/plots/\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"分析过程中出现错误: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # 清理内存\n",
    "            self.cleanup()\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"清理内存\"\"\"\n",
    "        if hasattr(self, 'data'):\n",
    "            del self.data\n",
    "        if hasattr(self, 'grouped_data'):\n",
    "            del self.grouped_data\n",
    "        gc.collect()\n",
    "        print(\"内存清理完成\")\n",
    "\n",
    "# 便捷函数\n",
    "def analyze_flight_data(data_path: str, output_dir: str = \"flight_analysis\", max_rows: int = 1000000):\n",
    "    \"\"\"分析航班数据的便捷函数\"\"\"\n",
    "    analyzer = EnhancedFlightDataAnalyzer(data_path, output_dir, max_rows)\n",
    "    analyzer.run_analysis()\n",
    "    return analyzer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 首先测试数据加载\n",
    "    print(\"=\" * 60)\n",
    "    print(\"测试数据文件\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_files = [\n",
    "        \"../data/test.parquet\",\n",
    "        \"../data/train.parquet\"\n",
    "    ]\n",
    "    \n",
    "    # 示例1: 分析测试数据\n",
    "    print(\"=\" * 60)\n",
    "    print(\"分析测试数据\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        test_analyzer = analyze_flight_data(\n",
    "            data_path=\"../data/test.parquet\",\n",
    "            output_dir=\"../data/test_flight_analysis_results\",\n",
    "            max_rows=500000  # 限制最大行数以控制内存\n",
    "        )\n",
    "        print(\"✅ 测试数据分析完成！\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 测试数据分析失败: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"分析训练数据\")\n",
    "    print(\"=\" * 60)\n",
    "    # 示例2: 分析训练数据\n",
    "    try:\n",
    "        train_analyzer = analyze_flight_data(\n",
    "            data_path=\"../data/train.parquet\",\n",
    "            output_dir=\"../data/train_flight_analysis_results\",\n",
    "            max_rows=500000  # 限制最大行数以控制内存\n",
    "        )\n",
    "        print(\"✅ 训练数据分析完成！\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 训练数据分析失败: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"分析完成，请查看输出目录中的结果\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# èˆªç­æ’åç«èµ› - ä¸šåŠ¡é€»è¾‘ç†è§£ä¸ç‰¹å¾åˆ†æ\n",
    "## Flight Ranking Competition - Business Logic Understanding & Feature Analysis\n",
    "\n",
    "#### 1. **æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†**\n",
    "- è‡ªåŠ¨ç§»é™¤ç¼ºå¤±å€¼è¶…è¿‡80%çš„ç‰¹å¾\n",
    "- æŒ‰`ranker_id`åˆ†ç»„åˆ†æï¼Œåªå¤„ç†æœ‰è¶³å¤Ÿæ•°æ®çš„æœç´¢ä¼šè¯\n",
    "- å¯¹ä¿ç•™ç‰¹å¾è¿›è¡Œåˆç†å¡«å……ç­–ç•¥\n",
    "\n",
    "#### 2. **åˆ†æç»´åº¦**\n",
    "- å•†åŠ¡vsä¼‘é—²æ—…è¡Œè€…æ·±åº¦å¯¹æ¯”\n",
    "- ä»·æ ¼æ•æ„Ÿæ€§å’Œæ”¯ä»˜æ„æ„¿åˆ†æ\n",
    "- æ—¶é—´åå¥½æ¨¡å¼ï¼ˆå‡ºå‘æ—¶é—´ã€é¢„è®¢æ—¶é—´ï¼‰\n",
    "- èˆªçº¿å¤æ‚æ€§åå¥½ï¼ˆç›´é£vsä¸­è½¬ï¼‰\n",
    "- èˆªç©ºå…¬å¸å’Œæœºåœºåå¥½\n",
    "- ç”¨æˆ·èšç±»å’Œå¿ è¯šåº¦åˆ†æ\n",
    "- ä¼ä¸šå·®æ—…æ”¿ç­–å½±å“\n",
    "- èˆ±ä½ç­‰çº§åå¥½\n",
    "- æå‰é¢„è®¢è¡Œä¸ºåˆ†æ\n",
    "- å¾€è¿”vså•ç¨‹åå¥½\n",
    "- ç”¨æˆ·è½¬åŒ–ç‡åˆ†æ\n",
    "- æ‹“å±•...\n",
    "\n",
    "#### 3. **åŠŸèƒ½**\n",
    "- **ç”¨æˆ·ç”»åƒç³»ç»Ÿ**ï¼šä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºè¯¦ç»†çš„åå¥½æ¡£æ¡ˆ\n",
    "- **æ™ºèƒ½èšç±»**ï¼šå°†ç”¨æˆ·åˆ†ä¸º4ä¸ªä¸»è¦ç¾¤ä½“\n",
    "- **äº¤äº’å¼å¯è§†åŒ–**ï¼šä½¿ç”¨Plotlyåˆ›å»ºåŠ¨æ€å›¾è¡¨\n",
    "- **å•†ä¸šæ´å¯Ÿ**ï¼šç”Ÿæˆå¯æ‰§è¡Œçš„å•†ä¸šå»ºè®®\n",
    "\n",
    "### ğŸ“Š ç³»ç»Ÿæ¶æ„\n",
    "\n",
    "```\n",
    "EnhancedFlightDataAnalyzer\n",
    "â”œâ”€â”€ æ•°æ®åŠ è½½ä¸é¢„å¤„ç†\n",
    "â”œâ”€â”€ åˆ†ç»„æ•°æ®åˆ›å»º (æŒ‰ranker_id)\n",
    "â”œâ”€â”€ ç”¨æˆ·ç”»åƒç”Ÿæˆ\n",
    "â”œâ”€â”€ å¤šç»´åº¦åˆ†ææ¨¡å—\n",
    "â”‚   â”œâ”€â”€ å•†åŠ¡vsä¼‘é—²åˆ†æ\n",
    "â”‚   â”œâ”€â”€ ä»·æ ¼æ•æ„Ÿæ€§åˆ†æ\n",
    "â”‚   â”œâ”€â”€ æ—¶é—´åå¥½åˆ†æ\n",
    "â”‚   â”œâ”€â”€ èˆªçº¿åå¥½åˆ†æ\n",
    "â”‚   â””â”€â”€ ç”¨æˆ·èšç±»åˆ†æ\n",
    "â””â”€â”€ ç»¼åˆæŠ¥å‘Šç”Ÿæˆ\n",
    "```\n",
    "\n",
    "### ğŸš€ ä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "```python\n",
    "# åˆå§‹åŒ–åˆ†æå™¨\n",
    "analyzer = EnhancedFlightDataAnalyzer(\n",
    "    train_path='data/train.parquet',\n",
    "    test_path='data/test.parquet'\n",
    ")\n",
    "\n",
    "# è¿è¡Œå®Œæ•´åˆ†æ\n",
    "analyzer.run_full_analysis()\n",
    "\n",
    "# æˆ–è€…è¿è¡Œå•ä¸ªåˆ†ææ¨¡å—\n",
    "analyzer.load_and_preprocess_data()\n",
    "analyzer.analyze_business_vs_leisure_detailed()\n",
    "analyzer.analyze_price_sensitivity()\n",
    "```\n",
    "\n",
    "### ğŸ¯ å…³é”®æ´å¯Ÿèƒ½åŠ›\n",
    "\n",
    "1. **ç”¨æˆ·åˆ†ç¾¤**ï¼šè‡ªåŠ¨è¯†åˆ«é«˜ä»·å€¼å•†åŠ¡ç”¨æˆ·å’Œä»·æ ¼æ•æ„Ÿä¼‘é—²ç”¨æˆ·\n",
    "2. **åå¥½é¢„æµ‹**ï¼šåŸºäºå†å²è¡Œä¸ºé¢„æµ‹ç”¨æˆ·é€‰æ‹©å€¾å‘\n",
    "3. **ä¸ªæ€§åŒ–æ¨è**ï¼šä¸ºä¸åŒç”¨æˆ·ç¾¤ä½“æä¾›å®šåˆ¶åŒ–çš„èˆªç­æ¨èç­–ç•¥\n",
    "4. **è½¬åŒ–ä¼˜åŒ–**ï¼šè¯†åˆ«å½±å“ç”¨æˆ·é€‰æ‹©çš„å…³é”®å› ç´ \n",
    "\n",
    "### ğŸ“ˆ è¾“å‡ºæŠ¥å‘Š\n",
    "\n",
    "ç³»ç»Ÿä¼šç”Ÿæˆï¼š\n",
    "- å¤šç»´åº¦å¯è§†åŒ–å›¾è¡¨\n",
    "- ç”¨æˆ·èšç±»åˆ†æç»“æœ\n",
    "- å•†ä¸šæ´å¯Ÿå’Œå»ºè®®\n",
    "- è¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Šæ–‡ä»¶\n",
    "\n",
    "è¿™ä¸ªç³»ç»Ÿç‰¹åˆ«é€‚åˆï¼š\n",
    "- èˆªç­æ¨èç³»ç»Ÿä¼˜åŒ–\n",
    "- ç”¨æˆ·è¡Œä¸ºåˆ†æ\n",
    "- ä¸ªæ€§åŒ–è¥é”€ç­–ç•¥åˆ¶å®š\n",
    "- äº§å“åŠŸèƒ½æ”¹è¿›å†³ç­–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "æµ‹è¯•æ•°æ®æ–‡ä»¶\n",
      "============================================================\n",
      "============================================================\n",
      "åˆ†ææµ‹è¯•æ•°æ®\n",
      "============================================================\n",
      "èˆªç­æ•°æ®åˆ†æå™¨å·²åˆå§‹åŒ–\n",
      "- æ•°æ®æ–‡ä»¶: ../data/test.parquet\n",
      "- è¾“å‡ºç›®å½•: ../data/test_flight_analysis_results\n",
      "- æœ€å¤§å¤„ç†è¡Œæ•°: 500,000\n",
      "============================================================\n",
      "å¼€å§‹èˆªç­æ•°æ®åˆ†æ...\n",
      "============================================================\n",
      "å¼€å§‹åŠ è½½æ•°æ®...\n",
      "æ–‡ä»¶å¤§å°: 137.5 MB\n",
      "ç›´æ¥åŠ è½½æ–‡ä»¶...\n",
      "æ•°æ®é‡ (6,897,776) è¶…è¿‡é™åˆ¶ (500,000)ï¼Œè¿›è¡Œéšæœºé‡‡æ ·...\n",
      "é‡‡æ ·åæ•°æ®é‡: 500,000\n",
      "æ•°æ®åŠ è½½å®Œæˆ | æ€»è¡Œæ•°: 500,000 | åˆ—æ•°: 125\n",
      "æ•°æ®åˆ—: ['Id', 'bySelf', 'companyID', 'corporateTariffCode', 'frequentFlyer', 'nationality', 'isAccess3D', 'isVip', 'legs0_arrivalAt', 'legs0_departureAt', 'legs0_duration', 'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata', 'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_baggageAllowance_quantity', 'legs0_segments0_baggageAllowance_weightMeasurementType', 'legs0_segments0_cabinClass', 'legs0_segments0_departureFrom_airport_iata', 'legs0_segments0_duration', 'legs0_segments0_flightNumber', 'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code', 'legs0_segments0_seatsAvailable', 'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata', 'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_baggageAllowance_quantity', 'legs0_segments1_baggageAllowance_weightMeasurementType', 'legs0_segments1_cabinClass', 'legs0_segments1_departureFrom_airport_iata', 'legs0_segments1_duration', 'legs0_segments1_flightNumber', 'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code', 'legs0_segments1_seatsAvailable', 'legs0_segments2_aircraft_code', 'legs0_segments2_arrivalTo_airport_city_iata', 'legs0_segments2_arrivalTo_airport_iata', 'legs0_segments2_baggageAllowance_quantity', 'legs0_segments2_baggageAllowance_weightMeasurementType', 'legs0_segments2_cabinClass', 'legs0_segments2_departureFrom_airport_iata', 'legs0_segments2_duration', 'legs0_segments2_flightNumber', 'legs0_segments2_marketingCarrier_code', 'legs0_segments2_operatingCarrier_code', 'legs0_segments2_seatsAvailable', 'legs0_segments3_aircraft_code', 'legs0_segments3_arrivalTo_airport_city_iata', 'legs0_segments3_arrivalTo_airport_iata', 'legs0_segments3_baggageAllowance_quantity', 'legs0_segments3_baggageAllowance_weightMeasurementType', 'legs0_segments3_cabinClass', 'legs0_segments3_departureFrom_airport_iata', 'legs0_segments3_duration', 'legs0_segments3_flightNumber', 'legs0_segments3_marketingCarrier_code', 'legs0_segments3_operatingCarrier_code', 'legs0_segments3_seatsAvailable', 'legs1_arrivalAt', 'legs1_departureAt', 'legs1_duration', 'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata', 'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_baggageAllowance_quantity', 'legs1_segments0_baggageAllowance_weightMeasurementType', 'legs1_segments0_cabinClass', 'legs1_segments0_departureFrom_airport_iata', 'legs1_segments0_duration', 'legs1_segments0_flightNumber', 'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code', 'legs1_segments0_seatsAvailable', 'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata', 'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_baggageAllowance_quantity', 'legs1_segments1_baggageAllowance_weightMeasurementType', 'legs1_segments1_cabinClass', 'legs1_segments1_departureFrom_airport_iata', 'legs1_segments1_duration', 'legs1_segments1_flightNumber', 'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code', 'legs1_segments1_seatsAvailable', 'legs1_segments2_aircraft_code', 'legs1_segments2_arrivalTo_airport_city_iata', 'legs1_segments2_arrivalTo_airport_iata', 'legs1_segments2_baggageAllowance_quantity', 'legs1_segments2_baggageAllowance_weightMeasurementType', 'legs1_segments2_cabinClass', 'legs1_segments2_departureFrom_airport_iata', 'legs1_segments2_duration', 'legs1_segments2_flightNumber', 'legs1_segments2_marketingCarrier_code', 'legs1_segments2_operatingCarrier_code', 'legs1_segments2_seatsAvailable', 'legs1_segments3_aircraft_code', 'legs1_segments3_arrivalTo_airport_city_iata', 'legs1_segments3_arrivalTo_airport_iata', 'legs1_segments3_baggageAllowance_quantity', 'legs1_segments3_baggageAllowance_weightMeasurementType', 'legs1_segments3_cabinClass', 'legs1_segments3_departureFrom_airport_iata', 'legs1_segments3_duration', 'legs1_segments3_flightNumber', 'legs1_segments3_marketingCarrier_code', 'legs1_segments3_operatingCarrier_code', 'legs1_segments3_seatsAvailable', 'miniRules0_monetaryAmount', 'miniRules0_percentage', 'miniRules0_statusInfos', 'miniRules1_monetaryAmount', 'miniRules1_percentage', 'miniRules1_statusInfos', 'pricingInfo_isAccessTP', 'pricingInfo_passengerCount', 'profileId', 'ranker_id', 'requestDate', 'searchRoute', 'sex', 'taxes', 'totalPrice']\n",
      "åˆ†ææ•°æ®ç»“æ„...\n",
      "æ•°æ®å½¢çŠ¶: (500000, 125)\n",
      "å†…å­˜ä½¿ç”¨: 1519.8 MB\n",
      "åˆ—æ•°: 125\n",
      "å¯ç”¨å…³é”®åˆ—: ['ranker_id']\n",
      "ç¼ºå¤±å…³é”®åˆ—: ['user_id', 'departure_datetime', 'booking_datetime', 'price', 'num_stops']\n",
      "é¢„å¤„ç†æ•°æ®...\n",
      "æ•°æ®é¢„å¤„ç†å®Œæˆ\n",
      "ä½¿ç”¨ç”¨æˆ·æ ‡è¯†åˆ—: ranker_id\n",
      "åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®...\n",
      "åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®å®Œæˆ | ç”¨æˆ·æ•°: 23,200\n",
      "[load_and_preprocess_data] è€—æ—¶: 18.94s | å†…å­˜: 6738.3MB (+5913.9MB)\n",
      "åˆ†ææ•´ä½“æ¨¡å¼...\n",
      "æ•´ä½“æ¨¡å¼åˆ†æå®Œæˆ\n",
      "[analyze_overall_patterns] è€—æ—¶: 0.04s | å†…å­˜: 6738.4MB (+0.1MB)\n",
      "åˆ†æç”¨æˆ·è¡Œä¸º...\n",
      "ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆ\n",
      "[analyze_user_behavior] è€—æ—¶: 0.06s | å†…å­˜: 6742.7MB (+4.3MB)\n",
      "ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...\n",
      "ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: ../data/test_flight_analysis_results/reports/comprehensive_analysis.md\n",
      "[generate_comprehensive_report] è€—æ—¶: 0.00s | å†…å­˜: 6742.7MB (+0.0MB)\n",
      "============================================================\n",
      "åˆ†æå®Œæˆï¼\n",
      "============================================================\n",
      "æŸ¥çœ‹ç»“æœ:\n",
      "- æŠ¥å‘Š: ../data/test_flight_analysis_results/reports/\n",
      "- å›¾è¡¨: ../data/test_flight_analysis_results/plots/\n",
      "å†…å­˜æ¸…ç†å®Œæˆ\n",
      "âœ… æµ‹è¯•æ•°æ®åˆ†æå®Œæˆï¼\n",
      "\n",
      "============================================================\n",
      "åˆ†æè®­ç»ƒæ•°æ®\n",
      "============================================================\n",
      "èˆªç­æ•°æ®åˆ†æå™¨å·²åˆå§‹åŒ–\n",
      "- æ•°æ®æ–‡ä»¶: ../data/train.parquet\n",
      "- è¾“å‡ºç›®å½•: ../data/train_flight_analysis_results\n",
      "- æœ€å¤§å¤„ç†è¡Œæ•°: 500,000\n",
      "============================================================\n",
      "å¼€å§‹èˆªç­æ•°æ®åˆ†æ...\n",
      "============================================================\n",
      "å¼€å§‹åŠ è½½æ•°æ®...\n",
      "æ–‡ä»¶å¤§å°: 376.6 MB\n",
      "ç›´æ¥åŠ è½½æ–‡ä»¶...\n",
      "æ•°æ®é‡ (18,145,372) è¶…è¿‡é™åˆ¶ (500,000)ï¼Œè¿›è¡Œéšæœºé‡‡æ ·...\n",
      "é‡‡æ ·åæ•°æ®é‡: 500,000\n",
      "æ•°æ®åŠ è½½å®Œæˆ | æ€»è¡Œæ•°: 500,000 | åˆ—æ•°: 126\n",
      "æ•°æ®åˆ—: ['Id', 'bySelf', 'companyID', 'corporateTariffCode', 'frequentFlyer', 'nationality', 'isAccess3D', 'isVip', 'legs0_arrivalAt', 'legs0_departureAt', 'legs0_duration', 'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata', 'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_baggageAllowance_quantity', 'legs0_segments0_baggageAllowance_weightMeasurementType', 'legs0_segments0_cabinClass', 'legs0_segments0_departureFrom_airport_iata', 'legs0_segments0_duration', 'legs0_segments0_flightNumber', 'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code', 'legs0_segments0_seatsAvailable', 'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata', 'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_baggageAllowance_quantity', 'legs0_segments1_baggageAllowance_weightMeasurementType', 'legs0_segments1_cabinClass', 'legs0_segments1_departureFrom_airport_iata', 'legs0_segments1_duration', 'legs0_segments1_flightNumber', 'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code', 'legs0_segments1_seatsAvailable', 'legs0_segments2_aircraft_code', 'legs0_segments2_arrivalTo_airport_city_iata', 'legs0_segments2_arrivalTo_airport_iata', 'legs0_segments2_baggageAllowance_quantity', 'legs0_segments2_baggageAllowance_weightMeasurementType', 'legs0_segments2_cabinClass', 'legs0_segments2_departureFrom_airport_iata', 'legs0_segments2_duration', 'legs0_segments2_flightNumber', 'legs0_segments2_marketingCarrier_code', 'legs0_segments2_operatingCarrier_code', 'legs0_segments2_seatsAvailable', 'legs0_segments3_aircraft_code', 'legs0_segments3_arrivalTo_airport_city_iata', 'legs0_segments3_arrivalTo_airport_iata', 'legs0_segments3_baggageAllowance_quantity', 'legs0_segments3_baggageAllowance_weightMeasurementType', 'legs0_segments3_cabinClass', 'legs0_segments3_departureFrom_airport_iata', 'legs0_segments3_duration', 'legs0_segments3_flightNumber', 'legs0_segments3_marketingCarrier_code', 'legs0_segments3_operatingCarrier_code', 'legs0_segments3_seatsAvailable', 'legs1_arrivalAt', 'legs1_departureAt', 'legs1_duration', 'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata', 'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_baggageAllowance_quantity', 'legs1_segments0_baggageAllowance_weightMeasurementType', 'legs1_segments0_cabinClass', 'legs1_segments0_departureFrom_airport_iata', 'legs1_segments0_duration', 'legs1_segments0_flightNumber', 'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code', 'legs1_segments0_seatsAvailable', 'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata', 'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_baggageAllowance_quantity', 'legs1_segments1_baggageAllowance_weightMeasurementType', 'legs1_segments1_cabinClass', 'legs1_segments1_departureFrom_airport_iata', 'legs1_segments1_duration', 'legs1_segments1_flightNumber', 'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code', 'legs1_segments1_seatsAvailable', 'legs1_segments2_aircraft_code', 'legs1_segments2_arrivalTo_airport_city_iata', 'legs1_segments2_arrivalTo_airport_iata', 'legs1_segments2_baggageAllowance_quantity', 'legs1_segments2_baggageAllowance_weightMeasurementType', 'legs1_segments2_cabinClass', 'legs1_segments2_departureFrom_airport_iata', 'legs1_segments2_duration', 'legs1_segments2_flightNumber', 'legs1_segments2_marketingCarrier_code', 'legs1_segments2_operatingCarrier_code', 'legs1_segments2_seatsAvailable', 'legs1_segments3_aircraft_code', 'legs1_segments3_arrivalTo_airport_city_iata', 'legs1_segments3_arrivalTo_airport_iata', 'legs1_segments3_baggageAllowance_quantity', 'legs1_segments3_baggageAllowance_weightMeasurementType', 'legs1_segments3_cabinClass', 'legs1_segments3_departureFrom_airport_iata', 'legs1_segments3_duration', 'legs1_segments3_flightNumber', 'legs1_segments3_marketingCarrier_code', 'legs1_segments3_operatingCarrier_code', 'legs1_segments3_seatsAvailable', 'miniRules0_monetaryAmount', 'miniRules0_percentage', 'miniRules0_statusInfos', 'miniRules1_monetaryAmount', 'miniRules1_percentage', 'miniRules1_statusInfos', 'pricingInfo_isAccessTP', 'pricingInfo_passengerCount', 'profileId', 'ranker_id', 'requestDate', 'searchRoute', 'sex', 'taxes', 'totalPrice', 'selected']\n",
      "åˆ†ææ•°æ®ç»“æ„...\n",
      "æ•°æ®å½¢çŠ¶: (500000, 126)\n",
      "å†…å­˜ä½¿ç”¨: 1507.1 MB\n",
      "åˆ—æ•°: 126\n",
      "å¯ç”¨å…³é”®åˆ—: ['ranker_id']\n",
      "ç¼ºå¤±å…³é”®åˆ—: ['user_id', 'departure_datetime', 'booking_datetime', 'price', 'num_stops']\n",
      "é¢„å¤„ç†æ•°æ®...\n",
      "æ•°æ®é¢„å¤„ç†å®Œæˆ\n",
      "ä½¿ç”¨ç”¨æˆ·æ ‡è¯†åˆ—: ranker_id\n",
      "åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®...\n",
      "åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®å®Œæˆ | ç”¨æˆ·æ•°: 38,675\n",
      "[load_and_preprocess_data] è€—æ—¶: 53.17s | å†…å­˜: 3117.8MB (-3156.2MB)\n",
      "åˆ†ææ•´ä½“æ¨¡å¼...\n",
      "æ•´ä½“æ¨¡å¼åˆ†æå®Œæˆ\n",
      "[analyze_overall_patterns] è€—æ—¶: 0.05s | å†…å­˜: 3118.7MB (+0.9MB)\n",
      "åˆ†æç”¨æˆ·è¡Œä¸º...\n",
      "ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆ\n",
      "[analyze_user_behavior] è€—æ—¶: 0.06s | å†…å­˜: 3123.7MB (+5.0MB)\n",
      "ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...\n",
      "ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: ../data/train_flight_analysis_results/reports/comprehensive_analysis.md\n",
      "[generate_comprehensive_report] è€—æ—¶: 0.00s | å†…å­˜: 3123.8MB (+0.1MB)\n",
      "============================================================\n",
      "åˆ†æå®Œæˆï¼\n",
      "============================================================\n",
      "æŸ¥çœ‹ç»“æœ:\n",
      "- æŠ¥å‘Š: ../data/train_flight_analysis_results/reports/\n",
      "- å›¾è¡¨: ../data/train_flight_analysis_results/plots/\n",
      "å†…å­˜æ¸…ç†å®Œæˆ\n",
      "âœ… è®­ç»ƒæ•°æ®åˆ†æå®Œæˆï¼\n",
      "\n",
      "============================================================\n",
      "åˆ†æå®Œæˆï¼Œè¯·æŸ¥çœ‹è¾“å‡ºç›®å½•ä¸­çš„ç»“æœ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "from functools import wraps\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def memory_monitor(func):\n",
    "    \"\"\"å†…å­˜ç›‘æ§è£…é¥°å™¨\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        process = psutil.Process()\n",
    "        mem_before = process.memory_info().rss / 1024**2\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        mem_after = process.memory_info().rss / 1024**2\n",
    "        print(f\"[{func.__name__}] è€—æ—¶: {end_time-start_time:.2f}s | å†…å­˜: {mem_after:.1f}MB ({mem_after-mem_before:+.1f}MB)\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "class EnhancedFlightDataAnalyzer:\n",
    "    \"\"\"èˆªç­æ•°æ®åˆ†æå™¨ - æ”¯æŒå•ä¸€æ•°æ®é›†åˆ†æ\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, output_dir: str = \"flight_analysis\", max_rows: int = 1000000):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–åˆ†æå™¨\n",
    "        \n",
    "        Args:\n",
    "            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„\n",
    "            output_dir: è¾“å‡ºç›®å½•\n",
    "            max_rows: æœ€å¤§å¤„ç†è¡Œæ•°ï¼ˆç”¨äºå†…å­˜æ§åˆ¶ï¼‰\n",
    "        \"\"\"\n",
    "        if not data_path or not os.path.exists(data_path):\n",
    "            raise ValueError(f\"æ•°æ®æ–‡ä»¶è·¯å¾„æ— æ•ˆ: {data_path}\")\n",
    "            \n",
    "        self.data_path = data_path\n",
    "        self.output_dir = output_dir\n",
    "        self.max_rows = max_rows\n",
    "        self.chunk_size = 100000\n",
    "        \n",
    "        # åˆå§‹åŒ–ç›®å½•\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        for subdir in ['reports', 'plots', 'processed_data']:\n",
    "            Path(f\"{output_dir}/{subdir}\").mkdir(exist_ok=True)\n",
    "        \n",
    "        # æ•°æ®å­˜å‚¨\n",
    "        self.data = None\n",
    "        self.grouped_data = None\n",
    "        self.analysis_results = {}\n",
    "        self.data_info = {}\n",
    "        \n",
    "        print(f\"èˆªç­æ•°æ®åˆ†æå™¨å·²åˆå§‹åŒ–\")\n",
    "        print(f\"- æ•°æ®æ–‡ä»¶: {data_path}\")\n",
    "        print(f\"- è¾“å‡ºç›®å½•: {output_dir}\")\n",
    "        print(f\"- æœ€å¤§å¤„ç†è¡Œæ•°: {max_rows:,}\")\n",
    "    \n",
    "    @memory_monitor\n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"åŠ è½½å’Œé¢„å¤„ç†æ•°æ®\"\"\"\n",
    "        print(\"å¼€å§‹åŠ è½½æ•°æ®...\")\n",
    "        \n",
    "        # è·å–æ–‡ä»¶ä¿¡æ¯\n",
    "        file_size_mb = os.path.getsize(self.data_path) / (1024**2)\n",
    "        print(f\"æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB\")\n",
    "        \n",
    "        # æ ¹æ®æ–‡ä»¶å¤§å°å†³å®šåŠ è½½ç­–ç•¥\n",
    "        if file_size_mb > 500:  # å¤§äº500MBè¿›è¡Œé‡‡æ ·\n",
    "            print(\"æ–‡ä»¶è¾ƒå¤§ï¼Œè¿›è¡Œé‡‡æ ·åŠ è½½...\")\n",
    "            self.data = self._load_large_file_sampled()\n",
    "        else:\n",
    "            print(\"ç›´æ¥åŠ è½½æ–‡ä»¶...\")\n",
    "            self.data = self._load_file_direct()\n",
    "        \n",
    "        if self.data is None or len(self.data) == 0:\n",
    "            raise ValueError(\"æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œè·¯å¾„\")\n",
    "        \n",
    "        print(f\"æ•°æ®åŠ è½½å®Œæˆ | æ€»è¡Œæ•°: {len(self.data):,} | åˆ—æ•°: {len(self.data.columns)}\")\n",
    "        print(f\"æ•°æ®åˆ—: {list(self.data.columns)}\")\n",
    "        \n",
    "        # æ•°æ®ä¿¡æ¯ç»Ÿè®¡\n",
    "        self._analyze_data_structure()\n",
    "        \n",
    "        # é¢„å¤„ç†\n",
    "        self._preprocess_data()\n",
    "        \n",
    "        # åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®ï¼ˆå¦‚æœæœ‰ç”¨æˆ·æ ‡è¯†ï¼‰\n",
    "        self._create_grouped_data()\n",
    "    \n",
    "    def _load_large_file_sampled(self) -> pd.DataFrame:\n",
    "        \"\"\"é‡‡æ ·åŠ è½½å¤§æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            # å…ˆè¯»å–å°æ ·æœ¬äº†è§£æ•°æ®ç»“æ„\n",
    "            sample_data = pd.read_parquet(self.data_path, nrows=10000)\n",
    "            print(f\"æ ·æœ¬æ•°æ®ç»“æ„: {sample_data.shape}\")\n",
    "            print(f\"åˆ—å: {list(sample_data.columns)}\")\n",
    "            \n",
    "            # è®¡ç®—æ€»è¡Œæ•°\n",
    "            try:\n",
    "                import pyarrow.parquet as pq\n",
    "                parquet_file = pq.ParquetFile(self.data_path)\n",
    "                total_rows = parquet_file.metadata.num_rows\n",
    "                print(f\"æ–‡ä»¶æ€»è¡Œæ•°: {total_rows:,}\")\n",
    "            except:\n",
    "                total_rows = None\n",
    "            \n",
    "            # æ ¹æ®å†…å­˜é™åˆ¶å†³å®šé‡‡æ ·ç­–ç•¥\n",
    "            if total_rows and total_rows > self.max_rows:\n",
    "                # éšæœºé‡‡æ ·\n",
    "                sample_fraction = min(self.max_rows / total_rows, 1.0)\n",
    "                print(f\"é‡‡æ ·æ¯”ä¾‹: {sample_fraction:.2%}\")\n",
    "     \n",
    "        except Exception as e:\n",
    "            print(f\"è¯»å–æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "            return None\n",
    "                \n",
    "    def _load_large_file_sampled(self) -> pd.DataFrame:\n",
    "        \"\"\"é‡‡æ ·åŠ è½½å¤§æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            # å…ˆè¯»å–å°æ ·æœ¬äº†è§£æ•°æ®ç»“æ„\n",
    "            print(\"è¯»å–æ•°æ®æ ·æœ¬ä»¥äº†è§£ç»“æ„...\")\n",
    "            \n",
    "            # ä½¿ç”¨pyarrowå°è¯•è·å–æ–‡ä»¶ä¿¡æ¯\n",
    "            try:\n",
    "                import pyarrow.parquet as pq\n",
    "                parquet_file = pq.ParquetFile(self.data_path)\n",
    "                total_rows = parquet_file.metadata.num_rows\n",
    "                print(f\"æ–‡ä»¶æ€»è¡Œæ•°: {total_rows:,}\")\n",
    "                \n",
    "                # å¦‚æœè¡Œæ•°ä¸å¤§ï¼Œç›´æ¥è¯»å–\n",
    "                if total_rows <= self.max_rows:\n",
    "                    table = parquet_file.read()\n",
    "                    return table.to_pandas()\n",
    "                \n",
    "                # å¦åˆ™è¿›è¡Œé‡‡æ ·\n",
    "                return self._sample_large_parquet(parquet_file, total_rows)\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"pyarrowæœªå®‰è£…ï¼Œä½¿ç”¨pandasç›´æ¥è¯»å–...\")\n",
    "                return self._load_file_direct()\n",
    "            except Exception as e:\n",
    "                print(f\"pyarrowè¯»å–å¤±è´¥: {e}\")\n",
    "                return self._load_file_direct()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"é‡‡æ ·åŠ è½½å¤±è´¥: {e}\")\n",
    "            return self._load_file_direct()\n",
    "    \n",
    "    def _sample_large_parquet(self, parquet_file, total_rows: int) -> pd.DataFrame:\n",
    "        \"\"\"å¯¹å¤§å‹parquetæ–‡ä»¶è¿›è¡Œé‡‡æ ·\"\"\"\n",
    "        print(f\"å¯¹å¤§æ–‡ä»¶è¿›è¡Œé‡‡æ · ({total_rows:,} -> {self.max_rows:,})\")\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®—é‡‡æ ·æ¯”ä¾‹\n",
    "            sample_ratio = self.max_rows / total_rows\n",
    "            batch_size = min(self.chunk_size, total_rows // 10)  # åˆ†æˆ10æ‰¹å¤„ç†\n",
    "            \n",
    "            sampled_chunks = []\n",
    "            processed_rows = 0\n",
    "            \n",
    "            for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "                batch_df = batch.to_pandas()\n",
    "                processed_rows += len(batch_df)\n",
    "                \n",
    "                # å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œé‡‡æ ·\n",
    "                if len(batch_df) > 0:\n",
    "                    sample_size = max(1, int(len(batch_df) * sample_ratio))\n",
    "                    sampled_batch = batch_df.sample(n=sample_size, random_state=42)\n",
    "                    sampled_chunks.append(sampled_batch)\n",
    "                \n",
    "                # æ˜¾ç¤ºè¿›åº¦\n",
    "                if processed_rows % (batch_size * 5) == 0:\n",
    "                    print(f\"å·²å¤„ç†: {processed_rows:,}/{total_rows:,} ({processed_rows/total_rows:.1%})\")\n",
    "                \n",
    "                # å¦‚æœå·²ç»é‡‡æ ·åˆ°è¶³å¤Ÿæ•°æ®ï¼Œåœæ­¢\n",
    "                current_sampled = sum(len(chunk) for chunk in sampled_chunks)\n",
    "                if current_sampled >= self.max_rows:\n",
    "                    break\n",
    "            \n",
    "            if sampled_chunks:\n",
    "                result = pd.concat(sampled_chunks, ignore_index=True)\n",
    "                # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§è¡Œæ•°\n",
    "                if len(result) > self.max_rows:\n",
    "                    result = result.sample(n=self.max_rows, random_state=42)\n",
    "                print(f\"é‡‡æ ·å®Œæˆï¼Œæœ€ç»ˆæ•°æ®é‡: {len(result):,}\")\n",
    "                return result\n",
    "            else:\n",
    "                print(\"é‡‡æ ·å¤±è´¥ï¼Œè¿”å›ç©ºDataFrame\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"é‡‡æ ·è¿‡ç¨‹å‡ºé”™: {e}\")\n",
    "            # å°è¯•ç›´æ¥è¯»å–ä¸€éƒ¨åˆ†æ•°æ®\n",
    "            try:\n",
    "                table = parquet_file.read(use_threads=False)\n",
    "                data = table.to_pandas()\n",
    "                if len(data) > self.max_rows:\n",
    "                    data = data.sample(n=self.max_rows, random_state=42)\n",
    "                return data\n",
    "            except:\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"é‡‡æ ·åŠ è½½å¤±è´¥: {e}\")\n",
    "            return self._load_file_direct()\n",
    "    \n",
    "    def _load_file_direct(self) -> pd.DataFrame:\n",
    "        \"\"\"ç›´æ¥åŠ è½½æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            # å…ˆå°è¯•è¯»å–å…¨éƒ¨æ•°æ®\n",
    "            data = pd.read_parquet(self.data_path)\n",
    "            \n",
    "            # å¦‚æœæ•°æ®é‡è¶…è¿‡é™åˆ¶ï¼Œè¿›è¡Œé‡‡æ ·\n",
    "            if len(data) > self.max_rows:\n",
    "                print(f\"æ•°æ®é‡ ({len(data):,}) è¶…è¿‡é™åˆ¶ ({self.max_rows:,})ï¼Œè¿›è¡Œéšæœºé‡‡æ ·...\")\n",
    "                data = data.sample(n=self.max_rows, random_state=42)\n",
    "                print(f\"é‡‡æ ·åæ•°æ®é‡: {len(data):,}\")\n",
    "            \n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"ç›´æ¥åŠ è½½å¤±è´¥: {e}\")\n",
    "    def _load_parquet_with_pyarrow(self) -> pd.DataFrame:\n",
    "        \"\"\"ä½¿ç”¨pyarrowåŠ è½½parquetæ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            import pyarrow.parquet as pq\n",
    "            \n",
    "            # è¯»å–parquetæ–‡ä»¶\n",
    "            parquet_file = pq.ParquetFile(self.data_path)\n",
    "            total_rows = parquet_file.metadata.num_rows\n",
    "            \n",
    "            print(f\"ä½¿ç”¨pyarrowè¯»å–ï¼Œæ€»è¡Œæ•°: {total_rows:,}\")\n",
    "            \n",
    "            # å¦‚æœè¡Œæ•°è¶…è¿‡é™åˆ¶ï¼ŒæŒ‰æ‰¹æ¬¡è¯»å–å¹¶é‡‡æ ·\n",
    "            if total_rows > self.max_rows:\n",
    "                print(f\"åˆ†æ‰¹è¯»å–å¹¶é‡‡æ ·åˆ° {self.max_rows:,} è¡Œ...\")\n",
    "                \n",
    "                # è®¡ç®—é‡‡æ ·æ¯”ä¾‹\n",
    "                sample_ratio = self.max_rows / total_rows\n",
    "                batch_size = min(self.chunk_size, self.max_rows)\n",
    "                \n",
    "                sampled_data = []\n",
    "                for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "                    batch_df = batch.to_pandas()\n",
    "                    \n",
    "                    # éšæœºé‡‡æ ·\n",
    "                    if len(batch_df) > 0:\n",
    "                        sample_size = max(1, int(len(batch_df) * sample_ratio))\n",
    "                        sampled_batch = batch_df.sample(n=sample_size, random_state=42)\n",
    "                        sampled_data.append(sampled_batch)\n",
    "                    \n",
    "                    # å¦‚æœå·²ç»é‡‡æ ·åˆ°è¶³å¤Ÿæ•°æ®ï¼Œåœæ­¢\n",
    "                    if sum(len(df) for df in sampled_data) >= self.max_rows:\n",
    "                        break\n",
    "                \n",
    "                if sampled_data:\n",
    "                    result = pd.concat(sampled_data, ignore_index=True)\n",
    "                    # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§è¡Œæ•°\n",
    "                    if len(result) > self.max_rows:\n",
    "                        result = result.sample(n=self.max_rows, random_state=42)\n",
    "                    return result\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "            else:\n",
    "                # ç›´æ¥è¯»å–å…¨éƒ¨æ•°æ®\n",
    "                table = parquet_file.read()\n",
    "                return table.to_pandas()\n",
    "                \n",
    "        except ImportError:\n",
    "            print(\"pyarrowæœªå®‰è£…ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...\")\n",
    "            return self._load_parquet_fallback()\n",
    "        except Exception as e:\n",
    "            print(f\"pyarrowåŠ è½½å¤±è´¥: {e}\")\n",
    "            return self._load_parquet_fallback()\n",
    "    \n",
    "    def _load_parquet_fallback(self) -> pd.DataFrame:\n",
    "        \"\"\"parquetæ–‡ä»¶æœ€åçš„å¤‡ç”¨æ–¹æ³•\"\"\"\n",
    "        try:\n",
    "            # å°è¯•è¯»å–ä¸€å°éƒ¨åˆ†æ•°æ®\n",
    "            print(\"å°è¯•è¯»å–æ•°æ®æ ·æœ¬...\")\n",
    "            \n",
    "            # å…ˆè¯»å–å¾ˆå°çš„æ ·æœ¬æ¥äº†è§£æ•°æ®ç»“æ„\n",
    "            import tempfile\n",
    "            import os\n",
    "            \n",
    "            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è¿›è¡Œæµ‹è¯•è¯»å–\n",
    "            data = pd.read_parquet(self.data_path)\n",
    "            \n",
    "            # å¦‚æœæˆåŠŸè¯»å–ä½†æ•°æ®é‡å¤ªå¤§ï¼Œè¿›è¡Œé‡‡æ ·\n",
    "            if len(data) > self.max_rows:\n",
    "                print(f\"æ•°æ®é‡è¿‡å¤§ï¼Œä» {len(data):,} è¡Œé‡‡æ ·åˆ° {self.max_rows:,} è¡Œ\")\n",
    "                data = data.sample(n=self.max_rows, random_state=42)\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"æ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥äº†: {e}\")\n",
    "            print(\"è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå®Œæ•´æ€§\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _analyze_data_structure(self):\n",
    "        \"\"\"åˆ†ææ•°æ®ç»“æ„\"\"\"\n",
    "        print(\"åˆ†ææ•°æ®ç»“æ„...\")\n",
    "        \n",
    "        self.data_info = {\n",
    "            'shape': self.data.shape,\n",
    "            'columns': list(self.data.columns),\n",
    "            'dtypes': self.data.dtypes.to_dict(),\n",
    "            'missing_values': self.data.isnull().sum().to_dict(),\n",
    "            'memory_usage_mb': self.data.memory_usage(deep=True).sum() / 1024**2\n",
    "        }\n",
    "        \n",
    "        print(f\"æ•°æ®å½¢çŠ¶: {self.data_info['shape']}\")\n",
    "        print(f\"å†…å­˜ä½¿ç”¨: {self.data_info['memory_usage_mb']:.1f} MB\")\n",
    "        print(f\"åˆ—æ•°: {len(self.data_info['columns'])}\")\n",
    "        \n",
    "        # æ£€æŸ¥å…³é”®åˆ—\n",
    "        key_columns = ['ranker_id', 'user_id', 'departure_datetime', 'booking_datetime', 'price', 'num_stops']\n",
    "        available_columns = [col for col in key_columns if col in self.data.columns]\n",
    "        missing_columns = [col for col in key_columns if col not in self.data.columns]\n",
    "        \n",
    "        print(f\"å¯ç”¨å…³é”®åˆ—: {available_columns}\")\n",
    "        if missing_columns:\n",
    "            print(f\"ç¼ºå¤±å…³é”®åˆ—: {missing_columns}\")\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"é¢„å¤„ç†æ•°æ®\"\"\"\n",
    "        print(\"é¢„å¤„ç†æ•°æ®...\")\n",
    "        \n",
    "        # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "        for col in self.data.columns:\n",
    "            if self.data[col].dtype == 'object':\n",
    "                try:\n",
    "                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼\n",
    "                    self.data[col] = pd.to_numeric(self.data[col], errors='ignore')\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # å¤„ç†æ—¶é—´åˆ—\n",
    "        datetime_columns = ['departure_datetime', 'booking_datetime', 'arrival_datetime']\n",
    "        for col in datetime_columns:\n",
    "            if col in self.data.columns:\n",
    "                try:\n",
    "                    self.data[col] = pd.to_datetime(self.data[col], errors='coerce')\n",
    "                    print(f\"å¤„ç†æ—¶é—´åˆ—: {col}\")\n",
    "                except:\n",
    "                    print(f\"æ—¶é—´åˆ—å¤„ç†å¤±è´¥: {col}\")\n",
    "        \n",
    "        # ç‰¹å¾å·¥ç¨‹\n",
    "        if 'departure_datetime' in self.data.columns:\n",
    "            self.data['departure_hour'] = self.data['departure_datetime'].dt.hour\n",
    "            self.data['departure_day'] = self.data['departure_datetime'].dt.day_name()\n",
    "            self.data['departure_month'] = self.data['departure_datetime'].dt.month\n",
    "            self.data['is_weekend'] = self.data['departure_datetime'].dt.weekday >= 5\n",
    "        \n",
    "        if 'booking_datetime' in self.data.columns and 'departure_datetime' in self.data.columns:\n",
    "            self.data['advance_booking_days'] = (\n",
    "                self.data['departure_datetime'] - self.data['booking_datetime']\n",
    "            ).dt.days\n",
    "        \n",
    "        # ä»·æ ¼ç›¸å…³ç‰¹å¾\n",
    "        if 'price' in self.data.columns:\n",
    "            self.data['price_level'] = pd.cut(\n",
    "                self.data['price'], \n",
    "                bins=5, \n",
    "                labels=['æä½ä»·', 'ä½ä»·', 'ä¸­ä»·', 'é«˜ä»·', 'æé«˜ä»·']\n",
    "            )\n",
    "        \n",
    "        # èˆªçº¿å¤æ‚æ€§\n",
    "        if 'num_stops' in self.data.columns:\n",
    "            self.data['flight_type'] = self.data['num_stops'].apply(\n",
    "                lambda x: 'ç›´é£' if x == 0 else ('ä¸€æ¬¡ä¸­è½¬' if x == 1 else 'å¤šæ¬¡ä¸­è½¬')\n",
    "            )\n",
    "        \n",
    "        print(\"æ•°æ®é¢„å¤„ç†å®Œæˆ\")\n",
    "    \n",
    "    def _create_grouped_data(self):\n",
    "        \"\"\"åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®\"\"\"\n",
    "        # å¯»æ‰¾ç”¨æˆ·æ ‡è¯†åˆ—\n",
    "        user_id_col = 'ranker_id'\n",
    "        \n",
    "        if not user_id_col in self.data.columns:\n",
    "            print(\"è­¦å‘Š: æœªæ‰¾åˆ°ç”¨æˆ·æ ‡è¯†åˆ—ï¼Œå°†è¿›è¡Œæ•´ä½“åˆ†æ\")\n",
    "            self.grouped_data = None\n",
    "            return\n",
    "        \n",
    "        print(f\"ä½¿ç”¨ç”¨æˆ·æ ‡è¯†åˆ—: {user_id_col}\")\n",
    "        print(\"åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®...\")\n",
    "        \n",
    "        # æŒ‰ç”¨æˆ·åˆ†ç»„ç»Ÿè®¡\n",
    "        user_stats = []\n",
    "        user_groups = self.data.groupby(user_id_col)\n",
    "        \n",
    "        for user_id, group in user_groups:\n",
    "            if len(group) >= 3:  # åªå¤„ç†æœ‰è¶³å¤Ÿæ•°æ®çš„ç”¨æˆ·\n",
    "                stats = self._calculate_user_stats(group)\n",
    "                stats[user_id_col] = user_id\n",
    "                user_stats.append(stats)\n",
    "        \n",
    "        if user_stats:\n",
    "            self.grouped_data = pd.DataFrame(user_stats)\n",
    "            print(f\"åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®å®Œæˆ | ç”¨æˆ·æ•°: {len(self.grouped_data):,}\")\n",
    "        else:\n",
    "            print(\"è­¦å‘Š: æ²¡æœ‰è¶³å¤Ÿçš„ç”¨æˆ·æ•°æ®è¿›è¡Œåˆ†ç»„åˆ†æ\")\n",
    "            self.grouped_data = None\n",
    "    \n",
    "    def _calculate_user_stats(self, group: pd.DataFrame) -> Dict:\n",
    "        \"\"\"è®¡ç®—ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        stats = {\n",
    "            'total_searches': len(group),\n",
    "            'avg_price': group['price'].mean() if 'price' in group.columns and not group['price'].isna().all() else 0,\n",
    "            'price_std': group['price'].std() if 'price' in group.columns and not group['price'].isna().all() else 0,\n",
    "            'price_sensitivity': self._calculate_price_sensitivity(group),\n",
    "            'preferred_departure_hour': group['departure_hour'].mode().iloc[0] if 'departure_hour' in group.columns and len(group['departure_hour'].dropna()) > 0 else 12,\n",
    "            'weekend_ratio': group['is_weekend'].mean() if 'is_weekend' in group.columns else 0,\n",
    "            'avg_advance_booking': group['advance_booking_days'].mean() if 'advance_booking_days' in group.columns and not group['advance_booking_days'].isna().all() else 0,\n",
    "            'direct_flight_ratio': (group['num_stops'] == 0).mean() if 'num_stops' in group.columns else 0,\n",
    "            'business_indicator': self._identify_business_traveler(group)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _calculate_price_sensitivity(self, group: pd.DataFrame) -> float:\n",
    "        \"\"\"è®¡ç®—ä»·æ ¼æ•æ„Ÿæ€§\"\"\"\n",
    "        if 'price' not in group.columns or len(group) < 3 or group['price'].isna().all():\n",
    "            return 0.5\n",
    "        \n",
    "        price_data = group['price'].dropna()\n",
    "        if len(price_data) < 3:\n",
    "            return 0.5\n",
    "        \n",
    "        price_ranges = price_data.quantile([0.25, 0.75])\n",
    "        low_price_selections = len(price_data[price_data <= price_ranges[0.25]])\n",
    "        high_price_selections = len(price_data[price_data >= price_ranges[0.75]])\n",
    "        \n",
    "        if low_price_selections + high_price_selections == 0:\n",
    "            return 0.5\n",
    "        \n",
    "        return low_price_selections / (low_price_selections + high_price_selections)\n",
    "    \n",
    "    def _identify_business_traveler(self, group: pd.DataFrame) -> float:\n",
    "        \"\"\"è¯†åˆ«å•†åŠ¡æ—…è¡Œè€…\"\"\"\n",
    "        business_indicators = []\n",
    "        \n",
    "        # å·¥ä½œæ—¥å‡ºè¡Œæ¯”ä¾‹\n",
    "        if 'is_weekend' in group.columns:\n",
    "            weekday_ratio = 1 - group['is_weekend'].mean()\n",
    "            business_indicators.append(weekday_ratio)\n",
    "        \n",
    "        # æ—©ç­æœºåå¥½\n",
    "        if 'departure_hour' in group.columns:\n",
    "            early_flight_ratio = (group['departure_hour'] <= 8).mean()\n",
    "            business_indicators.append(early_flight_ratio)\n",
    "        \n",
    "        # çŸ­æœŸé¢„è®¢\n",
    "        if 'advance_booking_days' in group.columns:\n",
    "            short_booking_data = group['advance_booking_days'].dropna()\n",
    "            if len(short_booking_data) > 0:\n",
    "                short_booking_ratio = (short_booking_data <= 7).mean()\n",
    "                business_indicators.append(short_booking_ratio)\n",
    "        \n",
    "        # ç›´é£åå¥½\n",
    "        if 'num_stops' in group.columns:\n",
    "            direct_ratio = (group['num_stops'] == 0).mean()\n",
    "            business_indicators.append(direct_ratio)\n",
    "        \n",
    "        return np.mean(business_indicators) if business_indicators else 0.5\n",
    "    \n",
    "    @memory_monitor\n",
    "    def analyze_overall_patterns(self):\n",
    "        \"\"\"åˆ†ææ•´ä½“æ¨¡å¼\"\"\"\n",
    "        print(\"åˆ†ææ•´ä½“æ¨¡å¼...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # ä»·æ ¼åˆ†æ\n",
    "        if 'price' in self.data.columns:\n",
    "            price_data = self.data['price'].dropna()\n",
    "            patterns['price_analysis'] = {\n",
    "                'mean': price_data.mean(),\n",
    "                'median': price_data.median(),\n",
    "                'std': price_data.std(),\n",
    "                'min': price_data.min(),\n",
    "                'max': price_data.max(),\n",
    "                'q25': price_data.quantile(0.25),\n",
    "                'q75': price_data.quantile(0.75)\n",
    "            }\n",
    "        \n",
    "        # æ—¶é—´æ¨¡å¼åˆ†æ\n",
    "        if 'departure_hour' in self.data.columns:\n",
    "            patterns['time_patterns'] = {\n",
    "                'peak_hours': self.data['departure_hour'].value_counts().head(5).to_dict(),\n",
    "                'weekend_vs_weekday': self.data['is_weekend'].value_counts().to_dict() if 'is_weekend' in self.data.columns else {}\n",
    "            }\n",
    "        \n",
    "        # èˆªçº¿ç±»å‹åˆ†æ\n",
    "        if 'num_stops' in self.data.columns:\n",
    "            patterns['flight_type_analysis'] = {\n",
    "                'direct_flights_ratio': (self.data['num_stops'] == 0).mean(),\n",
    "                'stops_distribution': self.data['num_stops'].value_counts().to_dict()\n",
    "            }\n",
    "        \n",
    "        # é¢„è®¢æ¨¡å¼åˆ†æ\n",
    "        if 'advance_booking_days' in self.data.columns:\n",
    "            booking_data = self.data['advance_booking_days'].dropna()\n",
    "            patterns['booking_patterns'] = {\n",
    "                'avg_advance_days': booking_data.mean(),\n",
    "                'median_advance_days': booking_data.median(),\n",
    "                'last_minute_ratio': (booking_data <= 1).mean(),\n",
    "                'planned_booking_ratio': (booking_data >= 14).mean()\n",
    "            }\n",
    "        \n",
    "        # å¯è§†åŒ–\n",
    "        self._create_overall_pattern_plots(patterns)\n",
    "        \n",
    "        self.analysis_results['overall_patterns'] = patterns\n",
    "        print(\"æ•´ä½“æ¨¡å¼åˆ†æå®Œæˆ\")\n",
    "    \n",
    "    def _create_overall_pattern_plots(self, patterns: Dict):\n",
    "        \"\"\"åˆ›å»ºæ•´ä½“æ¨¡å¼å›¾è¡¨\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('ä»·æ ¼åˆ†å¸ƒ', 'å‡ºå‘æ—¶é—´åˆ†å¸ƒ', 'èˆªç­ç±»å‹åˆ†å¸ƒ', 'é¢„è®¢æå‰å¤©æ•°åˆ†å¸ƒ'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # ä»·æ ¼åˆ†å¸ƒ\n",
    "        if 'price' in self.data.columns:\n",
    "            price_data = self.data['price'].dropna()\n",
    "            fig.add_trace(go.Histogram(x=price_data, name='ä»·æ ¼åˆ†å¸ƒ', nbinsx=50), row=1, col=1)\n",
    "        \n",
    "        # å‡ºå‘æ—¶é—´åˆ†å¸ƒ\n",
    "        if 'departure_hour' in self.data.columns:\n",
    "            hour_counts = self.data['departure_hour'].value_counts().sort_index()\n",
    "            fig.add_trace(go.Bar(x=hour_counts.index, y=hour_counts.values, name='å‡ºå‘æ—¶é—´'), row=1, col=2)\n",
    "        \n",
    "        # èˆªç­ç±»å‹åˆ†å¸ƒ\n",
    "        if 'num_stops' in self.data.columns:\n",
    "            stops_counts = self.data['num_stops'].value_counts().sort_index()\n",
    "            fig.add_trace(go.Bar(x=stops_counts.index, y=stops_counts.values, name='ä¸­è½¬æ¬¡æ•°'), row=2, col=1)\n",
    "        \n",
    "        # é¢„è®¢æå‰å¤©æ•°åˆ†å¸ƒ\n",
    "        if 'advance_booking_days' in self.data.columns:\n",
    "            booking_data = self.data['advance_booking_days'].dropna()\n",
    "            fig.add_trace(go.Histogram(x=booking_data, name='é¢„è®¢æå‰å¤©æ•°', nbinsx=50), row=2, col=2)\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"æ•´ä½“æ¨¡å¼åˆ†æ\")\n",
    "        fig.write_html(f\"{self.output_dir}/plots/overall_patterns.html\")\n",
    "    \n",
    "    @memory_monitor\n",
    "    def analyze_user_behavior(self):\n",
    "        \"\"\"åˆ†æç”¨æˆ·è¡Œä¸ºï¼ˆå¦‚æœæœ‰ç”¨æˆ·æ•°æ®ï¼‰\"\"\"\n",
    "        if self.grouped_data is None:\n",
    "            print(\"è·³è¿‡ç”¨æˆ·è¡Œä¸ºåˆ†æ - æ— ç”¨æˆ·åˆ†ç»„æ•°æ®\")\n",
    "            return\n",
    "        \n",
    "        print(\"åˆ†æç”¨æˆ·è¡Œä¸º...\")\n",
    "        \n",
    "        # ç”¨æˆ·åˆ†ç±»\n",
    "        if 'business_indicator' in self.grouped_data.columns:\n",
    "            threshold = self.grouped_data['business_indicator'].median()\n",
    "            business_users = self.grouped_data[self.grouped_data['business_indicator'] >= threshold]\n",
    "            leisure_users = self.grouped_data[self.grouped_data['business_indicator'] < threshold]\n",
    "            \n",
    "            behavior_analysis = {\n",
    "                'business_users': {\n",
    "                    'count': len(business_users),\n",
    "                    'avg_price': business_users['avg_price'].mean(),\n",
    "                    'price_sensitivity': business_users['price_sensitivity'].mean(),\n",
    "                    'advance_booking': business_users['avg_advance_booking'].mean(),\n",
    "                    'direct_flight_ratio': business_users['direct_flight_ratio'].mean(),\n",
    "                    'weekend_ratio': business_users['weekend_ratio'].mean()\n",
    "                },\n",
    "                'leisure_users': {\n",
    "                    'count': len(leisure_users),\n",
    "                    'avg_price': leisure_users['avg_price'].mean(),\n",
    "                    'price_sensitivity': leisure_users['price_sensitivity'].mean(),\n",
    "                    'advance_booking': leisure_users['avg_advance_booking'].mean(),\n",
    "                    'direct_flight_ratio': leisure_users['direct_flight_ratio'].mean(),\n",
    "                    'weekend_ratio': leisure_users['weekend_ratio'].mean()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # å¯è§†åŒ–\n",
    "            self._create_user_behavior_plots(business_users, leisure_users)\n",
    "            \n",
    "            self.analysis_results['user_behavior'] = behavior_analysis\n",
    "            \n",
    "        print(\"ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆ\")\n",
    "    \n",
    "    def _create_user_behavior_plots(self, business_users: pd.DataFrame, leisure_users: pd.DataFrame):\n",
    "        \"\"\"åˆ›å»ºç”¨æˆ·è¡Œä¸ºå›¾è¡¨\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('ä»·æ ¼æ•æ„Ÿæ€§å¯¹æ¯”', 'å¹³å‡ä»·æ ¼å¯¹æ¯”', 'æå‰é¢„è®¢å¤©æ•°å¯¹æ¯”', 'ç›´é£åå¥½å¯¹æ¯”'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # ä»·æ ¼æ•æ„Ÿæ€§\n",
    "        fig.add_trace(go.Histogram(x=business_users['price_sensitivity'], name='å•†åŠ¡ç”¨æˆ·', \n",
    "                                 opacity=0.7, nbinsx=20), row=1, col=1)\n",
    "        fig.add_trace(go.Histogram(x=leisure_users['price_sensitivity'], name='ä¼‘é—²ç”¨æˆ·', \n",
    "                                 opacity=0.7, nbinsx=20), row=1, col=1)\n",
    "        \n",
    "        # å¹³å‡ä»·æ ¼\n",
    "        categories = ['å•†åŠ¡ç”¨æˆ·', 'ä¼‘é—²ç”¨æˆ·']\n",
    "        avg_prices = [business_users['avg_price'].mean(), leisure_users['avg_price'].mean()]\n",
    "        fig.add_trace(go.Bar(x=categories, y=avg_prices, name='å¹³å‡ä»·æ ¼'), row=1, col=2)\n",
    "        \n",
    "        # æå‰é¢„è®¢\n",
    "        fig.add_trace(go.Histogram(x=business_users['avg_advance_booking'], name='å•†åŠ¡ç”¨æˆ·', \n",
    "                                 opacity=0.7, nbinsx=20), row=2, col=1)\n",
    "        fig.add_trace(go.Histogram(x=leisure_users['avg_advance_booking'], name='ä¼‘é—²ç”¨æˆ·', \n",
    "                                 opacity=0.7, nbinsx=20), row=2, col=1)\n",
    "        \n",
    "        # ç›´é£åå¥½\n",
    "        direct_ratios = [business_users['direct_flight_ratio'].mean(), leisure_users['direct_flight_ratio'].mean()]\n",
    "        fig.add_trace(go.Bar(x=categories, y=direct_ratios, name='ç›´é£æ¯”ä¾‹'), row=2, col=2)\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"ç”¨æˆ·è¡Œä¸ºå¯¹æ¯”åˆ†æ\")\n",
    "        fig.write_html(f\"{self.output_dir}/plots/user_behavior_analysis.html\")\n",
    "    \n",
    "    @memory_monitor\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š\"\"\"\n",
    "        print(\"ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...\")\n",
    "        \n",
    "        report = f\"\"\"# èˆªç­æ•°æ®åˆ†ææŠ¥å‘Š\n",
    "\n",
    "## æ•°æ®æ¦‚å†µ\n",
    "- æ•°æ®æ–‡ä»¶: {self.data_path}\n",
    "- è®°å½•æ•°: {len(self.data):,}\n",
    "- å†…å­˜ä½¿ç”¨: {self.data_info['memory_usage_mb']:.1f} MB\n",
    "- åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## æ•°æ®ç»“æ„\n",
    "- æ•°æ®ç»´åº¦: {self.data_info['shape']}\n",
    "- åˆ—æ•°: {len(self.data_info['columns'])}\n",
    "- ä¸»è¦åˆ—: {', '.join(self.data_info['columns'][:10])}\n",
    "\n",
    "## ä¸»è¦å‘ç°\n",
    "\n",
    "### 1. æ•´ä½“æ¨¡å¼åˆ†æ\n",
    "\"\"\"\n",
    "        \n",
    "        if 'overall_patterns' in self.analysis_results:\n",
    "            patterns = self.analysis_results['overall_patterns']\n",
    "            \n",
    "            if 'price_analysis' in patterns:\n",
    "                price_info = patterns['price_analysis']\n",
    "                report += f\"\"\"\n",
    "**ä»·æ ¼åˆ†æ:**\n",
    "- å¹³å‡ä»·æ ¼: {price_info['mean']:.2f}\n",
    "- ä¸­ä½ä»·æ ¼: {price_info['median']:.2f}\n",
    "- ä»·æ ¼æ ‡å‡†å·®: {price_info['std']:.2f}\n",
    "- ä»·æ ¼èŒƒå›´: {price_info['min']:.2f} - {price_info['max']:.2f}\n",
    "\"\"\"\n",
    "            \n",
    "            if 'flight_type_analysis' in patterns:\n",
    "                flight_info = patterns['flight_type_analysis']\n",
    "                report += f\"\"\"\n",
    "**èˆªç­ç±»å‹åˆ†æ:**\n",
    "- ç›´é£æ¯”ä¾‹: {flight_info['direct_flights_ratio']:.2%}\n",
    "- ä¸­è½¬åˆ†å¸ƒ: {flight_info['stops_distribution']}\n",
    "\"\"\"\n",
    "            \n",
    "            if 'booking_patterns' in patterns:\n",
    "                booking_info = patterns['booking_patterns']\n",
    "                report += f\"\"\"\n",
    "**é¢„è®¢æ¨¡å¼åˆ†æ:**\n",
    "- å¹³å‡æå‰é¢„è®¢: {booking_info['avg_advance_days']:.1f} å¤©\n",
    "- ä¸´æ—¶é¢„è®¢æ¯”ä¾‹: {booking_info['last_minute_ratio']:.2%}\n",
    "- è®¡åˆ’é¢„è®¢æ¯”ä¾‹: {booking_info['planned_booking_ratio']:.2%}\n",
    "\"\"\"\n",
    "        \n",
    "        if 'user_behavior' in self.analysis_results:\n",
    "            behavior = self.analysis_results['user_behavior']\n",
    "            report += f\"\"\"\n",
    "### 2. ç”¨æˆ·è¡Œä¸ºåˆ†æ\n",
    "\n",
    "**å•†åŠ¡ç”¨æˆ· ({behavior['business_users']['count']}äºº):**\n",
    "- å¹³å‡ä»·æ ¼: {behavior['business_users']['avg_price']:.2f}\n",
    "- ä»·æ ¼æ•æ„Ÿæ€§: {behavior['business_users']['price_sensitivity']:.2f}\n",
    "- æå‰é¢„è®¢: {behavior['business_users']['advance_booking']:.1f} å¤©\n",
    "- ç›´é£åå¥½: {behavior['business_users']['direct_flight_ratio']:.2%}\n",
    "\n",
    "**ä¼‘é—²ç”¨æˆ· ({behavior['leisure_users']['count']}äºº):**\n",
    "- å¹³å‡ä»·æ ¼: {behavior['leisure_users']['avg_price']:.2f}\n",
    "- ä»·æ ¼æ•æ„Ÿæ€§: {behavior['leisure_users']['price_sensitivity']:.2f}\n",
    "- æå‰é¢„è®¢: {behavior['leisure_users']['advance_booking']:.1f} å¤©\n",
    "- ç›´é£åå¥½: {behavior['leisure_users']['direct_flight_ratio']:.2%}\n",
    "\"\"\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "## å•†ä¸šå»ºè®®\n",
    "\n",
    "### æ•´ä½“ç­–ç•¥\n",
    "1. ä¼˜åŒ–ä»·æ ¼ç­–ç•¥ï¼Œå…³æ³¨ä»·æ ¼æ•æ„Ÿç”¨æˆ·ç¾¤ä½“\n",
    "2. æä¾›å¤šæ ·åŒ–çš„èˆªç­é€‰æ‹©ï¼ˆç›´é£/ä¸­è½¬ï¼‰\n",
    "3. é’ˆå¯¹ä¸åŒé¢„è®¢ä¹ æƒ¯åˆ¶å®šå·®å¼‚åŒ–æœåŠ¡\n",
    "\n",
    "### æŠ€æœ¯å»ºè®®\n",
    "1. æ”¹è¿›æœç´¢æ’åºç®—æ³•ï¼Œè€ƒè™‘ç”¨æˆ·å†å²è¡Œä¸º\n",
    "2. å®æ–½ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿ\n",
    "3. ä¼˜åŒ–ç§»åŠ¨ç«¯é¢„è®¢æµç¨‹\n",
    "\n",
    "---\n",
    "*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "*æ•°æ®æ¥æº: {self.data_path}*\n",
    "\"\"\"\n",
    "        \n",
    "        # ä¿å­˜æŠ¥å‘Š\n",
    "        report_path = f\"{self.output_dir}/reports/comprehensive_analysis.md\"\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}\")\n",
    "    \n",
    "    def run_analysis(self):\n",
    "        \"\"\"è¿è¡Œå®Œæ•´åˆ†ææµç¨‹\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"å¼€å§‹èˆªç­æ•°æ®åˆ†æ...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†\n",
    "            self.load_and_preprocess_data()\n",
    "            \n",
    "            # 2. æ•´ä½“æ¨¡å¼åˆ†æ\n",
    "            self.analyze_overall_patterns()\n",
    "            \n",
    "            # 3. ç”¨æˆ·è¡Œä¸ºåˆ†æï¼ˆå¦‚æœæœ‰ç”¨æˆ·æ•°æ®ï¼‰\n",
    "            self.analyze_user_behavior()\n",
    "            \n",
    "            # 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š\n",
    "            self.generate_comprehensive_report()\n",
    "            \n",
    "            print(\"=\" * 60)\n",
    "            print(\"åˆ†æå®Œæˆï¼\")\n",
    "            print(\"=\" * 60)\n",
    "            print(\"æŸ¥çœ‹ç»“æœ:\")\n",
    "            print(f\"- æŠ¥å‘Š: {self.output_dir}/reports/\")\n",
    "            print(f\"- å›¾è¡¨: {self.output_dir}/plots/\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # æ¸…ç†å†…å­˜\n",
    "            self.cleanup()\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"æ¸…ç†å†…å­˜\"\"\"\n",
    "        if hasattr(self, 'data'):\n",
    "            del self.data\n",
    "        if hasattr(self, 'grouped_data'):\n",
    "            del self.grouped_data\n",
    "        gc.collect()\n",
    "        print(\"å†…å­˜æ¸…ç†å®Œæˆ\")\n",
    "\n",
    "# ä¾¿æ·å‡½æ•°\n",
    "def analyze_flight_data(data_path: str, output_dir: str = \"flight_analysis\", max_rows: int = 1000000):\n",
    "    \"\"\"åˆ†æèˆªç­æ•°æ®çš„ä¾¿æ·å‡½æ•°\"\"\"\n",
    "    analyzer = EnhancedFlightDataAnalyzer(data_path, output_dir, max_rows)\n",
    "    analyzer.run_analysis()\n",
    "    return analyzer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # é¦–å…ˆæµ‹è¯•æ•°æ®åŠ è½½\n",
    "    print(\"=\" * 60)\n",
    "    print(\"æµ‹è¯•æ•°æ®æ–‡ä»¶\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_files = [\n",
    "        \"../data/test.parquet\",\n",
    "        \"../data/train.parquet\"\n",
    "    ]\n",
    "    \n",
    "    # ç¤ºä¾‹1: åˆ†ææµ‹è¯•æ•°æ®\n",
    "    print(\"=\" * 60)\n",
    "    print(\"åˆ†ææµ‹è¯•æ•°æ®\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        test_analyzer = analyze_flight_data(\n",
    "            data_path=\"../data/test.parquet\",\n",
    "            output_dir=\"../data/test_flight_analysis_results\",\n",
    "            max_rows=500000  # é™åˆ¶æœ€å¤§è¡Œæ•°ä»¥æ§åˆ¶å†…å­˜\n",
    "        )\n",
    "        print(\"âœ… æµ‹è¯•æ•°æ®åˆ†æå®Œæˆï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æµ‹è¯•æ•°æ®åˆ†æå¤±è´¥: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"åˆ†æè®­ç»ƒæ•°æ®\")\n",
    "    print(\"=\" * 60)\n",
    "    # ç¤ºä¾‹2: åˆ†æè®­ç»ƒæ•°æ®\n",
    "    try:\n",
    "        train_analyzer = analyze_flight_data(\n",
    "            data_path=\"../data/train.parquet\",\n",
    "            output_dir=\"../data/train_flight_analysis_results\",\n",
    "            max_rows=500000  # é™åˆ¶æœ€å¤§è¡Œæ•°ä»¥æ§åˆ¶å†…å­˜\n",
    "        )\n",
    "        print(\"âœ… è®­ç»ƒæ•°æ®åˆ†æå®Œæˆï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è®­ç»ƒæ•°æ®åˆ†æå¤±è´¥: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"åˆ†æå®Œæˆï¼Œè¯·æŸ¥çœ‹è¾“å‡ºç›®å½•ä¸­çš„ç»“æœ\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

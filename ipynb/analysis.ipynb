{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# èˆªç­æ’åç«èµ› - ä¸šåŠ¡é€»è¾‘ç†è§£ä¸ç‰¹å¾åˆ†æ\n",
    "## Flight Ranking Competition - Business Logic Understanding & Feature Analysis\n",
    "\n",
    "#### 1. **æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†**\n",
    "- è‡ªåŠ¨ç§»é™¤ç¼ºå¤±å€¼è¶…è¿‡80%çš„ç‰¹å¾\n",
    "- æŒ‰`ranker_id`åˆ†ç»„åˆ†æï¼Œåªå¤„ç†æœ‰è¶³å¤Ÿæ•°æ®çš„æœç´¢ä¼šè¯\n",
    "- å¯¹ä¿ç•™ç‰¹å¾è¿›è¡Œåˆç†å¡«å……ç­–ç•¥\n",
    "\n",
    "#### 2. **åˆ†æç»´åº¦**\n",
    "- å•†åŠ¡vsä¼‘é—²æ—…è¡Œè€…æ·±åº¦å¯¹æ¯”\n",
    "- ä»·æ ¼æ•æ„Ÿæ€§å’Œæ”¯ä»˜æ„æ„¿åˆ†æ\n",
    "- æ—¶é—´åå¥½æ¨¡å¼ï¼ˆå‡ºå‘æ—¶é—´ã€é¢„è®¢æ—¶é—´ï¼‰\n",
    "- èˆªçº¿å¤æ‚æ€§åå¥½ï¼ˆç›´é£vsä¸­è½¬ï¼‰\n",
    "- èˆªç©ºå…¬å¸å’Œæœºåœºåå¥½\n",
    "- ç”¨æˆ·èšç±»å’Œå¿ è¯šåº¦åˆ†æ\n",
    "- ä¼ä¸šå·®æ—…æ”¿ç­–å½±å“\n",
    "- èˆ±ä½ç­‰çº§åå¥½\n",
    "- æå‰é¢„è®¢è¡Œä¸ºåˆ†æ\n",
    "- å¾€è¿”vså•ç¨‹åå¥½\n",
    "- ç”¨æˆ·è½¬åŒ–ç‡åˆ†æ\n",
    "- æ‹“å±•...\n",
    "\n",
    "#### 3. **åŠŸèƒ½**\n",
    "- **ç”¨æˆ·ç”»åƒç³»ç»Ÿ**ï¼šä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºè¯¦ç»†çš„åå¥½æ¡£æ¡ˆ\n",
    "- **æ™ºèƒ½èšç±»**ï¼šå°†ç”¨æˆ·åˆ†ä¸º4ä¸ªä¸»è¦ç¾¤ä½“\n",
    "- **äº¤äº’å¼å¯è§†åŒ–**ï¼šä½¿ç”¨Plotlyåˆ›å»ºåŠ¨æ€å›¾è¡¨\n",
    "- **å•†ä¸šæ´å¯Ÿ**ï¼šç”Ÿæˆå¯æ‰§è¡Œçš„å•†ä¸šå»ºè®®\n",
    "\n",
    "### ğŸ“Š ç³»ç»Ÿæ¶æ„\n",
    "\n",
    "```\n",
    "EnhancedFlightDataAnalyzer\n",
    "â”œâ”€â”€ æ•°æ®åŠ è½½ä¸é¢„å¤„ç†\n",
    "â”œâ”€â”€ åˆ†ç»„æ•°æ®åˆ›å»º (æŒ‰ranker_id)\n",
    "â”œâ”€â”€ ç”¨æˆ·ç”»åƒç”Ÿæˆ\n",
    "â”œâ”€â”€ å¤šç»´åº¦åˆ†ææ¨¡å—\n",
    "â”‚   â”œâ”€â”€ å•†åŠ¡vsä¼‘é—²åˆ†æ\n",
    "â”‚   â”œâ”€â”€ ä»·æ ¼æ•æ„Ÿæ€§åˆ†æ\n",
    "â”‚   â”œâ”€â”€ æ—¶é—´åå¥½åˆ†æ\n",
    "â”‚   â”œâ”€â”€ èˆªçº¿åå¥½åˆ†æ\n",
    "â”‚   â””â”€â”€ ç”¨æˆ·èšç±»åˆ†æ\n",
    "â””â”€â”€ ç»¼åˆæŠ¥å‘Šç”Ÿæˆ\n",
    "```\n",
    "\n",
    "### ğŸš€ ä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "```python\n",
    "# åˆå§‹åŒ–åˆ†æå™¨\n",
    "analyzer = EnhancedFlightDataAnalyzer(\n",
    "    train_path='data/train.parquet',\n",
    "    test_path='data/test.parquet'\n",
    ")\n",
    "\n",
    "# è¿è¡Œå®Œæ•´åˆ†æ\n",
    "analyzer.run_full_analysis()\n",
    "\n",
    "# æˆ–è€…è¿è¡Œå•ä¸ªåˆ†ææ¨¡å—\n",
    "analyzer.load_and_preprocess_data()\n",
    "analyzer.analyze_business_vs_leisure_detailed()\n",
    "analyzer.analyze_price_sensitivity()\n",
    "```\n",
    "\n",
    "### ğŸ¯ å…³é”®æ´å¯Ÿèƒ½åŠ›\n",
    "\n",
    "1. **ç”¨æˆ·åˆ†ç¾¤**ï¼šè‡ªåŠ¨è¯†åˆ«é«˜ä»·å€¼å•†åŠ¡ç”¨æˆ·å’Œä»·æ ¼æ•æ„Ÿä¼‘é—²ç”¨æˆ·\n",
    "2. **åå¥½é¢„æµ‹**ï¼šåŸºäºå†å²è¡Œä¸ºé¢„æµ‹ç”¨æˆ·é€‰æ‹©å€¾å‘\n",
    "3. **ä¸ªæ€§åŒ–æ¨è**ï¼šä¸ºä¸åŒç”¨æˆ·ç¾¤ä½“æä¾›å®šåˆ¶åŒ–çš„èˆªç­æ¨èç­–ç•¥\n",
    "4. **è½¬åŒ–ä¼˜åŒ–**ï¼šè¯†åˆ«å½±å“ç”¨æˆ·é€‰æ‹©çš„å…³é”®å› ç´ \n",
    "\n",
    "### ğŸ“ˆ è¾“å‡ºæŠ¥å‘Š\n",
    "\n",
    "ç³»ç»Ÿä¼šç”Ÿæˆï¼š\n",
    "- å¤šç»´åº¦å¯è§†åŒ–å›¾è¡¨\n",
    "- ç”¨æˆ·èšç±»åˆ†æç»“æœ\n",
    "- å•†ä¸šæ´å¯Ÿå’Œå»ºè®®\n",
    "- è¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Šæ–‡ä»¶\n",
    "\n",
    "è¿™ä¸ªç³»ç»Ÿç‰¹åˆ«é€‚åˆï¼š\n",
    "- èˆªç­æ¨èç³»ç»Ÿä¼˜åŒ–\n",
    "- ç”¨æˆ·è¡Œä¸ºåˆ†æ\n",
    "- ä¸ªæ€§åŒ–è¥é”€ç­–ç•¥åˆ¶å®š\n",
    "- äº§å“åŠŸèƒ½æ”¹è¿›å†³ç­–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "æµ‹è¯•æ•°æ®æ–‡ä»¶\n",
      "============================================================\n",
      "============================================================\n",
      "åˆ†ææµ‹è¯•æ•°æ®\n",
      "============================================================\n",
      "èˆªç­æ•°æ®åˆ†æå™¨å·²åˆå§‹åŒ–\n",
      "- æ•°æ®æ–‡ä»¶: ../data/test.parquet\n",
      "- è¾“å‡ºç›®å½•: ../data/test_flight_analysis_results\n",
      "- æœ€å¤§å¤„ç†è¡Œæ•°: 500,000\n",
      "============================================================\n",
      "å¼€å§‹èˆªç­æ•°æ®åˆ†æ...\n",
      "============================================================\n",
      "å¼€å§‹åŠ è½½æ•°æ®...\n",
      "æ–‡ä»¶å¤§å°: 137.5 MB\n",
      "ç›´æ¥åŠ è½½æ–‡ä»¶...\n",
      "æ•°æ®é‡ (6,897,776) è¶…è¿‡é™åˆ¶ (500,000)ï¼Œè¿›è¡Œéšæœºé‡‡æ ·...\n",
      "é‡‡æ ·åæ•°æ®é‡: 500,000\n",
      "æ•°æ®åŠ è½½å®Œæˆ | æ€»è¡Œæ•°: 500,000 | åˆ—æ•°: 125\n",
      "æ•°æ®åˆ—: ['Id', 'bySelf', 'companyID', 'corporateTariffCode', 'frequentFlyer', 'nationality', 'isAccess3D', 'isVip', 'legs0_arrivalAt', 'legs0_departureAt', 'legs0_duration', 'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata', 'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_baggageAllowance_quantity', 'legs0_segments0_baggageAllowance_weightMeasurementType', 'legs0_segments0_cabinClass', 'legs0_segments0_departureFrom_airport_iata', 'legs0_segments0_duration', 'legs0_segments0_flightNumber', 'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code', 'legs0_segments0_seatsAvailable', 'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata', 'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_baggageAllowance_quantity', 'legs0_segments1_baggageAllowance_weightMeasurementType', 'legs0_segments1_cabinClass', 'legs0_segments1_departureFrom_airport_iata', 'legs0_segments1_duration', 'legs0_segments1_flightNumber', 'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code', 'legs0_segments1_seatsAvailable', 'legs0_segments2_aircraft_code', 'legs0_segments2_arrivalTo_airport_city_iata', 'legs0_segments2_arrivalTo_airport_iata', 'legs0_segments2_baggageAllowance_quantity', 'legs0_segments2_baggageAllowance_weightMeasurementType', 'legs0_segments2_cabinClass', 'legs0_segments2_departureFrom_airport_iata', 'legs0_segments2_duration', 'legs0_segments2_flightNumber', 'legs0_segments2_marketingCarrier_code', 'legs0_segments2_operatingCarrier_code', 'legs0_segments2_seatsAvailable', 'legs0_segments3_aircraft_code', 'legs0_segments3_arrivalTo_airport_city_iata', 'legs0_segments3_arrivalTo_airport_iata', 'legs0_segments3_baggageAllowance_quantity', 'legs0_segments3_baggageAllowance_weightMeasurementType', 'legs0_segments3_cabinClass', 'legs0_segments3_departureFrom_airport_iata', 'legs0_segments3_duration', 'legs0_segments3_flightNumber', 'legs0_segments3_marketingCarrier_code', 'legs0_segments3_operatingCarrier_code', 'legs0_segments3_seatsAvailable', 'legs1_arrivalAt', 'legs1_departureAt', 'legs1_duration', 'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata', 'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_baggageAllowance_quantity', 'legs1_segments0_baggageAllowance_weightMeasurementType', 'legs1_segments0_cabinClass', 'legs1_segments0_departureFrom_airport_iata', 'legs1_segments0_duration', 'legs1_segments0_flightNumber', 'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code', 'legs1_segments0_seatsAvailable', 'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata', 'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_baggageAllowance_quantity', 'legs1_segments1_baggageAllowance_weightMeasurementType', 'legs1_segments1_cabinClass', 'legs1_segments1_departureFrom_airport_iata', 'legs1_segments1_duration', 'legs1_segments1_flightNumber', 'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code', 'legs1_segments1_seatsAvailable', 'legs1_segments2_aircraft_code', 'legs1_segments2_arrivalTo_airport_city_iata', 'legs1_segments2_arrivalTo_airport_iata', 'legs1_segments2_baggageAllowance_quantity', 'legs1_segments2_baggageAllowance_weightMeasurementType', 'legs1_segments2_cabinClass', 'legs1_segments2_departureFrom_airport_iata', 'legs1_segments2_duration', 'legs1_segments2_flightNumber', 'legs1_segments2_marketingCarrier_code', 'legs1_segments2_operatingCarrier_code', 'legs1_segments2_seatsAvailable', 'legs1_segments3_aircraft_code', 'legs1_segments3_arrivalTo_airport_city_iata', 'legs1_segments3_arrivalTo_airport_iata', 'legs1_segments3_baggageAllowance_quantity', 'legs1_segments3_baggageAllowance_weightMeasurementType', 'legs1_segments3_cabinClass', 'legs1_segments3_departureFrom_airport_iata', 'legs1_segments3_duration', 'legs1_segments3_flightNumber', 'legs1_segments3_marketingCarrier_code', 'legs1_segments3_operatingCarrier_code', 'legs1_segments3_seatsAvailable', 'miniRules0_monetaryAmount', 'miniRules0_percentage', 'miniRules0_statusInfos', 'miniRules1_monetaryAmount', 'miniRules1_percentage', 'miniRules1_statusInfos', 'pricingInfo_isAccessTP', 'pricingInfo_passengerCount', 'profileId', 'ranker_id', 'requestDate', 'searchRoute', 'sex', 'taxes', 'totalPrice']\n",
      "åˆ†ææ•°æ®ç»“æ„...\n",
      "æ•°æ®å½¢çŠ¶: (500000, 125)\n",
      "å†…å­˜ä½¿ç”¨: 1519.8 MB\n",
      "åˆ—æ•°: 125\n",
      "å¯ç”¨å…³é”®åˆ—: ['ranker_id']\n",
      "ç¼ºå¤±å…³é”®åˆ—: ['user_id', 'departure_datetime', 'booking_datetime', 'price', 'num_stops']\n",
      "é¢„å¤„ç†æ•°æ®...\n",
      "æ•°æ®é¢„å¤„ç†å®Œæˆ\n",
      "ä½¿ç”¨ç”¨æˆ·æ ‡è¯†åˆ—: ranker_id\n",
      "åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®...\n",
      "åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®å®Œæˆ | ç”¨æˆ·æ•°: 23,200\n",
      "[load_and_preprocess_data] è€—æ—¶: 18.94s | å†…å­˜: 6738.3MB (+5913.9MB)\n",
      "åˆ†ææ•´ä½“æ¨¡å¼...\n",
      "æ•´ä½“æ¨¡å¼åˆ†æå®Œæˆ\n",
      "[analyze_overall_patterns] è€—æ—¶: 0.04s | å†…å­˜: 6738.4MB (+0.1MB)\n",
      "åˆ†æç”¨æˆ·è¡Œä¸º...\n",
      "ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆ\n",
      "[analyze_user_behavior] è€—æ—¶: 0.06s | å†…å­˜: 6742.7MB (+4.3MB)\n",
      "ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...\n",
      "ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: ../data/test_flight_analysis_results/reports/comprehensive_analysis.md\n",
      "[generate_comprehensive_report] è€—æ—¶: 0.00s | å†…å­˜: 6742.7MB (+0.0MB)\n",
      "============================================================\n",
      "åˆ†æå®Œæˆï¼\n",
      "============================================================\n",
      "æŸ¥çœ‹ç»“æœ:\n",
      "- æŠ¥å‘Š: ../data/test_flight_analysis_results/reports/\n",
      "- å›¾è¡¨: ../data/test_flight_analysis_results/plots/\n",
      "å†…å­˜æ¸…ç†å®Œæˆ\n",
      "âœ… æµ‹è¯•æ•°æ®åˆ†æå®Œæˆï¼\n",
      "\n",
      "============================================================\n",
      "åˆ†æè®­ç»ƒæ•°æ®\n",
      "============================================================\n",
      "èˆªç­æ•°æ®åˆ†æå™¨å·²åˆå§‹åŒ–\n",
      "- æ•°æ®æ–‡ä»¶: ../data/train.parquet\n",
      "- è¾“å‡ºç›®å½•: ../data/train_flight_analysis_results\n",
      "- æœ€å¤§å¤„ç†è¡Œæ•°: 500,000\n",
      "============================================================\n",
      "å¼€å§‹èˆªç­æ•°æ®åˆ†æ...\n",
      "============================================================\n",
      "å¼€å§‹åŠ è½½æ•°æ®...\n",
      "æ–‡ä»¶å¤§å°: 376.6 MB\n",
      "ç›´æ¥åŠ è½½æ–‡ä»¶...\n",
      "æ•°æ®é‡ (18,145,372) è¶…è¿‡é™åˆ¶ (500,000)ï¼Œè¿›è¡Œéšæœºé‡‡æ ·...\n",
      "é‡‡æ ·åæ•°æ®é‡: 500,000\n",
      "æ•°æ®åŠ è½½å®Œæˆ | æ€»è¡Œæ•°: 500,000 | åˆ—æ•°: 126\n",
      "æ•°æ®åˆ—: ['Id', 'bySelf', 'companyID', 'corporateTariffCode', 'frequentFlyer', 'nationality', 'isAccess3D', 'isVip', 'legs0_arrivalAt', 'legs0_departureAt', 'legs0_duration', 'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata', 'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_baggageAllowance_quantity', 'legs0_segments0_baggageAllowance_weightMeasurementType', 'legs0_segments0_cabinClass', 'legs0_segments0_departureFrom_airport_iata', 'legs0_segments0_duration', 'legs0_segments0_flightNumber', 'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code', 'legs0_segments0_seatsAvailable', 'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata', 'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_baggageAllowance_quantity', 'legs0_segments1_baggageAllowance_weightMeasurementType', 'legs0_segments1_cabinClass', 'legs0_segments1_departureFrom_airport_iata', 'legs0_segments1_duration', 'legs0_segments1_flightNumber', 'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code', 'legs0_segments1_seatsAvailable', 'legs0_segments2_aircraft_code', 'legs0_segments2_arrivalTo_airport_city_iata', 'legs0_segments2_arrivalTo_airport_iata', 'legs0_segments2_baggageAllowance_quantity', 'legs0_segments2_baggageAllowance_weightMeasurementType', 'legs0_segments2_cabinClass', 'legs0_segments2_departureFrom_airport_iata', 'legs0_segments2_duration', 'legs0_segments2_flightNumber', 'legs0_segments2_marketingCarrier_code', 'legs0_segments2_operatingCarrier_code', 'legs0_segments2_seatsAvailable', 'legs0_segments3_aircraft_code', 'legs0_segments3_arrivalTo_airport_city_iata', 'legs0_segments3_arrivalTo_airport_iata', 'legs0_segments3_baggageAllowance_quantity', 'legs0_segments3_baggageAllowance_weightMeasurementType', 'legs0_segments3_cabinClass', 'legs0_segments3_departureFrom_airport_iata', 'legs0_segments3_duration', 'legs0_segments3_flightNumber', 'legs0_segments3_marketingCarrier_code', 'legs0_segments3_operatingCarrier_code', 'legs0_segments3_seatsAvailable', 'legs1_arrivalAt', 'legs1_departureAt', 'legs1_duration', 'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata', 'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_baggageAllowance_quantity', 'legs1_segments0_baggageAllowance_weightMeasurementType', 'legs1_segments0_cabinClass', 'legs1_segments0_departureFrom_airport_iata', 'legs1_segments0_duration', 'legs1_segments0_flightNumber', 'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code', 'legs1_segments0_seatsAvailable', 'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata', 'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_baggageAllowance_quantity', 'legs1_segments1_baggageAllowance_weightMeasurementType', 'legs1_segments1_cabinClass', 'legs1_segments1_departureFrom_airport_iata', 'legs1_segments1_duration', 'legs1_segments1_flightNumber', 'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code', 'legs1_segments1_seatsAvailable', 'legs1_segments2_aircraft_code', 'legs1_segments2_arrivalTo_airport_city_iata', 'legs1_segments2_arrivalTo_airport_iata', 'legs1_segments2_baggageAllowance_quantity', 'legs1_segments2_baggageAllowance_weightMeasurementType', 'legs1_segments2_cabinClass', 'legs1_segments2_departureFrom_airport_iata', 'legs1_segments2_duration', 'legs1_segments2_flightNumber', 'legs1_segments2_marketingCarrier_code', 'legs1_segments2_operatingCarrier_code', 'legs1_segments2_seatsAvailable', 'legs1_segments3_aircraft_code', 'legs1_segments3_arrivalTo_airport_city_iata', 'legs1_segments3_arrivalTo_airport_iata', 'legs1_segments3_baggageAllowance_quantity', 'legs1_segments3_baggageAllowance_weightMeasurementType', 'legs1_segments3_cabinClass', 'legs1_segments3_departureFrom_airport_iata', 'legs1_segments3_duration', 'legs1_segments3_flightNumber', 'legs1_segments3_marketingCarrier_code', 'legs1_segments3_operatingCarrier_code', 'legs1_segments3_seatsAvailable', 'miniRules0_monetaryAmount', 'miniRules0_percentage', 'miniRules0_statusInfos', 'miniRules1_monetaryAmount', 'miniRules1_percentage', 'miniRules1_statusInfos', 'pricingInfo_isAccessTP', 'pricingInfo_passengerCount', 'profileId', 'ranker_id', 'requestDate', 'searchRoute', 'sex', 'taxes', 'totalPrice', 'selected']\n",
      "åˆ†ææ•°æ®ç»“æ„...\n",
      "æ•°æ®å½¢çŠ¶: (500000, 126)\n",
      "å†…å­˜ä½¿ç”¨: 1507.1 MB\n",
      "åˆ—æ•°: 126\n",
      "å¯ç”¨å…³é”®åˆ—: ['ranker_id']\n",
      "ç¼ºå¤±å…³é”®åˆ—: ['user_id', 'departure_datetime', 'booking_datetime', 'price', 'num_stops']\n",
      "é¢„å¤„ç†æ•°æ®...\n",
      "æ•°æ®é¢„å¤„ç†å®Œæˆ\n",
      "ä½¿ç”¨ç”¨æˆ·æ ‡è¯†åˆ—: ranker_id\n",
      "åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®...\n",
      "åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®å®Œæˆ | ç”¨æˆ·æ•°: 38,675\n",
      "[load_and_preprocess_data] è€—æ—¶: 53.17s | å†…å­˜: 3117.8MB (-3156.2MB)\n",
      "åˆ†ææ•´ä½“æ¨¡å¼...\n",
      "æ•´ä½“æ¨¡å¼åˆ†æå®Œæˆ\n",
      "[analyze_overall_patterns] è€—æ—¶: 0.05s | å†…å­˜: 3118.7MB (+0.9MB)\n",
      "åˆ†æç”¨æˆ·è¡Œä¸º...\n",
      "ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆ\n",
      "[analyze_user_behavior] è€—æ—¶: 0.06s | å†…å­˜: 3123.7MB (+5.0MB)\n",
      "ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...\n",
      "ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: ../data/train_flight_analysis_results/reports/comprehensive_analysis.md\n",
      "[generate_comprehensive_report] è€—æ—¶: 0.00s | å†…å­˜: 3123.8MB (+0.1MB)\n",
      "============================================================\n",
      "åˆ†æå®Œæˆï¼\n",
      "============================================================\n",
      "æŸ¥çœ‹ç»“æœ:\n",
      "- æŠ¥å‘Š: ../data/train_flight_analysis_results/reports/\n",
      "- å›¾è¡¨: ../data/train_flight_analysis_results/plots/\n",
      "å†…å­˜æ¸…ç†å®Œæˆ\n",
      "âœ… è®­ç»ƒæ•°æ®åˆ†æå®Œæˆï¼\n",
      "\n",
      "============================================================\n",
      "åˆ†æå®Œæˆï¼Œè¯·æŸ¥çœ‹è¾“å‡ºç›®å½•ä¸­çš„ç»“æœ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "from functools import wraps\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def memory_monitor(func):\n",
    "    \"\"\"å†…å­˜ç›‘æ§è£…é¥°å™¨\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        process = psutil.Process()\n",
    "        mem_before = process.memory_info().rss / 1024**2\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        mem_after = process.memory_info().rss / 1024**2\n",
    "        print(f\"[{func.__name__}] è€—æ—¶: {end_time-start_time:.2f}s | å†…å­˜: {mem_after:.1f}MB ({mem_after-mem_before:+.1f}MB)\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "class EnhancedFlightDataAnalyzer:\n",
    "    \"\"\"èˆªç­æ•°æ®åˆ†æå™¨ - æ”¯æŒå•ä¸€æ•°æ®é›†åˆ†æ\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, output_dir: str = \"flight_analysis\", max_rows: int = 1000000):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–åˆ†æå™¨\n",
    "        \n",
    "        Args:\n",
    "            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„\n",
    "            output_dir: è¾“å‡ºç›®å½•\n",
    "            max_rows: æœ€å¤§å¤„ç†è¡Œæ•°ï¼ˆç”¨äºå†…å­˜æ§åˆ¶ï¼‰\n",
    "        \"\"\"\n",
    "        if not data_path or not os.path.exists(data_path):\n",
    "            raise ValueError(f\"æ•°æ®æ–‡ä»¶è·¯å¾„æ— æ•ˆ: {data_path}\")\n",
    "            \n",
    "        self.data_path = data_path\n",
    "        self.output_dir = output_dir\n",
    "        self.max_rows = max_rows\n",
    "        self.chunk_size = 100000\n",
    "        \n",
    "        # åˆå§‹åŒ–ç›®å½•\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        for subdir in ['reports', 'plots', 'processed_data']:\n",
    "            Path(f\"{output_dir}/{subdir}\").mkdir(exist_ok=True)\n",
    "        \n",
    "        # æ•°æ®å­˜å‚¨\n",
    "        self.data = None\n",
    "        self.grouped_data = None\n",
    "        self.analysis_results = {}\n",
    "        self.data_info = {}\n",
    "        \n",
    "        print(f\"èˆªç­æ•°æ®åˆ†æå™¨å·²åˆå§‹åŒ–\")\n",
    "        print(f\"- æ•°æ®æ–‡ä»¶: {data_path}\")\n",
    "        print(f\"- è¾“å‡ºç›®å½•: {output_dir}\")\n",
    "        print(f\"- æœ€å¤§å¤„ç†è¡Œæ•°: {max_rows:,}\")\n",
    "    \n",
    "    @memory_monitor\n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"åŠ è½½å’Œé¢„å¤„ç†æ•°æ®\"\"\"\n",
    "        print(\"å¼€å§‹åŠ è½½æ•°æ®...\")\n",
    "        \n",
    "        # è·å–æ–‡ä»¶ä¿¡æ¯\n",
    "        file_size_mb = os.path.getsize(self.data_path) / (1024**2)\n",
    "        print(f\"æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB\")\n",
    "        \n",
    "        # æ ¹æ®æ–‡ä»¶å¤§å°å†³å®šåŠ è½½ç­–ç•¥\n",
    "        if file_size_mb > 500:  # å¤§äº500MBè¿›è¡Œé‡‡æ ·\n",
    "            print(\"æ–‡ä»¶è¾ƒå¤§ï¼Œè¿›è¡Œé‡‡æ ·åŠ è½½...\")\n",
    "            self.data = self._load_large_file_sampled()\n",
    "        else:\n",
    "            print(\"ç›´æ¥åŠ è½½æ–‡ä»¶...\")\n",
    "            self.data = self._load_file_direct()\n",
    "        \n",
    "        if self.data is None or len(self.data) == 0:\n",
    "            raise ValueError(\"æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œè·¯å¾„\")\n",
    "        \n",
    "        print(f\"æ•°æ®åŠ è½½å®Œæˆ | æ€»è¡Œæ•°: {len(self.data):,} | åˆ—æ•°: {len(self.data.columns)}\")\n",
    "        print(f\"æ•°æ®åˆ—: {list(self.data.columns)}\")\n",
    "        \n",
    "        # æ•°æ®ä¿¡æ¯ç»Ÿè®¡\n",
    "        self._analyze_data_structure()\n",
    "        \n",
    "        # é¢„å¤„ç†\n",
    "        self._preprocess_data()\n",
    "        \n",
    "        # åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®ï¼ˆå¦‚æœæœ‰ç”¨æˆ·æ ‡è¯†ï¼‰\n",
    "        self._create_grouped_data()\n",
    "    \n",
    "    def _load_large_file_sampled(self) -> pd.DataFrame:\n",
    "        \"\"\"é‡‡æ ·åŠ è½½å¤§æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            # å…ˆè¯»å–å°æ ·æœ¬äº†è§£æ•°æ®ç»“æ„\n",
    "            sample_data = pd.read_parquet(self.data_path, nrows=10000)\n",
    "            print(f\"æ ·æœ¬æ•°æ®ç»“æ„: {sample_data.shape}\")\n",
    "            print(f\"åˆ—å: {list(sample_data.columns)}\")\n",
    "            \n",
    "            # è®¡ç®—æ€»è¡Œæ•°\n",
    "            try:\n",
    "                import pyarrow.parquet as pq\n",
    "                parquet_file = pq.ParquetFile(self.data_path)\n",
    "                total_rows = parquet_file.metadata.num_rows\n",
    "                print(f\"æ–‡ä»¶æ€»è¡Œæ•°: {total_rows:,}\")\n",
    "            except:\n",
    "                total_rows = None\n",
    "            \n",
    "            # æ ¹æ®å†…å­˜é™åˆ¶å†³å®šé‡‡æ ·ç­–ç•¥\n",
    "            if total_rows and total_rows > self.max_rows:\n",
    "                # éšæœºé‡‡æ ·\n",
    "                sample_fraction = min(self.max_rows / total_rows, 1.0)\n",
    "                print(f\"é‡‡æ ·æ¯”ä¾‹: {sample_fraction:.2%}\")\n",
    "     \n",
    "        except Exception as e:\n",
    "            print(f\"è¯»å–æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "            return None\n",
    "                \n",
    "    def _load_large_file_sampled(self) -> pd.DataFrame:\n",
    "        \"\"\"é‡‡æ ·åŠ è½½å¤§æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            # å…ˆè¯»å–å°æ ·æœ¬äº†è§£æ•°æ®ç»“æ„\n",
    "            print(\"è¯»å–æ•°æ®æ ·æœ¬ä»¥äº†è§£ç»“æ„...\")\n",
    "            \n",
    "            # ä½¿ç”¨pyarrowå°è¯•è·å–æ–‡ä»¶ä¿¡æ¯\n",
    "            try:\n",
    "                import pyarrow.parquet as pq\n",
    "                parquet_file = pq.ParquetFile(self.data_path)\n",
    "                total_rows = parquet_file.metadata.num_rows\n",
    "                print(f\"æ–‡ä»¶æ€»è¡Œæ•°: {total_rows:,}\")\n",
    "                \n",
    "                # å¦‚æœè¡Œæ•°ä¸å¤§ï¼Œç›´æ¥è¯»å–\n",
    "                if total_rows <= self.max_rows:\n",
    "                    table = parquet_file.read()\n",
    "                    return table.to_pandas()\n",
    "                \n",
    "                # å¦åˆ™è¿›è¡Œé‡‡æ ·\n",
    "                return self._sample_large_parquet(parquet_file, total_rows)\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"pyarrowæœªå®‰è£…ï¼Œä½¿ç”¨pandasç›´æ¥è¯»å–...\")\n",
    "                return self._load_file_direct()\n",
    "            except Exception as e:\n",
    "                print(f\"pyarrowè¯»å–å¤±è´¥: {e}\")\n",
    "                return self._load_file_direct()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"é‡‡æ ·åŠ è½½å¤±è´¥: {e}\")\n",
    "            return self._load_file_direct()\n",
    "    \n",
    "    def _sample_large_parquet(self, parquet_file, total_rows: int) -> pd.DataFrame:\n",
    "        \"\"\"å¯¹å¤§å‹parquetæ–‡ä»¶è¿›è¡Œé‡‡æ ·\"\"\"\n",
    "        print(f\"å¯¹å¤§æ–‡ä»¶è¿›è¡Œé‡‡æ · ({total_rows:,} -> {self.max_rows:,})\")\n",
    "        \n",
    "        try:\n",
    "            # è®¡ç®—é‡‡æ ·æ¯”ä¾‹\n",
    "            sample_ratio = self.max_rows / total_rows\n",
    "            batch_size = min(self.chunk_size, total_rows // 10)  # åˆ†æˆ10æ‰¹å¤„ç†\n",
    "            \n",
    "            sampled_chunks = []\n",
    "            processed_rows = 0\n",
    "            \n",
    "            for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "                batch_df = batch.to_pandas()\n",
    "                processed_rows += len(batch_df)\n",
    "                \n",
    "                # å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œé‡‡æ ·\n",
    "                if len(batch_df) > 0:\n",
    "                    sample_size = max(1, int(len(batch_df) * sample_ratio))\n",
    "                    sampled_batch = batch_df.sample(n=sample_size, random_state=42)\n",
    "                    sampled_chunks.append(sampled_batch)\n",
    "                \n",
    "                # æ˜¾ç¤ºè¿›åº¦\n",
    "                if processed_rows % (batch_size * 5) == 0:\n",
    "                    print(f\"å·²å¤„ç†: {processed_rows:,}/{total_rows:,} ({processed_rows/total_rows:.1%})\")\n",
    "                \n",
    "                # å¦‚æœå·²ç»é‡‡æ ·åˆ°è¶³å¤Ÿæ•°æ®ï¼Œåœæ­¢\n",
    "                current_sampled = sum(len(chunk) for chunk in sampled_chunks)\n",
    "                if current_sampled >= self.max_rows:\n",
    "                    break\n",
    "            \n",
    "            if sampled_chunks:\n",
    "                result = pd.concat(sampled_chunks, ignore_index=True)\n",
    "                # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§è¡Œæ•°\n",
    "                if len(result) > self.max_rows:\n",
    "                    result = result.sample(n=self.max_rows, random_state=42)\n",
    "                print(f\"é‡‡æ ·å®Œæˆï¼Œæœ€ç»ˆæ•°æ®é‡: {len(result):,}\")\n",
    "                return result\n",
    "            else:\n",
    "                print(\"é‡‡æ ·å¤±è´¥ï¼Œè¿”å›ç©ºDataFrame\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"é‡‡æ ·è¿‡ç¨‹å‡ºé”™: {e}\")\n",
    "            # å°è¯•ç›´æ¥è¯»å–ä¸€éƒ¨åˆ†æ•°æ®\n",
    "            try:\n",
    "                table = parquet_file.read(use_threads=False)\n",
    "                data = table.to_pandas()\n",
    "                if len(data) > self.max_rows:\n",
    "                    data = data.sample(n=self.max_rows, random_state=42)\n",
    "                return data\n",
    "            except:\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"é‡‡æ ·åŠ è½½å¤±è´¥: {e}\")\n",
    "            return self._load_file_direct()\n",
    "    \n",
    "    def _load_file_direct(self) -> pd.DataFrame:\n",
    "        \"\"\"ç›´æ¥åŠ è½½æ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            # å…ˆå°è¯•è¯»å–å…¨éƒ¨æ•°æ®\n",
    "            data = pd.read_parquet(self.data_path)\n",
    "            \n",
    "            # å¦‚æœæ•°æ®é‡è¶…è¿‡é™åˆ¶ï¼Œè¿›è¡Œé‡‡æ ·\n",
    "            if len(data) > self.max_rows:\n",
    "                print(f\"æ•°æ®é‡ ({len(data):,}) è¶…è¿‡é™åˆ¶ ({self.max_rows:,})ï¼Œè¿›è¡Œéšæœºé‡‡æ ·...\")\n",
    "                data = data.sample(n=self.max_rows, random_state=42)\n",
    "                print(f\"é‡‡æ ·åæ•°æ®é‡: {len(data):,}\")\n",
    "            \n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"ç›´æ¥åŠ è½½å¤±è´¥: {e}\")\n",
    "    def _load_parquet_with_pyarrow(self) -> pd.DataFrame:\n",
    "        \"\"\"ä½¿ç”¨pyarrowåŠ è½½parquetæ–‡ä»¶\"\"\"\n",
    "        try:\n",
    "            import pyarrow.parquet as pq\n",
    "            \n",
    "            # è¯»å–parquetæ–‡ä»¶\n",
    "            parquet_file = pq.ParquetFile(self.data_path)\n",
    "            total_rows = parquet_file.metadata.num_rows\n",
    "            \n",
    "            print(f\"ä½¿ç”¨pyarrowè¯»å–ï¼Œæ€»è¡Œæ•°: {total_rows:,}\")\n",
    "            \n",
    "            # å¦‚æœè¡Œæ•°è¶…è¿‡é™åˆ¶ï¼ŒæŒ‰æ‰¹æ¬¡è¯»å–å¹¶é‡‡æ ·\n",
    "            if total_rows > self.max_rows:\n",
    "                print(f\"åˆ†æ‰¹è¯»å–å¹¶é‡‡æ ·åˆ° {self.max_rows:,} è¡Œ...\")\n",
    "                \n",
    "                # è®¡ç®—é‡‡æ ·æ¯”ä¾‹\n",
    "                sample_ratio = self.max_rows / total_rows\n",
    "                batch_size = min(self.chunk_size, self.max_rows)\n",
    "                \n",
    "                sampled_data = []\n",
    "                for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "                    batch_df = batch.to_pandas()\n",
    "                    \n",
    "                    # éšæœºé‡‡æ ·\n",
    "                    if len(batch_df) > 0:\n",
    "                        sample_size = max(1, int(len(batch_df) * sample_ratio))\n",
    "                        sampled_batch = batch_df.sample(n=sample_size, random_state=42)\n",
    "                        sampled_data.append(sampled_batch)\n",
    "                    \n",
    "                    # å¦‚æœå·²ç»é‡‡æ ·åˆ°è¶³å¤Ÿæ•°æ®ï¼Œåœæ­¢\n",
    "                    if sum(len(df) for df in sampled_data) >= self.max_rows:\n",
    "                        break\n",
    "                \n",
    "                if sampled_data:\n",
    "                    result = pd.concat(sampled_data, ignore_index=True)\n",
    "                    # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§è¡Œæ•°\n",
    "                    if len(result) > self.max_rows:\n",
    "                        result = result.sample(n=self.max_rows, random_state=42)\n",
    "                    return result\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "            else:\n",
    "                # ç›´æ¥è¯»å–å…¨éƒ¨æ•°æ®\n",
    "                table = parquet_file.read()\n",
    "                return table.to_pandas()\n",
    "                \n",
    "        except ImportError:\n",
    "            print(\"pyarrowæœªå®‰è£…ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...\")\n",
    "            return self._load_parquet_fallback()\n",
    "        except Exception as e:\n",
    "            print(f\"pyarrowåŠ è½½å¤±è´¥: {e}\")\n",
    "            return self._load_parquet_fallback()\n",
    "    \n",
    "    def _load_parquet_fallback(self) -> pd.DataFrame:\n",
    "        \"\"\"parquetæ–‡ä»¶æœ€åçš„å¤‡ç”¨æ–¹æ³•\"\"\"\n",
    "        try:\n",
    "            # å°è¯•è¯»å–ä¸€å°éƒ¨åˆ†æ•°æ®\n",
    "            print(\"å°è¯•è¯»å–æ•°æ®æ ·æœ¬...\")\n",
    "            \n",
    "            # å…ˆè¯»å–å¾ˆå°çš„æ ·æœ¬æ¥äº†è§£æ•°æ®ç»“æ„\n",
    "            import tempfile\n",
    "            import os\n",
    "            \n",
    "            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è¿›è¡Œæµ‹è¯•è¯»å–\n",
    "            data = pd.read_parquet(self.data_path)\n",
    "            \n",
    "            # å¦‚æœæˆåŠŸè¯»å–ä½†æ•°æ®é‡å¤ªå¤§ï¼Œè¿›è¡Œé‡‡æ ·\n",
    "            if len(data) > self.max_rows:\n",
    "                print(f\"æ•°æ®é‡è¿‡å¤§ï¼Œä» {len(data):,} è¡Œé‡‡æ ·åˆ° {self.max_rows:,} è¡Œ\")\n",
    "                data = data.sample(n=self.max_rows, random_state=42)\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"æ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥äº†: {e}\")\n",
    "            print(\"è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå®Œæ•´æ€§\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _analyze_data_structure(self):\n",
    "        \"\"\"åˆ†ææ•°æ®ç»“æ„\"\"\"\n",
    "        print(\"åˆ†ææ•°æ®ç»“æ„...\")\n",
    "        \n",
    "        self.data_info = {\n",
    "            'shape': self.data.shape,\n",
    "            'columns': list(self.data.columns),\n",
    "            'dtypes': self.data.dtypes.to_dict(),\n",
    "            'missing_values': self.data.isnull().sum().to_dict(),\n",
    "            'memory_usage_mb': self.data.memory_usage(deep=True).sum() / 1024**2\n",
    "        }\n",
    "        \n",
    "        print(f\"æ•°æ®å½¢çŠ¶: {self.data_info['shape']}\")\n",
    "        print(f\"å†…å­˜ä½¿ç”¨: {self.data_info['memory_usage_mb']:.1f} MB\")\n",
    "        print(f\"åˆ—æ•°: {len(self.data_info['columns'])}\")\n",
    "        \n",
    "        # æ£€æŸ¥å…³é”®åˆ—\n",
    "        key_columns = ['ranker_id', 'user_id', 'departure_datetime', 'booking_datetime', 'price', 'num_stops']\n",
    "        available_columns = [col for col in key_columns if col in self.data.columns]\n",
    "        missing_columns = [col for col in key_columns if col not in self.data.columns]\n",
    "        \n",
    "        print(f\"å¯ç”¨å…³é”®åˆ—: {available_columns}\")\n",
    "        if missing_columns:\n",
    "            print(f\"ç¼ºå¤±å…³é”®åˆ—: {missing_columns}\")\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"é¢„å¤„ç†æ•°æ®\"\"\"\n",
    "        print(\"é¢„å¤„ç†æ•°æ®...\")\n",
    "        \n",
    "        # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "        for col in self.data.columns:\n",
    "            if self.data[col].dtype == 'object':\n",
    "                try:\n",
    "                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼\n",
    "                    self.data[col] = pd.to_numeric(self.data[col], errors='ignore')\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # å¤„ç†æ—¶é—´åˆ—\n",
    "        datetime_columns = ['departure_datetime', 'booking_datetime', 'arrival_datetime']\n",
    "        for col in datetime_columns:\n",
    "            if col in self.data.columns:\n",
    "                try:\n",
    "                    self.data[col] = pd.to_datetime(self.data[col], errors='coerce')\n",
    "                    print(f\"å¤„ç†æ—¶é—´åˆ—: {col}\")\n",
    "                except:\n",
    "                    print(f\"æ—¶é—´åˆ—å¤„ç†å¤±è´¥: {col}\")\n",
    "        \n",
    "        # ç‰¹å¾å·¥ç¨‹\n",
    "        if 'departure_datetime' in self.data.columns:\n",
    "            self.data['departure_hour'] = self.data['departure_datetime'].dt.hour\n",
    "            self.data['departure_day'] = self.data['departure_datetime'].dt.day_name()\n",
    "            self.data['departure_month'] = self.data['departure_datetime'].dt.month\n",
    "            self.data['is_weekend'] = self.data['departure_datetime'].dt.weekday >= 5\n",
    "        \n",
    "        if 'booking_datetime' in self.data.columns and 'departure_datetime' in self.data.columns:\n",
    "            self.data['advance_booking_days'] = (\n",
    "                self.data['departure_datetime'] - self.data['booking_datetime']\n",
    "            ).dt.days\n",
    "        \n",
    "        # ä»·æ ¼ç›¸å…³ç‰¹å¾\n",
    "        if 'price' in self.data.columns:\n",
    "            self.data['price_level'] = pd.cut(\n",
    "                self.data['price'], \n",
    "                bins=5, \n",
    "                labels=['æä½ä»·', 'ä½ä»·', 'ä¸­ä»·', 'é«˜ä»·', 'æé«˜ä»·']\n",
    "            )\n",
    "        \n",
    "        # èˆªçº¿å¤æ‚æ€§\n",
    "        if 'num_stops' in self.data.columns:\n",
    "            self.data['flight_type'] = self.data['num_stops'].apply(\n",
    "                lambda x: 'ç›´é£' if x == 0 else ('ä¸€æ¬¡ä¸­è½¬' if x == 1 else 'å¤šæ¬¡ä¸­è½¬')\n",
    "            )\n",
    "        \n",
    "        print(\"æ•°æ®é¢„å¤„ç†å®Œæˆ\")\n",
    "    \n",
    "    def _create_grouped_data(self):\n",
    "        \"\"\"åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®\"\"\"\n",
    "        # å¯»æ‰¾ç”¨æˆ·æ ‡è¯†åˆ—\n",
    "        user_id_col = 'ranker_id'\n",
    "        \n",
    "        if not user_id_col in self.data.columns:\n",
    "            print(\"è­¦å‘Š: æœªæ‰¾åˆ°ç”¨æˆ·æ ‡è¯†åˆ—ï¼Œå°†è¿›è¡Œæ•´ä½“åˆ†æ\")\n",
    "            self.grouped_data = None\n",
    "            return\n",
    "        \n",
    "        print(f\"ä½¿ç”¨ç”¨æˆ·æ ‡è¯†åˆ—: {user_id_col}\")\n",
    "        print(\"åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®...\")\n",
    "        \n",
    "        # æŒ‰ç”¨æˆ·åˆ†ç»„ç»Ÿè®¡\n",
    "        user_stats = []\n",
    "        user_groups = self.data.groupby(user_id_col)\n",
    "        \n",
    "        for user_id, group in user_groups:\n",
    "            if len(group) >= 3:  # åªå¤„ç†æœ‰è¶³å¤Ÿæ•°æ®çš„ç”¨æˆ·\n",
    "                stats = self._calculate_user_stats(group)\n",
    "                stats[user_id_col] = user_id\n",
    "                user_stats.append(stats)\n",
    "        \n",
    "        if user_stats:\n",
    "            self.grouped_data = pd.DataFrame(user_stats)\n",
    "            print(f\"åˆ›å»ºç”¨æˆ·åˆ†ç»„æ•°æ®å®Œæˆ | ç”¨æˆ·æ•°: {len(self.grouped_data):,}\")\n",
    "        else:\n",
    "            print(\"è­¦å‘Š: æ²¡æœ‰è¶³å¤Ÿçš„ç”¨æˆ·æ•°æ®è¿›è¡Œåˆ†ç»„åˆ†æ\")\n",
    "            self.grouped_data = None\n",
    "    \n",
    "    def _calculate_user_stats(self, group: pd.DataFrame) -> Dict:\n",
    "        \"\"\"è®¡ç®—ç”¨æˆ·ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        stats = {\n",
    "            'total_searches': len(group),\n",
    "            'avg_price': group['price'].mean() if 'price' in group.columns and not group['price'].isna().all() else 0,\n",
    "            'price_std': group['price'].std() if 'price' in group.columns and not group['price'].isna().all() else 0,\n",
    "            'price_sensitivity': self._calculate_price_sensitivity(group),\n",
    "            'preferred_departure_hour': group['departure_hour'].mode().iloc[0] if 'departure_hour' in group.columns and len(group['departure_hour'].dropna()) > 0 else 12,\n",
    "            'weekend_ratio': group['is_weekend'].mean() if 'is_weekend' in group.columns else 0,\n",
    "            'avg_advance_booking': group['advance_booking_days'].mean() if 'advance_booking_days' in group.columns and not group['advance_booking_days'].isna().all() else 0,\n",
    "            'direct_flight_ratio': (group['num_stops'] == 0).mean() if 'num_stops' in group.columns else 0,\n",
    "            'business_indicator': self._identify_business_traveler(group)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _calculate_price_sensitivity(self, group: pd.DataFrame) -> float:\n",
    "        \"\"\"è®¡ç®—ä»·æ ¼æ•æ„Ÿæ€§\"\"\"\n",
    "        if 'price' not in group.columns or len(group) < 3 or group['price'].isna().all():\n",
    "            return 0.5\n",
    "        \n",
    "        price_data = group['price'].dropna()\n",
    "        if len(price_data) < 3:\n",
    "            return 0.5\n",
    "        \n",
    "        price_ranges = price_data.quantile([0.25, 0.75])\n",
    "        low_price_selections = len(price_data[price_data <= price_ranges[0.25]])\n",
    "        high_price_selections = len(price_data[price_data >= price_ranges[0.75]])\n",
    "        \n",
    "        if low_price_selections + high_price_selections == 0:\n",
    "            return 0.5\n",
    "        \n",
    "        return low_price_selections / (low_price_selections + high_price_selections)\n",
    "    \n",
    "    def _identify_business_traveler(self, group: pd.DataFrame) -> float:\n",
    "        \"\"\"è¯†åˆ«å•†åŠ¡æ—…è¡Œè€…\"\"\"\n",
    "        business_indicators = []\n",
    "        \n",
    "        # å·¥ä½œæ—¥å‡ºè¡Œæ¯”ä¾‹\n",
    "        if 'is_weekend' in group.columns:\n",
    "            weekday_ratio = 1 - group['is_weekend'].mean()\n",
    "            business_indicators.append(weekday_ratio)\n",
    "        \n",
    "        # æ—©ç­æœºåå¥½\n",
    "        if 'departure_hour' in group.columns:\n",
    "            early_flight_ratio = (group['departure_hour'] <= 8).mean()\n",
    "            business_indicators.append(early_flight_ratio)\n",
    "        \n",
    "        # çŸ­æœŸé¢„è®¢\n",
    "        if 'advance_booking_days' in group.columns:\n",
    "            short_booking_data = group['advance_booking_days'].dropna()\n",
    "            if len(short_booking_data) > 0:\n",
    "                short_booking_ratio = (short_booking_data <= 7).mean()\n",
    "                business_indicators.append(short_booking_ratio)\n",
    "        \n",
    "        # ç›´é£åå¥½\n",
    "        if 'num_stops' in group.columns:\n",
    "            direct_ratio = (group['num_stops'] == 0).mean()\n",
    "            business_indicators.append(direct_ratio)\n",
    "        \n",
    "        return np.mean(business_indicators) if business_indicators else 0.5\n",
    "    \n",
    "    @memory_monitor\n",
    "    def analyze_overall_patterns(self):\n",
    "        \"\"\"åˆ†ææ•´ä½“æ¨¡å¼\"\"\"\n",
    "        print(\"åˆ†ææ•´ä½“æ¨¡å¼...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # ä»·æ ¼åˆ†æ\n",
    "        if 'price' in self.data.columns:\n",
    "            price_data = self.data['price'].dropna()\n",
    "            patterns['price_analysis'] = {\n",
    "                'mean': price_data.mean(),\n",
    "                'median': price_data.median(),\n",
    "                'std': price_data.std(),\n",
    "                'min': price_data.min(),\n",
    "                'max': price_data.max(),\n",
    "                'q25': price_data.quantile(0.25),\n",
    "                'q75': price_data.quantile(0.75)\n",
    "            }\n",
    "        \n",
    "        # æ—¶é—´æ¨¡å¼åˆ†æ\n",
    "        if 'departure_hour' in self.data.columns:\n",
    "            patterns['time_patterns'] = {\n",
    "                'peak_hours': self.data['departure_hour'].value_counts().head(5).to_dict(),\n",
    "                'weekend_vs_weekday': self.data['is_weekend'].value_counts().to_dict() if 'is_weekend' in self.data.columns else {}\n",
    "            }\n",
    "        \n",
    "        # èˆªçº¿ç±»å‹åˆ†æ\n",
    "        if 'num_stops' in self.data.columns:\n",
    "            patterns['flight_type_analysis'] = {\n",
    "                'direct_flights_ratio': (self.data['num_stops'] == 0).mean(),\n",
    "                'stops_distribution': self.data['num_stops'].value_counts().to_dict()\n",
    "            }\n",
    "        \n",
    "        # é¢„è®¢æ¨¡å¼åˆ†æ\n",
    "        if 'advance_booking_days' in self.data.columns:\n",
    "            booking_data = self.data['advance_booking_days'].dropna()\n",
    "            patterns['booking_patterns'] = {\n",
    "                'avg_advance_days': booking_data.mean(),\n",
    "                'median_advance_days': booking_data.median(),\n",
    "                'last_minute_ratio': (booking_data <= 1).mean(),\n",
    "                'planned_booking_ratio': (booking_data >= 14).mean()\n",
    "            }\n",
    "        \n",
    "        # å¯è§†åŒ–\n",
    "        self._create_overall_pattern_plots(patterns)\n",
    "        \n",
    "        self.analysis_results['overall_patterns'] = patterns\n",
    "        print(\"æ•´ä½“æ¨¡å¼åˆ†æå®Œæˆ\")\n",
    "    \n",
    "    def _create_overall_pattern_plots(self, patterns: Dict):\n",
    "        \"\"\"åˆ›å»ºæ•´ä½“æ¨¡å¼å›¾è¡¨\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('ä»·æ ¼åˆ†å¸ƒ', 'å‡ºå‘æ—¶é—´åˆ†å¸ƒ', 'èˆªç­ç±»å‹åˆ†å¸ƒ', 'é¢„è®¢æå‰å¤©æ•°åˆ†å¸ƒ'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # ä»·æ ¼åˆ†å¸ƒ\n",
    "        if 'price' in self.data.columns:\n",
    "            price_data = self.data['price'].dropna()\n",
    "            fig.add_trace(go.Histogram(x=price_data, name='ä»·æ ¼åˆ†å¸ƒ', nbinsx=50), row=1, col=1)\n",
    "        \n",
    "        # å‡ºå‘æ—¶é—´åˆ†å¸ƒ\n",
    "        if 'departure_hour' in self.data.columns:\n",
    "            hour_counts = self.data['departure_hour'].value_counts().sort_index()\n",
    "            fig.add_trace(go.Bar(x=hour_counts.index, y=hour_counts.values, name='å‡ºå‘æ—¶é—´'), row=1, col=2)\n",
    "        \n",
    "        # èˆªç­ç±»å‹åˆ†å¸ƒ\n",
    "        if 'num_stops' in self.data.columns:\n",
    "            stops_counts = self.data['num_stops'].value_counts().sort_index()\n",
    "            fig.add_trace(go.Bar(x=stops_counts.index, y=stops_counts.values, name='ä¸­è½¬æ¬¡æ•°'), row=2, col=1)\n",
    "        \n",
    "        # é¢„è®¢æå‰å¤©æ•°åˆ†å¸ƒ\n",
    "        if 'advance_booking_days' in self.data.columns:\n",
    "            booking_data = self.data['advance_booking_days'].dropna()\n",
    "            fig.add_trace(go.Histogram(x=booking_data, name='é¢„è®¢æå‰å¤©æ•°', nbinsx=50), row=2, col=2)\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"æ•´ä½“æ¨¡å¼åˆ†æ\")\n",
    "        fig.write_html(f\"{self.output_dir}/plots/overall_patterns.html\")\n",
    "    \n",
    "    @memory_monitor\n",
    "    def analyze_user_behavior(self):\n",
    "        \"\"\"åˆ†æç”¨æˆ·è¡Œä¸ºï¼ˆå¦‚æœæœ‰ç”¨æˆ·æ•°æ®ï¼‰\"\"\"\n",
    "        if self.grouped_data is None:\n",
    "            print(\"è·³è¿‡ç”¨æˆ·è¡Œä¸ºåˆ†æ - æ— ç”¨æˆ·åˆ†ç»„æ•°æ®\")\n",
    "            return\n",
    "        \n",
    "        print(\"åˆ†æç”¨æˆ·è¡Œä¸º...\")\n",
    "        \n",
    "        # ç”¨æˆ·åˆ†ç±»\n",
    "        if 'business_indicator' in self.grouped_data.columns:\n",
    "            threshold = self.grouped_data['business_indicator'].median()\n",
    "            business_users = self.grouped_data[self.grouped_data['business_indicator'] >= threshold]\n",
    "            leisure_users = self.grouped_data[self.grouped_data['business_indicator'] < threshold]\n",
    "            \n",
    "            behavior_analysis = {\n",
    "                'business_users': {\n",
    "                    'count': len(business_users),\n",
    "                    'avg_price': business_users['avg_price'].mean(),\n",
    "                    'price_sensitivity': business_users['price_sensitivity'].mean(),\n",
    "                    'advance_booking': business_users['avg_advance_booking'].mean(),\n",
    "                    'direct_flight_ratio': business_users['direct_flight_ratio'].mean(),\n",
    "                    'weekend_ratio': business_users['weekend_ratio'].mean()\n",
    "                },\n",
    "                'leisure_users': {\n",
    "                    'count': len(leisure_users),\n",
    "                    'avg_price': leisure_users['avg_price'].mean(),\n",
    "                    'price_sensitivity': leisure_users['price_sensitivity'].mean(),\n",
    "                    'advance_booking': leisure_users['avg_advance_booking'].mean(),\n",
    "                    'direct_flight_ratio': leisure_users['direct_flight_ratio'].mean(),\n",
    "                    'weekend_ratio': leisure_users['weekend_ratio'].mean()\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # å¯è§†åŒ–\n",
    "            self._create_user_behavior_plots(business_users, leisure_users)\n",
    "            \n",
    "            self.analysis_results['user_behavior'] = behavior_analysis\n",
    "            \n",
    "        print(\"ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆ\")\n",
    "    \n",
    "    def _create_user_behavior_plots(self, business_users: pd.DataFrame, leisure_users: pd.DataFrame):\n",
    "        \"\"\"åˆ›å»ºç”¨æˆ·è¡Œä¸ºå›¾è¡¨\"\"\"\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('ä»·æ ¼æ•æ„Ÿæ€§å¯¹æ¯”', 'å¹³å‡ä»·æ ¼å¯¹æ¯”', 'æå‰é¢„è®¢å¤©æ•°å¯¹æ¯”', 'ç›´é£åå¥½å¯¹æ¯”'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # ä»·æ ¼æ•æ„Ÿæ€§\n",
    "        fig.add_trace(go.Histogram(x=business_users['price_sensitivity'], name='å•†åŠ¡ç”¨æˆ·', \n",
    "                                 opacity=0.7, nbinsx=20), row=1, col=1)\n",
    "        fig.add_trace(go.Histogram(x=leisure_users['price_sensitivity'], name='ä¼‘é—²ç”¨æˆ·', \n",
    "                                 opacity=0.7, nbinsx=20), row=1, col=1)\n",
    "        \n",
    "        # å¹³å‡ä»·æ ¼\n",
    "        categories = ['å•†åŠ¡ç”¨æˆ·', 'ä¼‘é—²ç”¨æˆ·']\n",
    "        avg_prices = [business_users['avg_price'].mean(), leisure_users['avg_price'].mean()]\n",
    "        fig.add_trace(go.Bar(x=categories, y=avg_prices, name='å¹³å‡ä»·æ ¼'), row=1, col=2)\n",
    "        \n",
    "        # æå‰é¢„è®¢\n",
    "        fig.add_trace(go.Histogram(x=business_users['avg_advance_booking'], name='å•†åŠ¡ç”¨æˆ·', \n",
    "                                 opacity=0.7, nbinsx=20), row=2, col=1)\n",
    "        fig.add_trace(go.Histogram(x=leisure_users['avg_advance_booking'], name='ä¼‘é—²ç”¨æˆ·', \n",
    "                                 opacity=0.7, nbinsx=20), row=2, col=1)\n",
    "        \n",
    "        # ç›´é£åå¥½\n",
    "        direct_ratios = [business_users['direct_flight_ratio'].mean(), leisure_users['direct_flight_ratio'].mean()]\n",
    "        fig.add_trace(go.Bar(x=categories, y=direct_ratios, name='ç›´é£æ¯”ä¾‹'), row=2, col=2)\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"ç”¨æˆ·è¡Œä¸ºå¯¹æ¯”åˆ†æ\")\n",
    "        fig.write_html(f\"{self.output_dir}/plots/user_behavior_analysis.html\")\n",
    "    \n",
    "    @memory_monitor\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š\"\"\"\n",
    "        print(\"ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...\")\n",
    "        \n",
    "        report = f\"\"\"# èˆªç­æ•°æ®åˆ†ææŠ¥å‘Š\n",
    "\n",
    "## æ•°æ®æ¦‚å†µ\n",
    "- æ•°æ®æ–‡ä»¶: {self.data_path}\n",
    "- è®°å½•æ•°: {len(self.data):,}\n",
    "- å†…å­˜ä½¿ç”¨: {self.data_info['memory_usage_mb']:.1f} MB\n",
    "- åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## æ•°æ®ç»“æ„\n",
    "- æ•°æ®ç»´åº¦: {self.data_info['shape']}\n",
    "- åˆ—æ•°: {len(self.data_info['columns'])}\n",
    "- ä¸»è¦åˆ—: {', '.join(self.data_info['columns'][:10])}\n",
    "\n",
    "## ä¸»è¦å‘ç°\n",
    "\n",
    "### 1. æ•´ä½“æ¨¡å¼åˆ†æ\n",
    "\"\"\"\n",
    "        \n",
    "        if 'overall_patterns' in self.analysis_results:\n",
    "            patterns = self.analysis_results['overall_patterns']\n",
    "            \n",
    "            if 'price_analysis' in patterns:\n",
    "                price_info = patterns['price_analysis']\n",
    "                report += f\"\"\"\n",
    "**ä»·æ ¼åˆ†æ:**\n",
    "- å¹³å‡ä»·æ ¼: {price_info['mean']:.2f}\n",
    "- ä¸­ä½ä»·æ ¼: {price_info['median']:.2f}\n",
    "- ä»·æ ¼æ ‡å‡†å·®: {price_info['std']:.2f}\n",
    "- ä»·æ ¼èŒƒå›´: {price_info['min']:.2f} - {price_info['max']:.2f}\n",
    "\"\"\"\n",
    "            \n",
    "            if 'flight_type_analysis' in patterns:\n",
    "                flight_info = patterns['flight_type_analysis']\n",
    "                report += f\"\"\"\n",
    "**èˆªç­ç±»å‹åˆ†æ:**\n",
    "- ç›´é£æ¯”ä¾‹: {flight_info['direct_flights_ratio']:.2%}\n",
    "- ä¸­è½¬åˆ†å¸ƒ: {flight_info['stops_distribution']}\n",
    "\"\"\"\n",
    "            \n",
    "            if 'booking_patterns' in patterns:\n",
    "                booking_info = patterns['booking_patterns']\n",
    "                report += f\"\"\"\n",
    "**é¢„è®¢æ¨¡å¼åˆ†æ:**\n",
    "- å¹³å‡æå‰é¢„è®¢: {booking_info['avg_advance_days']:.1f} å¤©\n",
    "- ä¸´æ—¶é¢„è®¢æ¯”ä¾‹: {booking_info['last_minute_ratio']:.2%}\n",
    "- è®¡åˆ’é¢„è®¢æ¯”ä¾‹: {booking_info['planned_booking_ratio']:.2%}\n",
    "\"\"\"\n",
    "        \n",
    "        if 'user_behavior' in self.analysis_results:\n",
    "            behavior = self.analysis_results['user_behavior']\n",
    "            report += f\"\"\"\n",
    "### 2. ç”¨æˆ·è¡Œä¸ºåˆ†æ\n",
    "\n",
    "**å•†åŠ¡ç”¨æˆ· ({behavior['business_users']['count']}äºº):**\n",
    "- å¹³å‡ä»·æ ¼: {behavior['business_users']['avg_price']:.2f}\n",
    "- ä»·æ ¼æ•æ„Ÿæ€§: {behavior['business_users']['price_sensitivity']:.2f}\n",
    "- æå‰é¢„è®¢: {behavior['business_users']['advance_booking']:.1f} å¤©\n",
    "- ç›´é£åå¥½: {behavior['business_users']['direct_flight_ratio']:.2%}\n",
    "\n",
    "**ä¼‘é—²ç”¨æˆ· ({behavior['leisure_users']['count']}äºº):**\n",
    "- å¹³å‡ä»·æ ¼: {behavior['leisure_users']['avg_price']:.2f}\n",
    "- ä»·æ ¼æ•æ„Ÿæ€§: {behavior['leisure_users']['price_sensitivity']:.2f}\n",
    "- æå‰é¢„è®¢: {behavior['leisure_users']['advance_booking']:.1f} å¤©\n",
    "- ç›´é£åå¥½: {behavior['leisure_users']['direct_flight_ratio']:.2%}\n",
    "\"\"\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "## å•†ä¸šå»ºè®®\n",
    "\n",
    "### æ•´ä½“ç­–ç•¥\n",
    "1. ä¼˜åŒ–ä»·æ ¼ç­–ç•¥ï¼Œå…³æ³¨ä»·æ ¼æ•æ„Ÿç”¨æˆ·ç¾¤ä½“\n",
    "2. æä¾›å¤šæ ·åŒ–çš„èˆªç­é€‰æ‹©ï¼ˆç›´é£/ä¸­è½¬ï¼‰\n",
    "3. é’ˆå¯¹ä¸åŒé¢„è®¢ä¹ æƒ¯åˆ¶å®šå·®å¼‚åŒ–æœåŠ¡\n",
    "\n",
    "### æŠ€æœ¯å»ºè®®\n",
    "1. æ”¹è¿›æœç´¢æ’åºç®—æ³•ï¼Œè€ƒè™‘ç”¨æˆ·å†å²è¡Œä¸º\n",
    "2. å®æ–½ä¸ªæ€§åŒ–æ¨èç³»ç»Ÿ\n",
    "3. ä¼˜åŒ–ç§»åŠ¨ç«¯é¢„è®¢æµç¨‹\n",
    "\n",
    "---\n",
    "*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "*æ•°æ®æ¥æº: {self.data_path}*\n",
    "\"\"\"\n",
    "        \n",
    "        # ä¿å­˜æŠ¥å‘Š\n",
    "        report_path = f\"{self.output_dir}/reports/comprehensive_analysis.md\"\n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}\")\n",
    "    \n",
    "    def run_analysis(self):\n",
    "        \"\"\"è¿è¡Œå®Œæ•´åˆ†ææµç¨‹\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"å¼€å§‹èˆªç­æ•°æ®åˆ†æ...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        try:\n",
    "            # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†\n",
    "            self.load_and_preprocess_data()\n",
    "            \n",
    "            # 2. æ•´ä½“æ¨¡å¼åˆ†æ\n",
    "            self.analyze_overall_patterns()\n",
    "            \n",
    "            # 3. ç”¨æˆ·è¡Œä¸ºåˆ†æï¼ˆå¦‚æœæœ‰ç”¨æˆ·æ•°æ®ï¼‰\n",
    "            self.analyze_user_behavior()\n",
    "            \n",
    "            # 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š\n",
    "            self.generate_comprehensive_report()\n",
    "            \n",
    "            print(\"=\" * 60)\n",
    "            print(\"åˆ†æå®Œæˆï¼\")\n",
    "            print(\"=\" * 60)\n",
    "            print(\"æŸ¥çœ‹ç»“æœ:\")\n",
    "            print(f\"- æŠ¥å‘Š: {self.output_dir}/reports/\")\n",
    "            print(f\"- å›¾è¡¨: {self.output_dir}/plots/\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # æ¸…ç†å†…å­˜\n",
    "            self.cleanup()\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"æ¸…ç†å†…å­˜\"\"\"\n",
    "        if hasattr(self, 'data'):\n",
    "            del self.data\n",
    "        if hasattr(self, 'grouped_data'):\n",
    "            del self.grouped_data\n",
    "        gc.collect()\n",
    "        print(\"å†…å­˜æ¸…ç†å®Œæˆ\")\n",
    "\n",
    "# ä¾¿æ·å‡½æ•°\n",
    "def analyze_flight_data(data_path: str, output_dir: str = \"flight_analysis\", max_rows: int = 1000000):\n",
    "    \"\"\"åˆ†æèˆªç­æ•°æ®çš„ä¾¿æ·å‡½æ•°\"\"\"\n",
    "    analyzer = EnhancedFlightDataAnalyzer(data_path, output_dir, max_rows)\n",
    "    analyzer.run_analysis()\n",
    "    return analyzer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # é¦–å…ˆæµ‹è¯•æ•°æ®åŠ è½½\n",
    "    print(\"=\" * 60)\n",
    "    print(\"æµ‹è¯•æ•°æ®æ–‡ä»¶\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_files = [\n",
    "        \"../data/test.parquet\",\n",
    "        \"../data/train.parquet\"\n",
    "    ]\n",
    "    \n",
    "    # ç¤ºä¾‹1: åˆ†ææµ‹è¯•æ•°æ®\n",
    "    print(\"=\" * 60)\n",
    "    print(\"åˆ†ææµ‹è¯•æ•°æ®\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        test_analyzer = analyze_flight_data(\n",
    "            data_path=\"../data/test.parquet\",\n",
    "            output_dir=\"../data/test_flight_analysis_results\",\n",
    "            max_rows=500000  # é™åˆ¶æœ€å¤§è¡Œæ•°ä»¥æ§åˆ¶å†…å­˜\n",
    "        )\n",
    "        print(\"âœ… æµ‹è¯•æ•°æ®åˆ†æå®Œæˆï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æµ‹è¯•æ•°æ®åˆ†æå¤±è´¥: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"åˆ†æè®­ç»ƒæ•°æ®\")\n",
    "    print(\"=\" * 60)\n",
    "    # ç¤ºä¾‹2: åˆ†æè®­ç»ƒæ•°æ®\n",
    "    try:\n",
    "        train_analyzer = analyze_flight_data(\n",
    "            data_path=\"../data/train.parquet\",\n",
    "            output_dir=\"../data/train_flight_analysis_results\",\n",
    "            max_rows=500000  # é™åˆ¶æœ€å¤§è¡Œæ•°ä»¥æ§åˆ¶å†…å­˜\n",
    "        )\n",
    "        print(\"âœ… è®­ç»ƒæ•°æ®åˆ†æå®Œæˆï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è®­ç»ƒæ•°æ®åˆ†æå¤±è´¥: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"åˆ†æå®Œæˆï¼Œè¯·æŸ¥çœ‹è¾“å‡ºç›®å½•ä¸­çš„ç»“æœ\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ•°æ®...\n",
      "å¼€å§‹ç‰¹å¾å·¥ç¨‹...\n",
      "è®¡ç®—å…¨å±€ç»Ÿè®¡...\n",
      "åˆ›å»ºç”¨æˆ·ç‰¹å¾...\n",
      "åˆ›å»ºä¼ä¸šç‰¹å¾...\n",
      "åˆ›å»ºæ—¶é—´ç‰¹å¾...\n",
      "åˆ›å»ºä»·æ ¼ç‰¹å¾...\n",
      "åˆ›å»ºèˆªçº¿ç‰¹å¾...\n",
      "åˆ›å»ºæœåŠ¡ç‰¹å¾...\n",
      "åˆ›å»ºæ”¿ç­–ç‰¹å¾...\n",
      "åˆ›å»ºå¤åˆç‰¹å¾...\n",
      "åˆ›å»ºäº¤äº’ç‰¹å¾...\n",
      "å¤„ç†ç¼ºå¤±å€¼...\n",
      "ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: (6126014, 177)\n",
      "åŸå§‹æ•°æ®å½¢çŠ¶: (6126014, 177)\n",
      "å¤„ç†åæ•°æ®å½¢çŠ¶: (6126014, 177)\n",
      "æ–°å¢ç‰¹å¾æ•°é‡: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FlightFeatureEngineering:\n",
    "    \"\"\"\n",
    "    ç®€åŒ–ç‰ˆèˆªç­æ’åç‰¹å¾å·¥ç¨‹ç±»\n",
    "    ä¸“æ³¨äºæ ¸å¿ƒä¸šåŠ¡ç‰¹å¾æ„å»º\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.global_stats = {}\n",
    "    \n",
    "    def _safe_numeric_convert(self, series: pd.Series, default_value: float = 0.0) -> pd.Series:\n",
    "        \"\"\"å®‰å…¨æ•°å€¼è½¬æ¢\"\"\"\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            return pd.to_numeric(series, errors='coerce').fillna(default_value)\n",
    "        \n",
    "        # å¤„ç†å¸ƒå°”å­—ç¬¦ä¸²\n",
    "        bool_mapping = {\n",
    "            'true': 1, 'false': 0, 'True': 1, 'False': 0,\n",
    "            'yes': 1, 'no': 0, 'Y': 1, 'N': 0, '1': 1, '0': 0\n",
    "        }\n",
    "        \n",
    "        numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "        if numeric_series.isna().sum() > len(series) * 0.5:\n",
    "            mapped_series = series.astype(str).str.lower().map(bool_mapping)\n",
    "            numeric_series = numeric_series.fillna(mapped_series)\n",
    "        \n",
    "        return numeric_series.fillna(default_value)\n",
    "    \n",
    "    def _safe_datetime_convert(self, series: pd.Series) -> pd.Series:\n",
    "        \"\"\"å®‰å…¨æ—¶é—´è½¬æ¢\"\"\"\n",
    "        if pd.api.types.is_datetime64_any_dtype(series):\n",
    "            return series\n",
    "        try:\n",
    "            return pd.to_datetime(series, unit='s', errors='coerce')\n",
    "        except:\n",
    "            return pd.to_datetime(series, errors='coerce')\n",
    "    \n",
    "    def _safe_qcut(self, series: pd.Series, q: int, labels: list = None) -> pd.Series:\n",
    "        \"\"\"å®‰å…¨åˆ†ä½æ•°åˆ‡åˆ†\"\"\"\n",
    "        try:\n",
    "            # æ£€æŸ¥å”¯ä¸€å€¼æ•°é‡\n",
    "            unique_vals = series.nunique()\n",
    "            if unique_vals < q:\n",
    "                q = max(1, unique_vals)\n",
    "                if labels:\n",
    "                    labels = labels[:q]\n",
    "            \n",
    "            return pd.qcut(series, q=q, labels=labels, duplicates='drop')\n",
    "        except:\n",
    "            # å¦‚æœåˆ†ä½æ•°åˆ‡åˆ†å¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ†ç±»\n",
    "            if labels:\n",
    "                return pd.Series([labels[0]] * len(series), index=series.index)\n",
    "            else:\n",
    "                return pd.Series([0] * len(series), index=series.index)\n",
    "    \n",
    "    def compute_global_statistics(self, df: pd.DataFrame):\n",
    "        \"\"\"è®¡ç®—å…³é”®å…¨å±€ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        # ä»·æ ¼ç»Ÿè®¡\n",
    "        if 'totalPrice_bin' in df.columns:\n",
    "            price_series = self._safe_numeric_convert(df['totalPrice_bin'])\n",
    "            self.global_stats['price'] = {\n",
    "                'mean': price_series.mean(),\n",
    "                'std': max(price_series.std(), 1e-6),\n",
    "                'q25': price_series.quantile(0.25),\n",
    "                'q75': price_series.quantile(0.75)\n",
    "            }\n",
    "        \n",
    "        # èˆªç©ºå…¬å¸ç»Ÿè®¡\n",
    "        airline_cols = [col for col in df.columns if 'Carrier_code' in col]\n",
    "        if airline_cols:\n",
    "            all_airlines = []\n",
    "            for col in airline_cols:\n",
    "                all_airlines.extend(df[col].dropna().astype(str).tolist())\n",
    "            \n",
    "            if all_airlines:\n",
    "                airline_freq = pd.Series(all_airlines).value_counts()\n",
    "                self.global_stats['top_airlines'] = airline_freq.head(10).index.tolist()\n",
    "        \n",
    "        # æœºåœºç»Ÿè®¡\n",
    "        airport_cols = [col for col in df.columns if 'airport_iata' in col]\n",
    "        if airport_cols:\n",
    "            all_airports = []\n",
    "            for col in airport_cols:\n",
    "                all_airports.extend(df[col].dropna().astype(str).tolist())\n",
    "            \n",
    "            if all_airports:\n",
    "                airport_freq = pd.Series(all_airports).value_counts()\n",
    "                self.global_stats['top_airports'] = airport_freq.head(20).index.tolist()\n",
    "    \n",
    "    def create_user_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ„å»ºç”¨æˆ·ç‰¹å¾\"\"\"\n",
    "        # åŸºç¡€ç”¨æˆ·ç‰¹å¾\n",
    "        user_cols = ['bySelf', 'frequentFlyer', 'isVip', 'isAccess3D']\n",
    "        for col in user_cols:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_num'] = self._safe_numeric_convert(df[col])\n",
    "        \n",
    "        # ç”¨æˆ·æˆç†Ÿåº¦è¯„åˆ†\n",
    "        if all(col in df.columns for col in ['frequentFlyer', 'isVip']):\n",
    "            freq_num = self._safe_numeric_convert(df['frequentFlyer'])\n",
    "            vip_num = self._safe_numeric_convert(df['isVip'])\n",
    "            df['user_maturity_score'] = freq_num * 0.4 + vip_num * 0.6\n",
    "        \n",
    "        # æ€§åˆ«ç¼–ç \n",
    "        if 'sex' in df.columns:\n",
    "            df['gender_encoded'] = self._safe_numeric_convert(df['sex'], -1)\n",
    "        \n",
    "        # å›½ç±ç‰¹å¾\n",
    "        if 'nationality' in df.columns:\n",
    "            # ç®€å•ç¼–ç ï¼šæ˜¯å¦ä¸ºä¸»è¦å›½ç±\n",
    "            if 'top_airlines' in self.global_stats:  # ä½œä¸ºä»£ç†æŒ‡æ ‡\n",
    "                df['is_major_nationality'] = df['nationality'].notna().astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_corporate_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ„å»ºä¼ä¸šç‰¹å¾\"\"\"\n",
    "        # ä¼ä¸šç‰¹å¾\n",
    "        if 'companyID' in df.columns:\n",
    "            df['has_company'] = df['companyID'].notna().astype(int)\n",
    "            # ä¼ä¸šæ´»è·ƒåº¦ï¼ˆåŸºäºå‡ºç°é¢‘ç‡ï¼‰\n",
    "            if df['companyID'].notna().sum() > 0:\n",
    "                company_freq = df['companyID'].value_counts()\n",
    "                df['company_frequency'] = df['companyID'].map(company_freq).fillna(1)\n",
    "                df['is_frequent_company'] = (df['company_frequency'] > \n",
    "                                           df['company_frequency'].median()).astype(int)\n",
    "        \n",
    "        # ä¼ä¸šæ”¿ç­–ç‰¹å¾\n",
    "        if 'corporateTariffCode' in df.columns:\n",
    "            df['has_corporate_tariff'] = df['corporateTariffCode'].notna().astype(int)\n",
    "        \n",
    "        if 'pricingInfo_isAccessTP' in df.columns:\n",
    "            df['corporate_pricing_access'] = self._safe_numeric_convert(df['pricingInfo_isAccessTP'])\n",
    "        \n",
    "        # ä¹˜å®¢æ•°é‡\n",
    "        if 'pricingInfo_passengerCount' in df.columns:\n",
    "            passenger_count = self._safe_numeric_convert(df['pricingInfo_passengerCount'], 1)\n",
    "            df['passenger_count'] = passenger_count\n",
    "            df['is_group_booking'] = (passenger_count > 1).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ„å»ºæ—¶é—´ç‰¹å¾\"\"\"\n",
    "        # æ—¶é—´è½¬æ¢\n",
    "        time_cols = ['requestDate', 'legs0_departureAt', 'legs0_arrivalAt', \n",
    "                    'legs1_departureAt', 'legs1_arrivalAt']\n",
    "        for col in time_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = self._safe_datetime_convert(df[col])\n",
    "        \n",
    "        # é¢„è®¢æå‰æœŸ\n",
    "        if all(col in df.columns for col in ['requestDate', 'legs0_departureAt']):\n",
    "            df['booking_advance_days'] = (\n",
    "                df['legs0_departureAt'] - df['requestDate']\n",
    "            ).dt.days.fillna(0).clip(0, 365)\n",
    "            \n",
    "            # é¢„è®¢æ—¶æœºåˆ†ç±»ï¼ˆä¿®å¤åŸé”™è¯¯ï¼‰\n",
    "            advance_days = df['booking_advance_days']\n",
    "            booking_category = pd.cut(\n",
    "                advance_days,\n",
    "                bins=[-1, 7, 30, 90, 365],\n",
    "                labels=['LastMinute', 'Weekly', 'Monthly', 'Advanced']\n",
    "            )\n",
    "            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²åå†å¡«å……ç¼ºå¤±å€¼\n",
    "            df['booking_timing_category'] = booking_category.astype(str).replace('nan', 'Unknown')\n",
    "            \n",
    "            # é¢„è®¢ç´§æ€¥åº¦\n",
    "            df['booking_urgency'] = (df['booking_advance_days'] < 7).astype(int)\n",
    "            df['advance_booking'] = (df['booking_advance_days'] > 30).astype(int)\n",
    "        \n",
    "        # å‡ºå‘æ—¶é—´ç‰¹å¾\n",
    "        if 'legs0_departureAt' in df.columns:\n",
    "            departure_dt = df['legs0_departureAt']\n",
    "            df['departure_hour'] = departure_dt.dt.hour.fillna(12)\n",
    "            df['departure_day_of_week'] = departure_dt.dt.dayofweek.fillna(0)\n",
    "            df['departure_month'] = departure_dt.dt.month.fillna(1)\n",
    "            df['is_weekend_departure'] = (df['departure_day_of_week'] >= 5).astype(int)\n",
    "            \n",
    "            # å•†åŠ¡å‹å¥½æ—¶é—´\n",
    "            business_hours = [7, 8, 9, 17, 18, 19, 20]\n",
    "            df['business_friendly_departure'] = df['departure_hour'].isin(business_hours).astype(int)\n",
    "            \n",
    "            # æ—¶é—´æ®µåˆ†ç±»\n",
    "            def categorize_time(hour):\n",
    "                if pd.isna(hour):\n",
    "                    return 'Unknown'\n",
    "                elif 5 <= hour < 12:\n",
    "                    return 'Morning'\n",
    "                elif 12 <= hour < 18:\n",
    "                    return 'Afternoon'\n",
    "                elif 18 <= hour < 22:\n",
    "                    return 'Evening'\n",
    "                else:\n",
    "                    return 'Night'\n",
    "            \n",
    "            df['departure_period'] = df['departure_hour'].apply(categorize_time)\n",
    "        \n",
    "        # é£è¡Œæ—¶é•¿ç‰¹å¾\n",
    "        duration_cols = ['legs0_duration', 'legs1_duration']\n",
    "        for col in duration_cols:\n",
    "            if col in df.columns:\n",
    "                duration_hours = self._safe_numeric_convert(df[col]) / 3600\n",
    "                df[f'{col}_hours'] = duration_hours\n",
    "                df[f'{col}_is_long'] = (duration_hours > 6).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_pricing_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ„å»ºä»·æ ¼ç‰¹å¾\"\"\"\n",
    "        if 'totalPrice_bin' in df.columns:\n",
    "            price_numeric = self._safe_numeric_convert(df['totalPrice_bin'])\n",
    "            df['total_price'] = price_numeric\n",
    "            \n",
    "            # ç»„å†…ä»·æ ¼ç‰¹å¾\n",
    "            if 'ranker_id' in df.columns:\n",
    "                # ç»„å†…ä»·æ ¼æ’å\n",
    "                df['price_rank_in_group'] = df.groupby('ranker_id')['total_price'].rank(method='min')\n",
    "                df['price_percentile_in_group'] = df.groupby('ranker_id')['total_price'].rank(pct=True)\n",
    "                \n",
    "                # ç»„å†…ç»Ÿè®¡\n",
    "                group_stats = df.groupby('ranker_id')['total_price'].agg(['mean', 'std', 'min', 'max'])\n",
    "                group_stats['std'] = group_stats['std'].fillna(1e-6)\n",
    "                group_stats.columns = ['group_price_mean', 'group_price_std', 'group_price_min', 'group_price_max']\n",
    "                \n",
    "                df = df.merge(group_stats, left_on='ranker_id', right_index=True, how='left')\n",
    "                \n",
    "                # ä»·æ ¼ç›¸å¯¹ä½ç½®\n",
    "                df['price_above_group_mean'] = (df['total_price'] > df['group_price_mean']).astype(int)\n",
    "                df['is_cheapest_option'] = (df['price_rank_in_group'] == 1).astype(int)\n",
    "                df['is_premium_option'] = (df['price_percentile_in_group'] >= 0.8).astype(int)\n",
    "        \n",
    "        # å…¨å±€ä»·æ ¼ç‰¹å¾\n",
    "        if 'price' in self.global_stats and 'total_price' in df.columns:\n",
    "            stats = self.global_stats['price']\n",
    "            df['price_above_global_mean'] = (df['total_price'] > stats['mean']).astype(int)\n",
    "            df['price_global_zscore'] = (df['total_price'] - stats['mean']) / stats['std']\n",
    "        \n",
    "        # ç¨è´¹ç‰¹å¾\n",
    "        if 'taxes_bin' in df.columns and 'total_price' in df.columns:\n",
    "            taxes_numeric = self._safe_numeric_convert(df['taxes_bin'])\n",
    "            df['taxes_amount'] = taxes_numeric\n",
    "            df['tax_ratio'] = taxes_numeric / (df['total_price'] + 1e-6)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_route_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ„å»ºèˆªçº¿ç‰¹å¾\"\"\"\n",
    "        # èˆªæ®µæ•°é‡ç‰¹å¾\n",
    "        legs_counts = []\n",
    "        for leg in ['legs0', 'legs1']:\n",
    "            max_segments = 0\n",
    "            for i in range(5):  # æ£€æŸ¥æœ€å¤š5ä¸ªèˆªæ®µ\n",
    "                segment_col = f'{leg}_segments{i}_aircraft_code'\n",
    "                if segment_col in df.columns and not df[segment_col].isna().all():\n",
    "                    max_segments = i + 1\n",
    "            legs_counts.append(max_segments)\n",
    "            df[f'{leg}_segment_count'] = max_segments\n",
    "        \n",
    "        # æ€»èˆªæ®µæ•°å’Œå¤æ‚åº¦\n",
    "        df['total_segments'] = sum(legs_counts)\n",
    "        df['is_direct_flight'] = (df['total_segments'] == 2).astype(int)\n",
    "        df['has_connections'] = (df['total_segments'] > 2).astype(int)\n",
    "        \n",
    "        # èˆªç©ºå…¬å¸ç‰¹å¾\n",
    "        marketing_cols = [col for col in df.columns if 'marketingCarrier_code' in col]\n",
    "        if marketing_cols and 'top_airlines' in self.global_stats:\n",
    "            top_airlines = self.global_stats['top_airlines']\n",
    "            df['uses_major_airline'] = df[marketing_cols[0]].astype(str).isin(top_airlines).astype(int)\n",
    "        \n",
    "        # æœºåœºç‰¹å¾\n",
    "        departure_airports = [col for col in df.columns if 'departureFrom_airport_iata' in col]\n",
    "        arrival_airports = [col for col in df.columns if 'arrivalTo_airport_iata' in col]\n",
    "        \n",
    "        if departure_airports and 'top_airports' in self.global_stats:\n",
    "            top_airports = self.global_stats['top_airports']\n",
    "            df['departure_major_airport'] = df[departure_airports[0]].astype(str).isin(top_airports).astype(int)\n",
    "        \n",
    "        if arrival_airports and 'top_airports' in self.global_stats:\n",
    "            top_airports = self.global_stats['top_airports']\n",
    "            df['arrival_major_airport'] = df[arrival_airports[0]].astype(str).isin(top_airports).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_service_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ„å»ºæœåŠ¡ç‰¹å¾\"\"\"\n",
    "        # èˆ±ä½ç‰¹å¾\n",
    "        cabin_cols = [col for col in df.columns if 'cabinClass' in col]\n",
    "        if cabin_cols:\n",
    "            cabin_values = []\n",
    "            for col in cabin_cols:\n",
    "                cabin_values.append(self._safe_numeric_convert(df[col]))\n",
    "            \n",
    "            if cabin_values:\n",
    "                cabin_df = pd.DataFrame(cabin_values).T\n",
    "                df['highest_cabin_class'] = cabin_df.max(axis=1)\n",
    "                df['lowest_cabin_class'] = cabin_df.min(axis=1)\n",
    "                df['cabin_consistency'] = (cabin_df.max(axis=1) == cabin_df.min(axis=1)).astype(int)\n",
    "        \n",
    "        # åº§ä½å¯ç”¨æ€§\n",
    "        seat_cols = [col for col in df.columns if 'seatsAvailable' in col]\n",
    "        if seat_cols:\n",
    "            seat_values = []\n",
    "            for col in seat_cols:\n",
    "                seat_values.append(self._safe_numeric_convert(df[col]))\n",
    "            \n",
    "            if seat_values:\n",
    "                seat_df = pd.DataFrame(seat_values).T\n",
    "                df['min_seats_available'] = seat_df.min(axis=1)\n",
    "                df['seat_scarcity'] = (df['min_seats_available'] <= 5).astype(int)\n",
    "        \n",
    "        # è¡Œæç‰¹å¾\n",
    "        baggage_cols = [col for col in df.columns if 'baggageAllowance_quantity' in col]\n",
    "        if baggage_cols:\n",
    "            baggage_values = []\n",
    "            for col in baggage_cols:\n",
    "                baggage_values.append(self._safe_numeric_convert(df[col]))\n",
    "            \n",
    "            if baggage_values:\n",
    "                baggage_df = pd.DataFrame(baggage_values).T\n",
    "                df['min_baggage_pieces'] = baggage_df.min(axis=1)\n",
    "                df['generous_baggage_policy'] = (df['min_baggage_pieces'] >= 2).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_policy_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ„å»ºæ”¿ç­–ç‰¹å¾\"\"\"\n",
    "        # æ”¹ç­¾è´¹ç”¨ç‰¹å¾\n",
    "        amount_cols = [col for col in df.columns if 'miniRules' in col and 'monetaryAmount' in col]\n",
    "        if amount_cols:\n",
    "            amount_values = []\n",
    "            for col in amount_cols:\n",
    "                amount_values.append(self._safe_numeric_convert(df[col]))\n",
    "            \n",
    "            if amount_values:\n",
    "                amount_df = pd.DataFrame(amount_values).T\n",
    "                df['max_change_fee'] = amount_df.max(axis=1)\n",
    "                df['has_free_changes'] = (amount_df.min(axis=1) == 0).astype(int)\n",
    "        \n",
    "        # æ”¿ç­–å­˜åœ¨æ€§\n",
    "        policy_cols = [col for col in df.columns if 'miniRules' in col]\n",
    "        if policy_cols:\n",
    "            df['policy_count'] = df[policy_cols].count(axis=1)\n",
    "            df['has_flexible_policy'] = (df['policy_count'] > 0).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_composite_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ„å»ºå¤åˆç‰¹å¾\"\"\"\n",
    "        # å•†åŠ¡å‹å¥½åº¦è¯„åˆ†\n",
    "        business_factors = []\n",
    "        if 'business_friendly_departure' in df.columns:\n",
    "            business_factors.append(df['business_friendly_departure'])\n",
    "        if 'is_direct_flight' in df.columns:\n",
    "            business_factors.append(df['is_direct_flight'])\n",
    "        if 'departure_major_airport' in df.columns:\n",
    "            business_factors.append(df['departure_major_airport'])\n",
    "        \n",
    "        if business_factors:\n",
    "            df['business_friendliness_score'] = np.mean(business_factors, axis=0)\n",
    "        \n",
    "        # æ€§ä»·æ¯”è¯„åˆ†\n",
    "        value_factors = []\n",
    "        if 'price_percentile_in_group' in df.columns:\n",
    "            value_factors.append(1 - df['price_percentile_in_group'])  # ä»·æ ¼è¶Šä½è¶Šå¥½\n",
    "        if 'business_friendliness_score' in df.columns:\n",
    "            value_factors.append(df['business_friendliness_score'])\n",
    "        \n",
    "        if value_factors:\n",
    "            df['value_for_money_score'] = np.mean(value_factors, axis=0)\n",
    "        \n",
    "        # ç”¨æˆ·åŒ¹é…åº¦\n",
    "        if 'user_maturity_score' in df.columns and 'business_friendliness_score' in df.columns:\n",
    "            df['user_match_score'] = df['user_maturity_score'] * df['business_friendliness_score']\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"æ„å»ºäº¤äº’ç‰¹å¾\"\"\"\n",
    "        # ç”¨æˆ·-ä»·æ ¼äº¤äº’\n",
    "        if all(col in df.columns for col in ['user_maturity_score', 'price_percentile_in_group']):\n",
    "            df['user_price_sensitivity'] = df['user_maturity_score'] * (1 - df['price_percentile_in_group'])\n",
    "        \n",
    "        # æ—¶é—´-ä»·æ ¼äº¤äº’\n",
    "        if all(col in df.columns for col in ['booking_urgency', 'price_percentile_in_group']):\n",
    "            df['urgency_price_tolerance'] = df['booking_urgency'] * df['price_percentile_in_group']\n",
    "        \n",
    "        # ä¼ä¸š-æœåŠ¡äº¤äº’\n",
    "        if all(col in df.columns for col in ['has_company', 'business_friendliness_score']):\n",
    "            df['company_service_match'] = df['has_company'] * df['business_friendliness_score']\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def process_features(self, df: pd.DataFrame, is_training: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"ä¸»è¦ç‰¹å¾å¤„ç†æµç¨‹\"\"\"\n",
    "        print(\"å¼€å§‹ç‰¹å¾å·¥ç¨‹...\")\n",
    "        \n",
    "        # è®¡ç®—å…¨å±€ç»Ÿè®¡ï¼ˆä»…è®­ç»ƒæ—¶ï¼‰\n",
    "        if is_training:\n",
    "            print(\"è®¡ç®—å…¨å±€ç»Ÿè®¡...\")\n",
    "            self.compute_global_statistics(df)\n",
    "        \n",
    "        # åˆ›å»ºå„ç±»ç‰¹å¾\n",
    "        print(\"åˆ›å»ºç”¨æˆ·ç‰¹å¾...\")\n",
    "        df = self.create_user_features(df)\n",
    "        \n",
    "        print(\"åˆ›å»ºä¼ä¸šç‰¹å¾...\")\n",
    "        df = self.create_corporate_features(df)\n",
    "        \n",
    "        print(\"åˆ›å»ºæ—¶é—´ç‰¹å¾...\")\n",
    "        df = self.create_temporal_features(df)\n",
    "        \n",
    "        print(\"åˆ›å»ºä»·æ ¼ç‰¹å¾...\")\n",
    "        df = self.create_pricing_features(df)\n",
    "        \n",
    "        print(\"åˆ›å»ºèˆªçº¿ç‰¹å¾...\")\n",
    "        df = self.create_route_features(df)\n",
    "        \n",
    "        print(\"åˆ›å»ºæœåŠ¡ç‰¹å¾...\")\n",
    "        df = self.create_service_features(df)\n",
    "        \n",
    "        print(\"åˆ›å»ºæ”¿ç­–ç‰¹å¾...\")\n",
    "        df = self.create_policy_features(df)\n",
    "        \n",
    "        print(\"åˆ›å»ºå¤åˆç‰¹å¾...\")\n",
    "        df = self.create_composite_features(df)\n",
    "        \n",
    "        print(\"åˆ›å»ºäº¤äº’ç‰¹å¾...\")\n",
    "        df = self.create_interaction_features(df)\n",
    "        \n",
    "        # å¤„ç†ç¼ºå¤±å€¼\n",
    "        print(\"å¤„ç†ç¼ºå¤±å€¼...\")\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        exclude_cols = ['ranker_id', 'Id'] + (['selected'] if 'selected' in df.columns else [])\n",
    "        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        # å¤„ç†æ— é™å€¼\n",
    "        df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        print(f\"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨\n",
    "    engineer = FlightFeatureEngineering()\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    print(\"åŠ è½½æ•°æ®...\")\n",
    "    train_data = pd.read_parquet(\"D:/kaggle/filght/data/aeroclub-recsys-2025/segment/train/train_segment_0.parquet\")\n",
    "    \n",
    "    # è®­ç»ƒæ¨¡å¼ç‰¹å¾å·¥ç¨‹\n",
    "    processed_data = engineer.process_features(train_data, is_training=True)\n",
    "    \n",
    "    print(f\"åŸå§‹æ•°æ®å½¢çŠ¶: {train_data.shape}\")\n",
    "    print(f\"å¤„ç†åæ•°æ®å½¢çŠ¶: {processed_data.shape}\")\n",
    "    print(f\"æ–°å¢ç‰¹å¾æ•°é‡: {processed_data.shape[1] - train_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = processed_data\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªæ–°çš„DataFrameæ¥å­˜å‚¨ç»“æœ\n",
    "result_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# ä¸ºæ¯åˆ—è·å–å‰5ä¸ªéç©ºå”¯ä¸€å€¼ï¼Œå¹¶æ ¼å¼åŒ–ä¸º\"[type] value\"\n",
    "for column in df.columns:\n",
    "    non_na_values = df[column].dropna()\n",
    "    unique_values = non_na_values.unique()[:5]\n",
    "    dtype = str(df[column].dtype)\n",
    "    \n",
    "    # æ ¼å¼åŒ–ä¸º\"[type] value\"å¹¶å¡«å……åˆ°ç»“æœDataFrameä¸­\n",
    "    formatted_values = [f\"[{dtype}] {value}\" for value in unique_values]\n",
    "    result_df[column] = pd.Series(formatted_values)\n",
    "\n",
    "# ä¿å­˜ä¸ºCSVæ–‡ä»¶\n",
    "result_df.to_csv('output.csv', index=False)\n",
    "\n",
    "print(\"å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°output.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

"""
æ’åºæ¨¡å‹å®šä¹‰æ–‡ä»¶ - PyTorchç‰ˆæœ¬

è¯¥æ¨¡å—åŒ…å«æ‰€æœ‰æ’åºæ¨¡å‹çš„å®šä¹‰å’Œå®ç°
- å°†TensorFlow/Kerasæ¨¡å‹è½¬æ¢ä¸ºPyTorchå®ç°
- æ”¹è¿›äº†å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒç¨³å®šæ€§

ä½œè€…: Flight Ranking Team
ç‰ˆæœ¬: 3.0 (PyTorchç‰ˆæœ¬)
"""
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from xgboost import XGBRanker
from lightgbm import LGBMRanker
from rank_bm25 import BM25Okapi
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Union, Any
import warnings
import math

warnings.filterwarnings('ignore')

# è®¾ç½®PyTorchè®¾å¤‡
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


class BaseRanker(ABC):
    """æ’åºæ¨¡å‹åŸºç±»"""
    
    @abstractmethod
    def fit(self, X, y, group, **kwargs):
        """è®­ç»ƒæ¨¡å‹"""
        pass
    
    @abstractmethod
    def predict(self, X):
        """é¢„æµ‹åˆ†æ•°"""
        pass
    
    @abstractmethod
    def get_model_name(self):
        """è·å–æ¨¡å‹åç§°"""
        pass
    
    def get_params(self):
        """è·å–æ¨¡å‹å‚æ•°"""
        return getattr(self, 'params', {})
    
    def set_params(self, **params):
        """è®¾ç½®æ¨¡å‹å‚æ•°"""
        self.params = params
        return self


class XGBRankerModel(BaseRanker):
    """XGBoostæ’åºæ¨¡å‹"""
    
    def __init__(self, use_gpu=True, **params):
        default_params = {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'eval_metric': 'ndcg'
        }
        default_params.update(params)
        
        if use_gpu:
            default_params.update({
                'tree_method': 'gpu_hist',
                'predictor': 'gpu_predictor'
            })
        else:
            default_params['n_jobs'] = -1
        
        self.params = default_params
        self.model = XGBRanker(**default_params)
        
    def fit(self, X, y, group, **kwargs):
        self.model.fit(X, y, group=group, **kwargs)
    
    def predict(self, X):
        return self.model.predict(X)
    
    def get_model_name(self):
        return "XGBRanker"
    
    @property
    def feature_importances_(self):
        return self.model.feature_importances_


class LGBMRankerModel(BaseRanker):
    """LightGBMæ’åºæ¨¡å‹"""
    
    def __init__(self, use_gpu=True, **params):
        default_params = {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'objective': 'lambdarank',
            'metric': 'ndcg'
        }
        default_params.update(params)
        
        if use_gpu:
            default_params['device'] = 'gpu'
        else:
            default_params['n_jobs'] = -1
        
        self.params = default_params
        self.model = LGBMRanker(**default_params)
        
    def fit(self, X, y, group, **kwargs):
        self.model.fit(X, y, group=group, **kwargs)
    
    def predict(self, X):
        return self.model.predict(X)
    
    def get_model_name(self):
        return "LGBMRanker"
    
    @property
    def feature_importances_(self):
        return self.model.feature_importances_


class LambdaMART(BaseRanker):
    """LambdaMARTå®ç°ï¼ˆåŸºäºXGBoostï¼‰"""
    
    def __init__(self, use_gpu=True, **params):
        default_params = {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42
        }
        default_params.update(params)
        
        xgb_params = {
            'objective': "rank:pairwise",
            **default_params
        }
        
        if use_gpu:
            xgb_params.update({
                'tree_method': 'gpu_hist',
                'predictor': 'gpu_predictor'
            })
        else:
            xgb_params['n_jobs'] = -1
        
        self.params = default_params
        self.model = XGBRanker(**xgb_params)
        
    def fit(self, X, y, group, **kwargs):
        self.model.fit(X, y, group=group, **kwargs)
    
    def predict(self, X):
        return self.model.predict(X)
    
    def get_model_name(self):
        return "LambdaMART"
    
    @property
    def feature_importances_(self):
        return self.model.feature_importances_


class ListNetModel(BaseRanker):
    """ListNetå®ç°ï¼ˆåŸºäºLightGBMï¼‰"""
    
    def __init__(self, use_gpu=True, **params):
        default_params = {
            'n_estimators': 100,
            'learning_rate': 0.05,
            'max_depth': 7,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42
        }
        default_params.update(params)
        
        lgb_params = {
            'objective': "lambdarank",
            'metric': "ndcg",
            **default_params
        }
        
        if use_gpu:
            lgb_params['device'] = 'gpu'
        else:
            lgb_params['n_jobs'] = -1
        
        self.params = default_params
        self.model = LGBMRanker(**lgb_params)
        
    def fit(self, X, y, group, **kwargs):
        self.model.fit(X, y, group=group, **kwargs)
    
    def predict(self, X):
        return self.model.predict(X)
    
    def get_model_name(self):
        return "ListNet"
    
    @property
    def feature_importances_(self):
        return self.model.feature_importances_


class NeuralRanker(nn.Module, BaseRanker):
    """PyTorchç¥ç»ç½‘ç»œæ’åºæ¨¡å‹"""
    
    def __init__(self, input_dim, **params):
        super(NeuralRanker, self).__init__()
        default_params = {
            'hidden_units': [256, 128, 64],
            'learning_rate': 0.001,
            'epochs': 10,
            'batch_size': 32,
            'dropout_rate': 0.2
        }
        default_params.update(params)
        
        self.input_dim = input_dim
        self.params = default_params
        self._feature_importance = None
        
        # æ„å»ºç½‘ç»œå±‚
        layers = []
        prev_dim = input_dim
        
        for i, units in enumerate(self.params['hidden_units']):
            layers.append(nn.Linear(prev_dim, units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(self.params['dropout_rate']))
            prev_dim = units
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, 1))
        
        self.network = nn.Sequential(*layers)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.to(DEVICE)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.parameters(), lr=self.params['learning_rate'])
        self.criterion = nn.MSELoss()
        
    def forward(self, x):
        return self.network(x)
    
    def fit(self, X, y, group, **kwargs):
        epochs = kwargs.get('epochs', self.params['epochs'])
        batch_size = kwargs.get('batch_size', self.params['batch_size'])
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_tensor = torch.FloatTensor(X).to(DEVICE)
        y_tensor = torch.FloatTensor(y).reshape(-1, 1).to(DEVICE)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataset = TensorDataset(X_tensor, y_tensor)
        
        # æ‰‹åŠ¨åˆ†å‰²éªŒè¯é›†ï¼ˆå¦‚æœæ•°æ®é‡è¶³å¤Ÿå¤§ï¼‰
        if len(X) > 1000:
            val_size = int(len(X) * 0.2)
            train_size = len(X) - val_size
            
            train_dataset = TensorDataset(X_tensor[:train_size], y_tensor[:train_size])
            val_dataset = TensorDataset(X_tensor[train_size:], y_tensor[train_size:])
            
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
            
            # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦éªŒè¯ï¼‰
            for epoch in range(epochs):
                # è®­ç»ƒé˜¶æ®µ
                self.train()
                train_loss = 0.0
                for batch_X, batch_y in train_loader:
                    self.optimizer.zero_grad()
                    outputs = self(batch_X)
                    loss = self.criterion(outputs, batch_y)
                    loss.backward()
                    self.optimizer.step()
                    train_loss += loss.item()
                
                # éªŒè¯é˜¶æ®µ
                if epoch % 5 == 0:
                    self.eval()
                    val_loss = 0.0
                    with torch.no_grad():
                        for batch_X, batch_y in val_loader:
                            outputs = self(batch_X)
                            val_loss += self.criterion(outputs, batch_y).item()
                    
                    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')
        else:
            # è®­ç»ƒæ¨¡å‹ï¼ˆæ— éªŒè¯ï¼‰
            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
            
            for epoch in range(epochs):
                self.train()
                total_loss = 0.0
                for batch_X, batch_y in train_loader:
                    self.optimizer.zero_grad()
                    outputs = self(batch_X)
                    loss = self.criterion(outputs, batch_y)
                    loss.backward()
                    self.optimizer.step()
                    total_loss += loss.item()
                
                if epoch % 5 == 0:
                    print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§
        self._compute_feature_importance(X[:min(1000, len(X))])
    
    def predict(self, X):
        self.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(DEVICE)
            outputs = self(X_tensor)
            return outputs.cpu().numpy().flatten()
    
    def _compute_feature_importance(self, X_sample):
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§"""
        try:
            self.eval()
            X_tensor = torch.FloatTensor(X_sample).to(DEVICE)
            X_tensor.requires_grad_(True)
            
            outputs = self(X_tensor)
            loss = outputs.mean()
            loss.backward()
            
            # ä½¿ç”¨æ¢¯åº¦çš„ç»å¯¹å€¼å‡å€¼ä½œä¸ºç‰¹å¾é‡è¦æ€§
            grads = X_tensor.grad
            if grads is not None:
                importance = torch.mean(torch.abs(grads), dim=0).cpu().numpy()
                # å½’ä¸€åŒ–
                self._feature_importance = importance / (np.sum(importance) + 1e-8)
            else:
                # å¦‚æœæ— æ³•è®¡ç®—æ¢¯åº¦ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
                self._feature_importance = np.ones(self.input_dim) / self.input_dim
                
        except Exception as e:
            print(f"è®¡ç®—NeuralRankerç‰¹å¾é‡è¦æ€§æ—¶å‡ºé”™: {e}")
            # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºåå¤‡æ–¹æ¡ˆ
            self._feature_importance = np.ones(self.input_dim) / self.input_dim
    
    def get_model_name(self):
        return "NeuralRanker"
    
    @property
    def feature_importances_(self):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self._feature_importance is None:
            return np.ones(self.input_dim) / self.input_dim
        return self._feature_importance


class RankNet(nn.Module, BaseRanker):
    """PyTorch RankNetæ’åºæ¨¡å‹"""
    
    def __init__(self, input_dim, **params):
        super(RankNet, self).__init__()
        default_params = {
            'hidden_units': [128, 64, 32],
            'learning_rate': 0.001,
            'epochs': 15,
            'batch_size': 64,
            'dropout_rate': 0.3
        }
        default_params.update(params)
        
        self.input_dim = input_dim
        self.params = default_params
        self._feature_importance = None
        
        print(f"ğŸ”§ åˆå§‹åŒ–RankNet:")
        print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
        print(f"   éšè—å±‚å•å…ƒ: {self.params['hidden_units']}")
        print(f"   å­¦ä¹ ç‡: {self.params['learning_rate']}")
        
        # æ„å»ºç½‘ç»œå±‚
        layers = []
        prev_dim = input_dim
        
        for i, units in enumerate(self.params['hidden_units']):
            layers.extend([
                nn.Linear(prev_dim, units),
                nn.ReLU(),
                nn.Dropout(self.params['dropout_rate']),
                nn.BatchNorm1d(units)
            ])
            prev_dim = units
        
        # è¾“å‡ºå±‚ - è¾“å‡ºå•ä¸ªåˆ†æ•°
        layers.append(nn.Linear(prev_dim, 1))
        
        self.network = nn.Sequential(*layers)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.to(DEVICE)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = optim.Adam(self.parameters(), lr=self.params['learning_rate'])
        self.criterion = nn.MSELoss()
        
        print("âœ… RankNetæ¨¡å‹æ„å»ºæˆåŠŸ")
        
    def forward(self, x):
        return self.network(x)
    
    def fit(self, X, y, group, **kwargs):
        """è®­ç»ƒRankNetæ¨¡å‹"""
        epochs = kwargs.get('epochs', self.params['epochs'])
        batch_size = kwargs.get('batch_size', self.params['batch_size'])
        
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒRankNetæ¨¡å‹ï¼Œæ•°æ®å½¢çŠ¶: {X.shape}")
        print(f"ğŸ“Š è®­ç»ƒå‚æ•°: epochs={epochs}, batch_size={batch_size}")
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_tensor = torch.FloatTensor(X).to(DEVICE)
        y_tensor = torch.FloatTensor(y).reshape(-1, 1).to(DEVICE)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataset = TensorDataset(X_tensor, y_tensor)
        
        # æ‰‹åŠ¨å¤„ç†éªŒè¯é›†åˆ†å‰²
        if len(X) > 1000:
            # è®¡ç®—éªŒè¯é›†å¤§å°
            val_size = int(len(X) * 0.2)
            train_size = len(X) - val_size
            
            # åˆ†å‰²æ•°æ®
            train_dataset = TensorDataset(X_tensor[:train_size], y_tensor[:train_size])
            val_dataset = TensorDataset(X_tensor[train_size:], y_tensor[train_size:])
            
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
            
            # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦éªŒè¯ï¼‰
            for epoch in range(epochs):
                # è®­ç»ƒé˜¶æ®µ
                self.train()
                train_loss = 0.0
                for batch_X, batch_y in train_loader:
                    self.optimizer.zero_grad()
                    outputs = self(batch_X)
                    loss = self.criterion(outputs, batch_y)
                    loss.backward()
                    self.optimizer.step()
                    train_loss += loss.item()
                
                # éªŒè¯é˜¶æ®µ
                if epoch % 5 == 0:
                    self.eval()
                    val_loss = 0.0
                    with torch.no_grad():
                        for batch_X, batch_y in val_loader:
                            outputs = self(batch_X)
                            val_loss += self.criterion(outputs, batch_y).item()
                    
                    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')
        else:
            # è®­ç»ƒæ¨¡å‹ï¼ˆæ— éªŒè¯ï¼‰
            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
            
            for epoch in range(epochs):
                self.train()
                total_loss = 0.0
                for batch_X, batch_y in train_loader:
                    self.optimizer.zero_grad()
                    outputs = self(batch_X)
                    loss = self.criterion(outputs, batch_y)
                    loss.backward()
                    self.optimizer.step()
                    total_loss += loss.item()
                
                if epoch % 5 == 0:
                    print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')
        
        # è®¡ç®—ç‰¹å¾é‡è¦æ€§
        self._compute_feature_importance(X[:min(1000, len(X))])
        
        print("âœ… RankNetæ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def _compute_feature_importance(self, X_sample):
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§"""
        try:
            self.eval()
            X_tensor = torch.FloatTensor(X_sample).to(DEVICE)
            X_tensor.requires_grad_(True)
            
            outputs = self(X_tensor)
            loss = outputs.mean()
            loss.backward()
            
            # ä½¿ç”¨æ¢¯åº¦çš„ç»å¯¹å€¼å‡å€¼ä½œä¸ºç‰¹å¾é‡è¦æ€§
            grads = X_tensor.grad
            if grads is not None:
                importance = torch.mean(torch.abs(grads), dim=0).cpu().numpy()
                # å½’ä¸€åŒ–
                self._feature_importance = importance / (np.sum(importance) + 1e-8)
            else:
                # å¦‚æœæ— æ³•è®¡ç®—æ¢¯åº¦ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
                self._feature_importance = np.ones(self.input_dim) / self.input_dim
                
        except Exception as e:
            print(f"è®¡ç®—RankNetç‰¹å¾é‡è¦æ€§æ—¶å‡ºé”™: {e}")
            # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºåå¤‡æ–¹æ¡ˆ
            self._feature_importance = np.ones(self.input_dim) / self.input_dim
    
    def predict(self, X):
        """é¢„æµ‹åˆ†æ•°"""
        self.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(DEVICE)
            outputs = self(X_tensor)
            return outputs.cpu().numpy().flatten()
    
    def get_model_name(self):
        return "RankNet"
    
    @property
    def feature_importances_(self):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self._feature_importance is None:
            return np.ones(self.input_dim) / self.input_dim
        return self._feature_importance


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, d_model, num_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # åº”ç”¨softmax
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        output = torch.matmul(attn_weights, V)
        
        return output, attn_weights
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # çº¿æ€§å˜æ¢å¹¶åˆ†å¤´
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # åˆå¹¶å¤šå¤´
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # æœ€ç»ˆçº¿æ€§å˜æ¢
        output = self.W_o(attn_output)
        
        return output


class TransformerBlock(nn.Module):
    """Transformerå—"""
    
    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):
        super(TransformerBlock, self).__init__()
        self.attention = MultiHeadAttention(d_model, num_heads, dropout_rate)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, dff),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(dff, d_model)
        )
        
        self.layernorm1 = nn.LayerNorm(d_model)
        self.layernorm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout_rate)
        self.dropout2 = nn.Dropout(dropout_rate)
    
    def forward(self, x):
        # å¤šå¤´è‡ªæ³¨æ„åŠ›
        attn_output = self.attention(x, x, x)
        x = self.layernorm1(x + self.dropout1(attn_output))
        
        # å‰é¦ˆç½‘ç»œ
        ff_output = self.feed_forward(x)
        x = self.layernorm2(x + self.dropout2(ff_output))
        
        return x


class TransformerRanker(nn.Module, BaseRanker):
    """PyTorch Transformeræ’åºæ¨¡å‹"""
    
    def __init__(self, input_dim, **params):
        super(TransformerRanker, self).__init__()
        # ä½¿ç”¨æ›´ä¿å®ˆçš„é»˜è®¤å‚æ•°
        default_params = {
            'num_heads': 4,           # å‡å°‘æ³¨æ„åŠ›å¤´æ•°
            'num_layers': 2,          # å‡å°‘å±‚æ•°
            'd_model': 64,            # å‡å°‘æ¨¡å‹ç»´åº¦
            'dff': 128,               # å‡å°‘å‰é¦ˆç½‘ç»œç»´åº¦
            'learning_rate': 0.001,
            'epochs': 10,
            'batch_size': 64,
            'dropout_rate': 0.1,
            'max_seq_length': 16      # å›ºå®šåºåˆ—é•¿åº¦
        }
        default_params.update(params)
        
        self.input_dim = input_dim
        self.params = default_params
        self._feature_importance = None
        
        print(f"ğŸ”§ åˆå§‹åŒ–TransformerRanker:")
        print(f"   è¾“å…¥ç»´åº¦: {input_dim}")
        print(f"   åºåˆ—é•¿åº¦: {self.params['max_seq_length']}")
        print(f"   æ¨¡å‹ç»´åº¦: {self.params['d_model']}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {self.params['num_heads']}")
        
        try:
            self._build_model()
            print("âœ… TransformerRankeræ¨¡å‹æ„å»ºæˆåŠŸ")
        except Exception as e:
            print(f"âŒ TransformerRankeræ„å»ºå¤±è´¥: {e}")
            raise
    
    def _build_model(self):
        """æ„å»ºTransformeræ¨¡å‹"""
        seq_len = self.params['max_seq_length']
        d_model = self.params['d_model']
        
        # ç‰¹å¾é¢„å¤„ç†å’Œç»´åº¦è°ƒæ•´
        if self.input_dim % seq_len == 0:
            features_per_token = self.input_dim // seq_len
            if features_per_token != d_model:
                self.feature_projection = nn.Linear(features_per_token, d_model)
            else:
                self.feature_projection = nn.Identity()
            
            self.reshape_method = "divide"
        else:
            # ä½¿ç”¨çº¿æ€§å˜æ¢
            target_dim = seq_len * d_model
            self.linear_projection = nn.Linear(self.input_dim, target_dim)
            self.reshape_method = "linear"
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = self._create_positional_encoding(seq_len, d_model)
        
        # Transformerå±‚
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, self.params['num_heads'], 
                           self.params['dff'], self.params['dropout_rate'])
            for _ in range(self.params['num_layers'])
        ])
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model * 2, 128),  # *2 å› ä¸ºç”¨äº†avgå’Œmax pooling
            nn.ReLU(),
            nn.Dropout(self.params['dropout_rate']),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(self.params['dropout_rate']),
            nn.Linear(64, 1)
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.to(DEVICE)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.parameters(), 
                                  lr=self.params['learning_rate'])
        self.criterion = nn.MSELoss()
    
    def _create_positional_encoding(self, seq_len, d_model):
        """åˆ›å»ºä½ç½®ç¼–ç """
        pe = torch.zeros(seq_len, d_model)
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe.unsqueeze(0)  # (1, seq_len, d_model)
    
    def forward(self, x):
        batch_size = x.size(0)
        seq_len = self.params['max_seq_length']
        d_model = self.params['d_model']
        
        # ç‰¹å¾é‡å¡‘
        if self.reshape_method == "divide":
            features_per_token = self.input_dim // seq_len
            x = x.view(batch_size, seq_len, features_per_token)
            x = self.feature_projection(x)
        else:
            x = self.linear_projection(x)
            x = x.view(batch_size, seq_len, d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        pos_encoding = self.pos_encoding.to(x.device)
        x = x + pos_encoding
        
        # Transformerå±‚
        for transformer_block in self.transformer_blocks:
            x = transformer_block(x)
        
        # å…¨å±€æ± åŒ–
        avg_pool = torch.mean(x, dim=1)  # (batch_size, d_model)
        max_pool = torch.max(x, dim=1)[0]  # (batch_size, d_model)
        
        # åˆå¹¶ä¸åŒçš„æ± åŒ–ç»“æœ
        pooled = torch.cat([avg_pool, max_pool], dim=1)  # (batch_size, d_model*2)
        
        # åˆ†ç±»å¤´
        output = self.classifier(pooled)
        
        return output
    
    def fit(self, X, y, group, **kwargs):
        """è®­ç»ƒæ¨¡å‹"""
        epochs = kwargs.get('epochs', self.params['epochs'])
        batch_size = kwargs.get('batch_size', self.params['batch_size'])
        
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒTransformerRanker")
        print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {X.shape}")
        print(f"ğŸ“Š è®­ç»ƒå‚æ•°: epochs={epochs}, batch_size={batch_size}")
        
        try:
            # æ•°æ®é¢„å¤„ç†
            X_processed = X.astype(np.float32)
            y_processed = y.astype(np.float32)
            
            # æ£€æŸ¥å’Œæ¸…ç†å¼‚å¸¸å€¼
            if np.any(np.isnan(X_processed)) or np.any(np.isinf(X_processed)):
                print("âš ï¸ å‘ç°NaNæˆ–Infå€¼ï¼Œè¿›è¡Œæ¸…ç†...")
                X_processed = np.nan_to_num(X_processed, nan=0.0, posinf=1.0, neginf=-1.0)
            
            if np.any(np.isnan(y_processed)) or np.any(np.isinf(y_processed)):
                print("âš ï¸ æ ‡ç­¾ä¸­å‘ç°NaNæˆ–Infå€¼ï¼Œè¿›è¡Œæ¸…ç†...")
                y_processed = np.nan_to_num(y_processed, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # æ•°æ®æ ‡å‡†åŒ–
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_processed = scaler.fit_transform(X_processed).astype(np.float32)
            self.scaler = scaler  # ä¿å­˜scalerç”¨äºé¢„æµ‹
            
            print("âœ“ æ•°æ®é¢„å¤„ç†å®Œæˆ")
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            X_tensor = torch.FloatTensor(X_processed).to(DEVICE)
            y_tensor = torch.FloatTensor(y_processed).reshape(-1, 1).to(DEVICE)
            
            # æ™ºèƒ½ç¡®å®šéªŒè¯é›†å¤§å°
            total_samples = len(X_processed)
            if total_samples > 100000:
                val_ratio = 0.02
            elif total_samples > 10000:
                val_ratio = 0.05
            else:
                val_ratio = 0.2
            
            val_size = int(total_samples * val_ratio)
            train_size = total_samples - val_size
            
            print(f"ğŸ“Š æ•°æ®åˆ†å‰²: è®­ç»ƒ={train_size}, éªŒè¯={val_size} ({val_ratio*100:.1f}%)")
            
            # åˆ†å‰²æ•°æ®
            if val_size > 100:
                train_dataset = TensorDataset(X_tensor[:train_size], y_tensor[:train_size])
                val_dataset = TensorDataset(X_tensor[train_size:], y_tensor[train_size:])
                
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
                
                print("ğŸ”„ å¼€å§‹è®­ç»ƒï¼ˆå¸¦éªŒè¯é›†ï¼‰...")
                for epoch in range(epochs):
                    # è®­ç»ƒé˜¶æ®µ
                    self.train()
                    train_loss = 0.0
                    for batch_X, batch_y in train_loader:
                        self.optimizer.zero_grad()
                        outputs = self(batch_X)
                        loss = self.criterion(outputs, batch_y)
                        loss.backward()
                        # æ¢¯åº¦è£å‰ª
                        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
                        self.optimizer.step()
                        train_loss += loss.item()
                    
                    # éªŒè¯é˜¶æ®µ
                    if epoch % 2 == 0:
                        self.eval()
                        val_loss = 0.0
                        with torch.no_grad():
                            for batch_X, batch_y in val_loader:
                                outputs = self(batch_X)
                                val_loss += self.criterion(outputs, batch_y).item()
                        
                        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')
            else:
                train_dataset = TensorDataset(X_tensor, y_tensor)
                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
                
                print("ğŸ”„ å¼€å§‹è®­ç»ƒï¼ˆæ— éªŒè¯é›†ï¼‰...")
                for epoch in range(epochs):
                    self.train()
                    total_loss = 0.0
                    for batch_X, batch_y in train_loader:
                        self.optimizer.zero_grad()
                        outputs = self(batch_X)
                        loss = self.criterion(outputs, batch_y)
                        loss.backward()
                        # æ¢¯åº¦è£å‰ª
                        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
                        self.optimizer.step()
                        total_loss += loss.item()
                    
                    if epoch % 2 == 0:
                        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')
            
            print("âœ… TransformerRankerè®­ç»ƒå®Œæˆ")
            
            # è®¡ç®—ç‰¹å¾é‡è¦æ€§
            try:
                print("ğŸ” è®¡ç®—ç‰¹å¾é‡è¦æ€§...")
                sample_size = min(1000, len(X))
                self._compute_feature_importance(X[:sample_size])
                print("âœ… ç‰¹å¾é‡è¦æ€§è®¡ç®—å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ ç‰¹å¾é‡è¦æ€§è®¡ç®—å¤±è´¥: {e}")
                self._feature_importance = np.ones(self.input_dim) / self.input_dim
                
        except Exception as e:
            print(f"âŒ TransformerRankerè®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _compute_feature_importance(self, X_sample):
        """è®¡ç®—ç‰¹å¾é‡è¦æ€§"""
        try:
            self.eval()
            
            # æ•°æ®é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            if hasattr(self, 'scaler'):
                X_processed = self.scaler.transform(X_sample.astype(np.float32))
            else:
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                X_processed = scaler.fit_transform(X_sample.astype(np.float32))
            
            X_tensor = torch.FloatTensor(X_processed).to(DEVICE)
            X_tensor.requires_grad_(True)
            
            outputs = self(X_tensor)
            loss = outputs.mean()
            loss.backward()
            
            grads = X_tensor.grad
            if grads is not None:
                importance = torch.mean(torch.abs(grads), dim=0).cpu().numpy()
                importance = importance / (np.sum(importance) + 1e-8)
                self._feature_importance = importance
                print("âœ“ ä½¿ç”¨æ¢¯åº¦è®¡ç®—ç‰¹å¾é‡è¦æ€§")
            else:
                raise ValueError("æ— æ³•è®¡ç®—æ¢¯åº¦")
                
        except Exception as e:
            print(f"âš ï¸ æ¢¯åº¦æ–¹æ³•å¤±è´¥: {e}ï¼Œä½¿ç”¨æ–¹å·®æ–¹æ³•")
            try:
                feature_variance = np.var(X_sample, axis=0)
                importance = feature_variance / (np.sum(feature_variance) + 1e-8)
                self._feature_importance = importance
                print("âœ“ ä½¿ç”¨æ–¹å·®è®¡ç®—ç‰¹å¾é‡è¦æ€§")
            except:
                self._feature_importance = np.ones(self.input_dim) / self.input_dim
                print("âœ“ ä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºç‰¹å¾é‡è¦æ€§")
    
    def predict(self, X):
        """é¢„æµ‹åˆ†æ•°"""
        try:
            print(f"ğŸ”® TransformerRankeré¢„æµ‹ï¼Œæ•°æ®å½¢çŠ¶: {X.shape}")
            
            # æ•°æ®é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            X_processed = X.astype(np.float32)
            
            # æ¸…ç†å¼‚å¸¸å€¼
            if np.any(np.isnan(X_processed)) or np.any(np.isinf(X_processed)):
                X_processed = np.nan_to_num(X_processed, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # æ ‡å‡†åŒ–
            if hasattr(self, 'scaler'):
                X_processed = self.scaler.transform(X_processed)
            else:
                from sklearn.preprocessing import StandardScaler
                scaler = StandardScaler()
                X_processed = scaler.fit_transform(X_processed)
            
            X_processed = X_processed.astype(np.float32)
            
            # æ‰¹é‡é¢„æµ‹ä»¥é¿å…å†…å­˜é—®é¢˜
            self.eval()
            batch_size = 1000
            predictions = []
            
            with torch.no_grad():
                for i in range(0, len(X_processed), batch_size):
                    batch = X_processed[i:i+batch_size]
                    batch_tensor = torch.FloatTensor(batch).to(DEVICE)
                    batch_pred = self(batch_tensor)
                    predictions.append(batch_pred.cpu().numpy().flatten())
            
            result = np.concatenate(predictions)
            print(f"âœ… TransformerRankeré¢„æµ‹å®Œæˆï¼Œç»“æœå½¢çŠ¶: {result.shape}")
            return result
            
        except Exception as e:
            print(f"âŒ TransformerRankeré¢„æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # è¿”å›éšæœºå€¼ä½œä¸ºåå¤‡
            return np.random.random(len(X)).astype(np.float32)
    
    def get_model_name(self):
        return "TransformerRanker"
    
    @property
    def feature_importances_(self):
        if self._feature_importance is None:
            return np.ones(self.input_dim) / self.input_dim
        return self._feature_importance


class BM25Ranker(BaseRanker):
    """BM25æ’åºæ¨¡å‹"""
    
    def __init__(self, **params):
        self.params = params
        self.model = None
        self.tokenized_corpus = None
        self.feature_names = None
        
    def fit(self, X, y, group, **kwargs):
        # å°†ç‰¹å¾è½¬æ¢ä¸º"æ–‡æ¡£"å½¢å¼
        self.feature_names = [f"feature_{i}" for i in range(X.shape[1])]
        self.tokenized_corpus = []
        
        for i in range(X.shape[0]):
            # å°†ç‰¹å¾å€¼å¤§äºé˜ˆå€¼çš„ç‰¹å¾åä½œä¸º"è¯"
            threshold = kwargs.get('threshold', 0.5)
            doc = [self.feature_names[j] for j in np.where(X[i] > threshold)[0]]
            if not doc:  # å¦‚æœæ²¡æœ‰ç‰¹å¾è¶…è¿‡é˜ˆå€¼ï¼Œä½¿ç”¨æ‰€æœ‰éé›¶ç‰¹å¾
                doc = [self.feature_names[j] for j in np.where(X[i] != 0)[0]]
            self.tokenized_corpus.append(doc)
        
        self.model = BM25Okapi(self.tokenized_corpus)
    
    def predict(self, X):
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")
        
        scores = []
        for i in range(X.shape[0]):
            query = [self.feature_names[j] for j in np.where(X[i] > 0.5)[0]]
            if not query:
                query = [self.feature_names[j] for j in np.where(X[i] != 0)[0]]
            
            if query:
                doc_scores = self.model.get_scores(query)
                scores.append(doc_scores[i] if i < len(doc_scores) else 0.0)
            else:
                scores.append(0.0)
        
        return np.array(scores)
    
    def get_model_name(self):
        return "BM25Ranker"


class ModelFactory:
    """æ¨¡å‹å·¥å‚ç±»"""
    
    @staticmethod
    def create_model(model_name: str, use_gpu: bool = True, input_dim: Optional[int] = None, **params) -> BaseRanker:
        """
        åˆ›å»ºæŒ‡å®šçš„æ¨¡å‹å®ä¾‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            use_gpu: æ˜¯å¦ä½¿ç”¨GPU
            input_dim: è¾“å…¥ç»´åº¦ï¼ˆç¥ç»ç½‘ç»œæ¨¡å‹éœ€è¦ï¼‰
            **params: æ¨¡å‹å‚æ•°
            
        Returns:
            BaseRanker: æ¨¡å‹å®ä¾‹
        """
        if model_name == 'XGBRanker':
            return XGBRankerModel(use_gpu=use_gpu, **params)
        elif model_name == 'LGBMRanker':
            return LGBMRankerModel(use_gpu=use_gpu, **params)
        elif model_name == 'LambdaMART':
            return LambdaMART(use_gpu=use_gpu, **params)
        elif model_name == 'ListNet':
            return ListNetModel(use_gpu=use_gpu, **params)
        elif model_name == 'NeuralRanker':
            if input_dim is None:
                raise ValueError("NeuralRankeréœ€è¦æŒ‡å®šinput_dimå‚æ•°")
            return NeuralRanker(input_dim=input_dim, **params)
        elif model_name == 'RankNet':
            if input_dim is None:
                raise ValueError("RankNetéœ€è¦æŒ‡å®šinput_dimå‚æ•°")
            return RankNet(input_dim=input_dim, **params)
        elif model_name == 'TransformerRanker':
            if input_dim is None:
                raise ValueError("TransformerRankeréœ€è¦æŒ‡å®šinput_dimå‚æ•°")
            return TransformerRanker(input_dim=input_dim, **params)
        elif model_name == 'BM25Ranker':
            return BM25Ranker(**params)
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_name}")
    
    @staticmethod
    def get_available_models() -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åç§°"""
        return [
            'XGBRanker', 'LGBMRanker', 'LambdaMART', 'ListNet', 
            'NeuralRanker', 'RankNet', 'TransformerRanker', 'BM25Ranker'
        ]
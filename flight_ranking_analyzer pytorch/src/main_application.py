"""
ä¸»ç¨‹åºæ¨¡å— - é‡æ„ç‰ˆ v5.1
ç»Ÿä¸€çš„ç¨‹åºå…¥å£å’Œå·¥ä½œæµç®¡ç†ï¼Œæä¾›å®Œå–„çš„æµæ°´çº¿é€‰æ‹©

ä½œè€…: Flight Ranking Team
ç‰ˆæœ¬: 5.1 (æ”¹è¿›ç‰ˆ)
"""

import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd
import numpy as np
import warnings

warnings.filterwarnings('ignore')

# å¯¼å…¥é‡æ„åçš„æ¨¡å—
from core_config import ConfigManager, FeatureLevel, DataProcessMode, FeatureSelectionMode
from data_processor import DataProcessor
from models_module import ModelFactory
from training_prediction import ModelManager, ModelTrainer, ModelPredictor
from evaluation_metrics import ModelEvaluator
from visualization import ResultsVisualizer
from utils import ProgressTracker, SystemChecker, FileManager

# å…¨å±€é…ç½®
config = ConfigManager()


class EnhancedUserInterface:
    """å¢å¼ºçš„ç”¨æˆ·äº¤äº’ç•Œé¢"""
    
    @staticmethod
    def show_welcome():
        """æ˜¾ç¤ºæ¬¢è¿ç•Œé¢"""
        print(" èˆªç­æ’åºåˆ†æç³»ç»Ÿ v5.1 (æ”¹è¿›ç‰ˆ)")
        print("æ–°å¢åŠŸèƒ½:")
        print("  â€¢ çµæ´»çš„æ•°æ®å¤„ç†æµæ°´çº¿é€‰æ‹©")
        print("  â€¢  æ™ºèƒ½ç¼“å­˜ç®¡ç†")
        print("  â€¢  å¤šç§ç‰¹å¾å·¥ç¨‹æ¨¡å¼æ¯”è¾ƒ")
        print("  â€¢ âš¡ å¿«é€Ÿé‡ç”¨å·²å¤„ç†æ•°æ®")
    
    @staticmethod
    def get_run_mode() -> str:
        """è·å–è¿è¡Œæ¨¡å¼"""
        print("\nğŸ¯ è¿è¡Œæ¨¡å¼é€‰æ‹©:")
        print("1. ğŸ’» å®Œæ•´æµç¨‹ (æ•°æ®å¤„ç† + è®­ç»ƒ + é¢„æµ‹)")
        print("2. ğŸ”§ ä»…æ•°æ®å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹")
        print("3. ğŸ¯ ä»…æ¨¡å‹è®­ç»ƒ")
        print("4. ğŸ“ˆ ä»…é¢„æµ‹ (ä½¿ç”¨å·²ä¿å­˜çš„æ¨¡å‹)")
        print("5. ğŸ“Š æ¨¡å‹æ¯”è¾ƒåˆ†æ")
        print("6. ğŸ”„ ç¼“å­˜ç®¡ç†")
        
        while True:
            choice = input("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ (1-6): ").strip()
            if choice in ['1', '2', '3', '4', '5', '6']:
                return choice
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆé€‰æ‹© (1-6)")
    
    @staticmethod
    def get_data_processing_config() -> Dict[str, Any]:
        """è·å–æ•°æ®å¤„ç†é…ç½®"""
        print("\n" + "="*60)
        print("ğŸ”„ æ•°æ®å¤„ç†æµæ°´çº¿é…ç½®")
        print("="*60)
        
        config_dict = {}
        
        # æ£€æŸ¥ç¼“å­˜æƒ…å†µ
        cache_info = config.get_cache_info()
        has_cache = cache_info['cache_count'] > 0 if 'cache_count' in cache_info else False
        
        if has_cache:
            print(f"\nğŸ’¾ å‘ç°ç¼“å­˜æ•°æ®: {cache_info['cache_count']} ä¸ªæ–‡ä»¶")
            print("ç¼“å­˜æ–‡ä»¶:", ", ".join(cache_info['cached_files'][:3]))
            if len(cache_info['cached_files']) > 3:
                print(f"... è¿˜æœ‰ {len(cache_info['cached_files']) - 3} ä¸ªæ–‡ä»¶")
        
        # æ•°æ®å¤„ç†æ¨¡å¼é€‰æ‹©
        print("\nğŸ”„ æ•°æ®å¤„ç†æ¨¡å¼:")
        modes = [
            ("å®Œæ•´å¤„ç†", "full_process", "ç¼–ç  â†’ ç‰¹å¾å·¥ç¨‹ â†’ ç‰¹å¾é€‰æ‹©"),
            ("ä»…æ•°æ®ç¼–ç ", "encoding_only", "ä»…å¯¹åŸå§‹æ•°æ®è¿›è¡Œç¼–ç å¤„ç†"),
            ("ä»…ç‰¹å¾å·¥ç¨‹", "feature_only", "å‡è®¾æ•°æ®å·²ç¼–ç ï¼Œä»…åšç‰¹å¾å·¥ç¨‹"),
            ("ä½¿ç”¨åŸå§‹æ•°æ®", "raw_data", "ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®è®­ç»ƒ"),
        ]
        
        if has_cache:
            modes.insert(1, ("åŠ è½½ç¼“å­˜æ•°æ®", "load_cached", "ä½¿ç”¨å·²ä¿å­˜çš„å¤„ç†ç»“æœ"))
            modes.append(("æ¯”è¾ƒå¤„ç†æ¨¡å¼", "compare_modes", "æ¯”è¾ƒä¸åŒå¤„ç†æ–¹å¼çš„æ•ˆæœ"))
        
        for i, (name, _, desc) in enumerate(modes, 1):
            print(f"{i}. {name} - {desc}")
        
        while True:
            try:
                choice = int(input(f"\nè¯·é€‰æ‹©æ•°æ®å¤„ç†æ¨¡å¼ (1-{len(modes)}, æ¨è1): ") or "1")
                if 1 <= choice <= len(modes):
                    config_dict['data_process_mode'] = modes[choice-1][1]
                    break
                else:
                    print(f"âŒ è¯·è¾“å…¥ 1-{len(modes)} ä¹‹é—´çš„æ•°å­—")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
        
        # æ ¹æ®æ¨¡å¼è·å–è¯¦ç»†é…ç½®
        if config_dict['data_process_mode'] == 'load_cached':
            return config_dict  # åŠ è½½ç¼“å­˜æ¨¡å¼ä¸éœ€è¦å…¶ä»–é…ç½®
        
        elif config_dict['data_process_mode'] == 'raw_data':
            config_dict['feature_level'] = 'none'
            config_dict['selection_mode'] = 'none'
            return config_dict
        
        elif config_dict['data_process_mode'] == 'encoding_only':
            config_dict['feature_level'] = 'none'
            config_dict.update(EnhancedUserInterface._get_feature_selection_config())
            return config_dict
        
        elif config_dict['data_process_mode'] == 'compare_modes':
            return EnhancedUserInterface._get_comparison_config()
        
        # å®Œæ•´å¤„ç†æˆ–ä»…ç‰¹å¾å·¥ç¨‹æ¨¡å¼
        config_dict.update(EnhancedUserInterface._get_feature_engineering_config())
        config_dict.update(EnhancedUserInterface._get_feature_selection_config())
        config_dict.update(EnhancedUserInterface._get_cache_config())
        
        return config_dict
    
    @staticmethod
    def _get_feature_engineering_config() -> Dict[str, Any]:
        """è·å–ç‰¹å¾å·¥ç¨‹é…ç½®"""
        print("\nğŸ› ï¸ ç‰¹å¾å·¥ç¨‹é…ç½®:")
        
        levels = [
            ("è·³è¿‡ç‰¹å¾å·¥ç¨‹", "none", "ç›´æ¥ä½¿ç”¨ç¼–ç åçš„åŸå§‹ç‰¹å¾"),
            ("åŸºç¡€ç‰¹å¾å·¥ç¨‹", "basic", "ä»·æ ¼ã€æ—¶é—´ã€æŒç»­æ—¶é—´ç­‰æ ¸å¿ƒç‰¹å¾"),
            ("å¢å¼ºç‰¹å¾å·¥ç¨‹", "enhanced", "åŸºç¡€ + é¢„è®¢æ—¶æœºã€èˆ±ä½ç­‰çº§ã€ç”¨æˆ·ç±»å‹ç‰¹å¾"),
            ("é«˜çº§ç‰¹å¾å·¥ç¨‹", "advanced", "å¢å¼º + ç»æµå­¦ç‰¹å¾ã€é€‰æ‹©å¤æ‚åº¦ã€ç»„åˆç‰¹å¾"),
        ]
        
        for i, (name, _, desc) in enumerate(levels, 1):
            emoji = "â­" if i == 3 else ""  # æ¨èå¢å¼ºçº§åˆ«
            print(f"{i}. {name} {emoji}")
            print(f"   â””â”€ {desc}")
        
        while True:
            try:
                choice = int(input(f"\nè¯·é€‰æ‹©ç‰¹å¾å·¥ç¨‹çº§åˆ« (1-{len(levels)}, æ¨è3): ") or "3")
                if 1 <= choice <= len(levels):
                    return {'feature_level': levels[choice-1][1]}
                else:
                    print(f"âŒ è¯·è¾“å…¥ 1-{len(levels)} ä¹‹é—´çš„æ•°å­—")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
    
    @staticmethod
    def _get_feature_selection_config() -> Dict[str, Any]:
        """è·å–ç‰¹å¾é€‰æ‹©é…ç½®"""
        print("\nğŸ¯ ç‰¹å¾é€‰æ‹©é…ç½®:")
        
        selections = [
            ("è·³è¿‡ç‰¹å¾é€‰æ‹©", "none", "ä½¿ç”¨æ‰€æœ‰ç”Ÿæˆçš„ç‰¹å¾"),
            ("æ–¹å·®é€‰æ‹©", "variance", "åŸºäºç‰¹å¾æ–¹å·®è¿›è¡Œé€‰æ‹© (æ¨è)"),
            ("ç›¸å…³æ€§é€‰æ‹©", "correlation", "ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾"),
            ("äº’ä¿¡æ¯é€‰æ‹©", "mutual_info", "åŸºäºäº’ä¿¡æ¯è¿›è¡Œé€‰æ‹©"),
        ]
        
        for i, (name, _, desc) in enumerate(selections, 1):
            emoji = "â­" if i == 2 else ""  # æ¨èæ–¹å·®é€‰æ‹©
            print(f"{i}. {name} {emoji}")
            print(f"   â””â”€ {desc}")
        
        while True:
            try:
                choice = int(input(f"\nè¯·é€‰æ‹©ç‰¹å¾é€‰æ‹©æ–¹æ³• (1-{len(selections)}, æ¨è2): ") or "2")
                if 1 <= choice <= len(selections):
                    selection_mode = selections[choice-1][1]
                    config_dict = {'selection_mode': selection_mode}
                    
                    # å¦‚æœé€‰æ‹©äº†ç‰¹å¾é€‰æ‹©ï¼Œè¯¢é—®ç‰¹å¾æ•°é‡
                    if selection_mode != 'none':
                        try:
                            max_features = int(input("æœ€å¤§ç‰¹å¾æ•° (é»˜è®¤200): ") or "200")
                            config_dict['max_features'] = max_features
                        except ValueError:
                            config_dict['max_features'] = 200
                    
                    return config_dict
                else:
                    print(f"âŒ è¯·è¾“å…¥ 1-{len(selections)} ä¹‹é—´çš„æ•°å­—")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
    
    @staticmethod
    def _get_cache_config() -> Dict[str, Any]:
        """è·å–ç¼“å­˜é…ç½®"""
        print("\nğŸ’¾ ç¼“å­˜é…ç½®:")
        cache_data = input("ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°ç¼“å­˜? (y/n, é»˜è®¤y): ").strip().lower() != 'n'
        auto_load = input("ä¸‹æ¬¡è‡ªåŠ¨åŠ è½½ç¼“å­˜? (y/n, é»˜è®¤y): ").strip().lower() != 'n'
        
        return {
            'cache_processed_data': cache_data,
            'auto_load_cache': auto_load
        }
    
    @staticmethod
    def _get_comparison_config() -> Dict[str, Any]:
        """è·å–æ¯”è¾ƒæ¨¡å¼é…ç½®"""
        print("\nğŸ“Š æ¯”è¾ƒæ¨¡å¼é…ç½®:")
        print("å°†æ¯”è¾ƒä»¥ä¸‹å¤„ç†æ–¹å¼çš„æ•ˆæœ:")
        print("  â€¢ åŸå§‹æ•°æ®")
        print("  â€¢ ä»…ç¼–ç ")
        print("  â€¢ åŸºç¡€ç‰¹å¾å·¥ç¨‹")
        print("  â€¢ å¢å¼ºç‰¹å¾å·¥ç¨‹")
        print("  â€¢ é«˜çº§ç‰¹å¾å·¥ç¨‹")
        
        return {
            'data_process_mode': 'compare_modes',
            'comparison_modes': ['raw_data', 'encoding_only', 'basic', 'enhanced', 'advanced']
        }
    
    @staticmethod
    def get_training_config() -> Dict[str, Any]:
        """è·å–è®­ç»ƒé…ç½®"""
        print("\n" + "="*60)
        print("ğŸ¯ æ¨¡å‹è®­ç»ƒé…ç½®")
        print("="*60)
        
        config_dict = {}
        
        # æ•°æ®æ¨¡å¼
        print("\nğŸ“Š æ•°æ®åŠ è½½æ¨¡å¼:")
        print("1. ğŸ”¬ æŠ½æ ·æ¨¡å¼ (æ¨èï¼Œå¿«é€ŸéªŒè¯)")
        print("2. ğŸ¢ å…¨é‡æ¨¡å¼ (å®Œæ•´æ•°æ®ï¼Œè€—æ—¶è¾ƒé•¿)")
        
        data_mode = input("è¯·é€‰æ‹© (1-2, é»˜è®¤1): ").strip() or '1'
        config_dict['use_sampling'] = data_mode == '1'
        
        if config_dict['use_sampling']:
            try:
                config_dict['num_groups'] = int(input(f"æŠ½æ ·ç»„æ•° (é»˜è®¤{config.training.num_groups}): ") 
                                               or config.training.num_groups)
                config_dict['min_group_size'] = int(input(f"æœ€å°ç»„å¤§å° (é»˜è®¤{config.training.min_group_size}): ") 
                                                   or config.training.min_group_size)
            except ValueError:
                config_dict['num_groups'] = config.training.num_groups
                config_dict['min_group_size'] = config.training.min_group_size
        
        # æ¨¡å‹é€‰æ‹©
        available_models = ModelFactory.get_available_models()
        print(f"\nğŸ¤– å¯ç”¨æ¨¡å‹:")
        for i, model in enumerate(available_models, 1):
            model_type = "ğŸ”¥ PyTorch" if ModelFactory.is_pytorch_model(model) else "ğŸ“Š ä¼ ç»Ÿ"
            print(f"{i}. {model} ({model_type})")
        print(f"{len(available_models) + 1}. æ‰€æœ‰æ¨¡å‹")
        print(f"{len(available_models) + 2}. å¿«é€Ÿæ¨¡å¼ (ä»…XGBRanker + NeuralRanker)")
        
        model_input = input("\né€‰æ‹©æ¨¡å‹ (ç”¨é€—å·åˆ†éš”, é»˜è®¤å¿«é€Ÿæ¨¡å¼): ").strip()
        if not model_input:
            config_dict['selected_models'] = ['XGBRanker', 'NeuralRanker']
        else:
            try:
                choices = [int(x.strip()) for x in model_input.split(',')]
                if len(available_models) + 1 in choices:
                    config_dict['selected_models'] = available_models
                elif len(available_models) + 2 in choices:
                    config_dict['selected_models'] = ['XGBRanker', 'NeuralRanker']
                else:
                    config_dict['selected_models'] = [
                        available_models[i-1] 
                        for i in choices 
                        if 1 <= i <= len(available_models)
                    ]
            except:
                config_dict['selected_models'] = ['XGBRanker', 'NeuralRanker']
        
        # è‡ªåŠ¨è°ƒå‚
        config_dict['enable_auto_tuning'] = input("\nğŸ”§ å¯ç”¨è‡ªåŠ¨è°ƒå‚? (y/n, é»˜è®¤n): ").strip().lower() == 'y'
        if config_dict['enable_auto_tuning']:
            try:
                config_dict['auto_tuning_trials'] = int(input("è°ƒå‚è¯•éªŒæ¬¡æ•° (é»˜è®¤20): ") or 20)
            except:
                config_dict['auto_tuning_trials'] = 20
        
        # æ¨¡å‹ä¿å­˜
        config_dict['save_models'] = input("ğŸ’¾ ä¿å­˜è®­ç»ƒçš„æ¨¡å‹? (y/n, é»˜è®¤y): ").strip().lower() != 'n'
        
        return config_dict
    
    @staticmethod
    def get_prediction_config(model_manager: ModelManager) -> Dict[str, Any]:
        """è·å–é¢„æµ‹é…ç½®"""
        print("\n" + "="*60)
        print("ğŸ“ˆ é¢„æµ‹é…ç½®")
        print("="*60)
        
        config_dict = {}
        
        # æ£€æŸ¥å¯ç”¨æ¨¡å‹
        available_models = model_manager.get_available_models()
        
        if not available_models:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            config_dict['prediction_possible'] = False
            return config_dict
        
        print("\nğŸ¤– å¯ç”¨çš„å·²ä¿å­˜æ¨¡å‹:")
        for model_name, segments in available_models.items():
            model_type = "ğŸ”¥ PyTorch" if ModelFactory.is_pytorch_model(model_name) else "ğŸ“Š ä¼ ç»Ÿ"
            print(f"  {model_name} ({model_type}): æ®µ {segments}")
        
        # é€‰æ‹©æ¨¡å‹
        model_names = list(available_models.keys())
        print(f"\næ¨¡å‹é€‰æ‹©:")
        for i, model_name in enumerate(model_names, 1):
            print(f"{i}. {model_name}")
        print(f"{len(model_names) + 1}. æ‰€æœ‰æ¨¡å‹")
        print(f"{len(model_names) + 2}. æœ€ä½³ç»„åˆ (æ¨è)")
        
        model_input = input("é€‰æ‹©æ¨¡å‹ (ç”¨é€—å·åˆ†éš”, é»˜è®¤æœ€ä½³ç»„åˆ): ").strip()
        if not model_input or model_input == str(len(model_names) + 2):
            # é€‰æ‹©æœ€ä½³ç»„åˆï¼šä¼ ç»Ÿæ¨¡å‹ + PyTorchæ¨¡å‹å„ä¸€ä¸ª
            traditional = [m for m in model_names if not ModelFactory.is_pytorch_model(m)]
            pytorch = [m for m in model_names if ModelFactory.is_pytorch_model(m)]
            config_dict['prediction_models'] = (traditional[:1] + pytorch[:1]) or model_names[:2]
        elif model_input == str(len(model_names) + 1):
            config_dict['prediction_models'] = model_names
        else:
            try:
                choices = [int(x.strip()) for x in model_input.split(',')]
                config_dict['prediction_models'] = [
                    model_names[i-1] for i in choices if 1 <= i <= len(model_names)
                ]
            except:
                config_dict['prediction_models'] = model_names[:2]
        
        # é€‰æ‹©æ•°æ®æ®µ
        all_segments = set()
        for segments in available_models.values():
            all_segments.update(segments)
        all_segments = sorted(list(all_segments))
        
        print(f"\nğŸ“Š å¯é¢„æµ‹æ•°æ®æ®µ: {all_segments}")
        segment_input = input("é€‰æ‹©æ•°æ®æ®µ (ç”¨é€—å·åˆ†éš”ï¼Œé»˜è®¤å…¨éƒ¨): ").strip()
        
        if segment_input:
            try:
                config_dict['prediction_segments'] = [int(s.strip()) for s in segment_input.split(',')]
                config_dict['prediction_segments'] = [s for s in config_dict['prediction_segments'] if s in all_segments]
            except:
                config_dict['prediction_segments'] = all_segments
        else:
            config_dict['prediction_segments'] = all_segments
        
        # é›†æˆæ–¹æ³•
        print("\nğŸ”— é›†æˆæ–¹æ³•:")
        print("1. ç®€å•å¹³å‡")
        print("2. åŠ æƒå¹³å‡")
        print("3. æŠ•ç¥¨æœºåˆ¶")
        
        ensemble_choice = input("é€‰æ‹©é›†æˆæ–¹æ³• (1-3, é»˜è®¤1): ").strip() or '1'
        ensemble_map = {'1': 'average', '2': 'weighted_average', '3': 'voting'}
        config_dict['ensemble_method'] = ensemble_map.get(ensemble_choice, 'average')
        
        config_dict['prediction_possible'] = True
        return config_dict
    
    @staticmethod
    def show_cache_management() -> str:
        """æ˜¾ç¤ºç¼“å­˜ç®¡ç†ç•Œé¢"""
        print("\n" + "="*60)
        print("ğŸ’¾ ç¼“å­˜ç®¡ç†")
        print("="*60)
        
        cache_info = config.get_cache_info()
        
        if cache_info['cache_count'] > 0:
            print(f"\nå½“å‰ç¼“å­˜çŠ¶æ€:")
            print(f"  ç¼“å­˜ç›®å½•: {cache_info['cache_dir']}")
            print(f"  ç¼“å­˜æ–‡ä»¶æ•°: {cache_info['cache_count']}")
            print(f"  ç¼“å­˜æ–‡ä»¶:")
            for file_name in cache_info['cached_files']:
                print(f"    â€¢ {file_name}")
        else:
            print("\nğŸ“­ æš‚æ— ç¼“å­˜æ•°æ®")
        
        print(f"\nç¼“å­˜æ“ä½œ:")
        print("1. ğŸ“‹ æŸ¥çœ‹ç¼“å­˜è¯¦æƒ…")
        print("2. ğŸ—‘ï¸ æ¸…ç†æ‰€æœ‰ç¼“å­˜")
        print("3. ğŸ”„ é‡æ–°ç”Ÿæˆç¼“å­˜")
        print("4. â†©ï¸ è¿”å›ä¸»èœå•")
        
        return input("è¯·é€‰æ‹©æ“ä½œ (1-4): ").strip()
    
    @staticmethod
    def show_config_summary(data_config: Dict[str, Any], training_config: Dict[str, Any] = None):
        """æ˜¾ç¤ºé…ç½®æ€»ç»“"""
        print(f"\n{'='*60}")
        print("ğŸ“‹ é…ç½®æ€»ç»“")
        print('='*60)
        
        # ç³»ç»Ÿä¿¡æ¯
        device_info = config.get_device_info()
        print(f"ğŸ’» è®¾å¤‡: {device_info['device']}")
        print(f"ğŸ PyTorchç‰ˆæœ¬: {device_info['pytorch_version']}")
        
        # æ•°æ®å¤„ç†é…ç½®
        print(f"\nğŸ”„ æ•°æ®å¤„ç†:")
        mode_desc = {
            'full_process': 'å®Œæ•´å¤„ç† (ç¼–ç â†’ç‰¹å¾å·¥ç¨‹â†’é€‰æ‹©)',
            'encoding_only': 'ä»…æ•°æ®ç¼–ç ',
            'feature_only': 'ä»…ç‰¹å¾å·¥ç¨‹', 
            'load_cached': 'åŠ è½½ç¼“å­˜æ•°æ®',
            'raw_data': 'ä½¿ç”¨åŸå§‹æ•°æ®',
            'compare_modes': 'æ¯”è¾ƒå¤šç§å¤„ç†æ¨¡å¼'
        }
        print(f"  æ¨¡å¼: {mode_desc.get(data_config.get('data_process_mode'), 'æœªçŸ¥')}")
        
        if 'feature_level' in data_config:
            level_desc = {
                'none': 'è·³è¿‡',
                'basic': 'åŸºç¡€',
                'enhanced': 'å¢å¼º',
                'advanced': 'é«˜çº§'
            }
            print(f"  ç‰¹å¾å·¥ç¨‹: {level_desc.get(data_config['feature_level'], 'æœªçŸ¥')}")
        
        if 'selection_mode' in data_config:
            selection_desc = {
                'none': 'è·³è¿‡',
                'variance': 'æ–¹å·®é€‰æ‹©',
                'correlation': 'ç›¸å…³æ€§é€‰æ‹©',
                'mutual_info': 'äº’ä¿¡æ¯é€‰æ‹©'
            }
            print(f"  ç‰¹å¾é€‰æ‹©: {selection_desc.get(data_config['selection_mode'], 'æœªçŸ¥')}")
            
            if data_config.get('max_features'):
                print(f"  æœ€å¤§ç‰¹å¾æ•°: {data_config['max_features']}")
        
        # è®­ç»ƒé…ç½®
        if training_config:
            print(f"\nğŸ¯ æ¨¡å‹è®­ç»ƒ:")
            print(f"  æ•°æ®æ¨¡å¼: {'æŠ½æ ·' if training_config.get('use_sampling', True) else 'å…¨é‡'}")
            if training_config.get('selected_models'):
                models_str = ', '.join(training_config['selected_models'])
                print(f"  é€‰æ‹©æ¨¡å‹: {models_str}")
            print(f"  è‡ªåŠ¨è°ƒå‚: {'å¯ç”¨' if training_config.get('enable_auto_tuning', False) else 'å…³é—­'}")


class EnhancedWorkflowManager:
    """å¢å¼ºçš„å·¥ä½œæµç®¡ç†å™¨"""
    
    def __init__(self):
        self.ui = EnhancedUserInterface()
        self.system_checker = SystemChecker()
        self.file_manager = FileManager(config.paths)
        self.progress_tracker = ProgressTracker()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model_manager = ModelManager(config.paths.models)
        self.visualizer = ResultsVisualizer(config.paths.output)
        
        # ç¡®ä¿è·¯å¾„å­˜åœ¨
        config.ensure_paths()
    
    def run(self):
        """è¿è¡Œä¸»å·¥ä½œæµ"""
        try:
            # æ˜¾ç¤ºæ¬¢è¿ç•Œé¢
            self.ui.show_welcome()
            
            # ç³»ç»Ÿæ£€æŸ¥
            print("ğŸ” ç³»ç»Ÿæ£€æŸ¥ä¸­...")
            system_info = self.system_checker.check_system()
            self._display_system_info(system_info)
            
            # æ–‡ä»¶æ£€æŸ¥
            train_files, test_files = self.file_manager.find_data_files()
            
            # è·å–è¿è¡Œæ¨¡å¼
            run_mode = self.ui.get_run_mode()
            
            # æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒå·¥ä½œæµ
            if run_mode == '1':
                self._run_full_workflow(train_files, test_files)
            elif run_mode == '2':
                self._run_data_processing_only(train_files)
            elif run_mode == '3':
                self._run_training_only(train_files)
            elif run_mode == '4':
                self._run_prediction_only(test_files)
            elif run_mode == '5':
                self._run_model_comparison()
            elif run_mode == '6':
                self._run_cache_management()
            
            print("\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ!")
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        finally:
            input("\næŒ‰Enteré”®é€€å‡º...")
    
    def _run_full_workflow(self, train_files: List[Path], test_files: List[Path]):
        """å®Œæ•´å·¥ä½œæµ"""
        print("\nğŸš€ å®Œæ•´å·¥ä½œæµæ¨¡å¼")
        
        # è·å–é…ç½®
        data_config = self.ui.get_data_processing_config()
        training_config = self.ui.get_training_config()
        
        self.ui.show_config_summary(data_config, training_config)
        input("\næŒ‰Enteré”®å¼€å§‹...")
        
        # æ•°æ®å¤„ç†é˜¶æ®µ
        processed_data = self._execute_data_processing(train_files, data_config)
        if not processed_data:
            print("âŒ æ•°æ®å¤„ç†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return
        
        # è®­ç»ƒé˜¶æ®µ
        training_results = self._execute_training_with_processed_data(processed_data, training_config)
        
        # é¢„æµ‹é˜¶æ®µ
        if test_files and training_results:
            print(f"\n{'='*60}")
            print("ğŸ“ˆ é¢„æµ‹é˜¶æ®µ")
            print('='*60)
            
            prediction_config = {
                'prediction_models': training_config['selected_models'],
                'prediction_segments': list(range(len(test_files))),
                'ensemble_method': 'average'
            }
            
            self._execute_prediction(test_files, prediction_config)
        
        # ç»“æœå¯è§†åŒ–
        if training_results:
            self._generate_visualizations(training_results)
    
    def _run_data_processing_only(self, train_files: List[Path]):
        """ä»…æ•°æ®å¤„ç†æ¨¡å¼"""
        print("\nğŸ”„ ä»…æ•°æ®å¤„ç†æ¨¡å¼")
        
        data_config = self.ui.get_data_processing_config()
        self.ui.show_config_summary(data_config)
        
        input("\næŒ‰Enteré”®å¼€å§‹...")
        self._execute_data_processing(train_files, data_config)
    
    def _run_training_only(self, train_files: List[Path]):
        """ä»…è®­ç»ƒæ¨¡å¼"""
        print("\nğŸ¯ ä»…è®­ç»ƒæ¨¡å¼")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å¤„ç†æ•°æ®
        cache_info = config.get_cache_info()
        if cache_info['cache_count'] > 0:
            use_cache = input(f"å‘ç° {cache_info['cache_count']} ä¸ªç¼“å­˜æ–‡ä»¶ï¼Œæ˜¯å¦ä½¿ç”¨? (y/n, é»˜è®¤y): ").strip().lower() != 'n'
            if use_cache:
                data_config = {'data_process_mode': 'load_cached'}
            else:
                data_config = self.ui.get_data_processing_config()
        else:
            data_config = self.ui.get_data_processing_config()
        
        # è®­ç»ƒé…ç½®
        training_config = self.ui.get_training_config()
        self.ui.show_config_summary(data_config, training_config)
        
        input("\næŒ‰Enteré”®å¼€å§‹...")
        
        # æ•°æ®å¤„ç†
        processed_data = self._execute_data_processing(train_files, data_config)
        if not processed_data:
            return
        
        # è®­ç»ƒ
        training_results = self._execute_training_with_processed_data(processed_data, training_config)
        
        if training_results:
            self._generate_visualizations(training_results)
    
    def _run_prediction_only(self, test_files: List[Path]):
        """ä»…é¢„æµ‹æ¨¡å¼"""
        print("\nğŸ“ˆ ä»…é¢„æµ‹æ¨¡å¼")
        
        prediction_config = self.ui.get_prediction_config(self.model_manager)
        if not prediction_config.get('prediction_possible', False):
            return
        
        self._execute_prediction(test_files, prediction_config)
    
    def _run_model_comparison(self):
        """æ¨¡å‹æ¯”è¾ƒåˆ†æ"""
        print("\nğŸ“Š æ¨¡å‹æ¯”è¾ƒåˆ†ææ¨¡å¼")
        
        # è¯»å–å·²ä¿å­˜çš„ç»“æœ
        result_files = list(config.paths.output.glob("model_results_segment_*.csv"))
        
        if not result_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹ç»“æœ")
            return
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = []
        for file_path in result_files:
            df = pd.read_csv(file_path)
            segment_id = int(file_path.stem.split('_')[-1])
            df['Segment'] = segment_id
            all_results.append(df)
        
        if all_results:
            combined_results = pd.concat(all_results, ignore_index=True)
            self.visualizer.create_model_comparison_dashboard(combined_results)
            print("âœ… æ¨¡å‹æ¯”è¾ƒåˆ†æå®Œæˆ")
    
    def _run_cache_management(self):
        """ç¼“å­˜ç®¡ç†"""
        while True:
            choice = self.ui.show_cache_management()
            
            if choice == '1':
                self._show_cache_details()
            elif choice == '2':
                self._clear_cache()
            elif choice == '3':
                print("ğŸ”„ é‡æ–°ç”Ÿæˆç¼“å­˜éœ€è¦é‡æ–°å¤„ç†æ•°æ®ï¼Œè¯·é€‰æ‹©æ¨¡å¼2è¿›è¡Œæ•°æ®å¤„ç†")
            elif choice == '4':
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©")
    
    def _execute_data_processing(self, train_files: List[Path], data_config: Dict[str, Any]) -> Optional[List[Dict]]:
        """æ‰§è¡Œæ•°æ®å¤„ç†"""
        mode = data_config.get('data_process_mode', 'full_process')
        
        if mode == 'compare_modes':
            return self._execute_comparison_processing(train_files, data_config)
        elif mode == 'load_cached':
            return self._load_cached_data(train_files)
        else:
            return self._execute_single_processing(train_files, data_config)
    
    def _execute_single_processing(self, train_files: List[Path], data_config: Dict[str, Any]) -> Optional[List[Dict]]:
        """æ‰§è¡Œå•ä¸€æ•°æ®å¤„ç†æ¨¡å¼"""
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        data_processor = DataProcessor(
            feature_level=data_config.get('feature_level', 'enhanced'),
            max_features=data_config.get('max_features', 200),
            enable_selection=data_config.get('selection_mode', 'variance') != 'none'
        )
        
        processed_data = []
        
        with self.progress_tracker.create_training_progress(len(train_files)) as progress:
            for i, train_file in enumerate(train_files):
                progress.update_current_stage(f"å¤„ç†æ®µ {i}: {train_file.name}")
                
                try:
                    # åŠ è½½å’Œå¤„ç†æ•°æ®
                    df = data_processor.load_and_process_data(
                        train_file,
                        use_sampling=data_config.get('use_sampling', True),
                        num_groups=data_config.get('num_groups', 2000),
                        min_group_size=data_config.get('min_group_size', 20)
                    )
                    
                    # ä¿å­˜åˆ°ç¼“å­˜
                    if data_config.get('cache_processed_data', True):
                        cache_file = config.paths.cache_data / f"processed_segment_{i}.pkl"
                        df.to_pickle(cache_file)
                        print(f"ğŸ’¾ å·²ç¼“å­˜: {cache_file.name}")
                    
                    processed_data.append({
                        'segment_id': i,
                        'data': df,
                        'processor': data_processor
                    })
                    
                    progress.complete_stage()
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†æ®µ {i} å¤±è´¥: {e}")
                    progress.complete_stage(success=False)
                    continue
        
        return processed_data
    
    def _load_cached_data(self, train_files: List[Path]) -> Optional[List[Dict]]:
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        processed_data = []
        
        for i in range(len(train_files)):
            cache_file = config.paths.cache_data / f"processed_segment_{i}.pkl"
            if cache_file.exists():
                try:
                    df = pd.read_pickle(cache_file)
                    # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„æ•°æ®å¤„ç†å™¨
                    data_processor = DataProcessor()
                    
                    processed_data.append({
                        'segment_id': i,
                        'data': df,
                        'processor': data_processor
                    })
                    print(f"âœ… åŠ è½½ç¼“å­˜æ®µ {i}: {len(df)} æ¡è®°å½•")
                except Exception as e:
                    print(f"âŒ åŠ è½½ç¼“å­˜æ®µ {i} å¤±è´¥: {e}")
        
        if not processed_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç¼“å­˜æ•°æ®")
            return None
        
        return processed_data
    
    def _execute_training_with_processed_data(self, processed_data: List[Dict], training_config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å¤„ç†åçš„æ•°æ®æ‰§è¡Œè®­ç»ƒ"""
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ModelTrainer(
            model_manager=self.model_manager,
            enable_auto_tuning=training_config.get('enable_auto_tuning', False),
            auto_tuning_trials=training_config.get('auto_tuning_trials', 30)
        )
        
        training_results = []
        
        with self.progress_tracker.create_training_progress(len(processed_data)) as progress:
            for processed_item in processed_data:
                segment_id = processed_item['segment_id']
                df = processed_item['data']
                data_processor = processed_item['processor']
                
                progress.update_current_stage(f"è®­ç»ƒæ®µ {segment_id}")
                
                try:
                    # åˆ†å‰²æ•°æ®
                    data_split = data_processor.split_ranking_data(df)
                    (X_train, X_test, y_train, y_test, 
                     train_group_sizes, test_group_sizes, feature_cols, test_info) = data_split
                    
                    # è®­ç»ƒæ¨¡å‹
                    results_df = trainer.train_models(
                        model_names=training_config['selected_models'],
                        X_train=X_train, y_train=y_train, train_groups=train_group_sizes,
                        X_val=X_test, y_val=y_test, val_groups=test_group_sizes,
                        segment_id=segment_id, feature_names=feature_cols,
                        save_models=training_config.get('save_models', True)
                    )
                    
                    # ä¿å­˜ç»“æœ
                    if not results_df.empty:
                        results_file = config.paths.output / f"model_results_segment_{segment_id}.csv"
                        results_df.to_csv(results_file, index=False)
                        
                        best_model = results_df.loc[results_df['HitRate@3'].idxmax()]
                        training_results.append({
                            'segment_id': segment_id,
                            'results': results_df,
                            'best_model': best_model,
                            'feature_count': len(feature_cols)
                        })
                        
                        print(f"âœ… æ®µ {segment_id} æœ€ä½³: {best_model['Model']} "
                              f"(HitRate@3: {best_model['HitRate@3']:.4f})")
                    
                    progress.complete_stage()
                    
                except Exception as e:
                    print(f"âŒ è®­ç»ƒæ®µ {segment_id} å¤±è´¥: {e}")
                    progress.complete_stage(success=False)
                    continue
        
        return training_results
    
    def _execute_prediction(self, test_files: List[Path], prediction_config: Dict[str, Any]):
        """æ‰§è¡Œé¢„æµ‹"""
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨ï¼ˆç”¨äºæµ‹è¯•æ•°æ®å¤„ç†ï¼‰
        from data_processor import EnhancedDataProcessor
        data_processor = EnhancedDataProcessor()
        
        # åˆ›å»ºé¢„æµ‹å™¨
        predictor = ModelPredictor(self.model_manager, data_processor)
        
        # æ‰§è¡Œé¢„æµ‹
        final_result = predictor.predict_all_segments(
            segments=prediction_config['prediction_segments'],
            model_names=prediction_config['prediction_models'],
            test_data_path=config.paths.test_data,
            ensemble_method=prediction_config['ensemble_method'],
            output_path=config.paths.output
        )
        
        if final_result is not None:
            print(f"âœ… é¢„æµ‹å®Œæˆï¼Œæ€»è®°å½•æ•°: {len(final_result)}")
        else:
            print("âŒ é¢„æµ‹å¤±è´¥")
    
    def _execute_comparison_processing(self, train_files: List[Path], data_config: Dict[str, Any]) -> Optional[List[Dict]]:
        """æ‰§è¡Œæ¯”è¾ƒå¤„ç†æ¨¡å¼"""
        print("ğŸ”„ æ¯”è¾ƒä¸åŒå¤„ç†æ¨¡å¼...")
        
        comparison_modes = data_config.get('comparison_modes', ['raw_data', 'encoding_only', 'basic', 'enhanced', 'advanced'])
        comparison_results = []
        
        for mode in comparison_modes:
            print(f"\nå¤„ç†æ¨¡å¼: {mode}")
            
            # é…ç½®è¯¥æ¨¡å¼
            mode_config = data_config.copy()
            if mode == 'raw_data':
                mode_config.update({
                    'data_process_mode': 'raw_data',
                    'feature_level': 'none',
                    'selection_mode': 'none'
                })
            elif mode == 'encoding_only':
                mode_config.update({
                    'data_process_mode': 'encoding_only',
                    'feature_level': 'none'
                })
            else:
                mode_config.update({
                    'data_process_mode': 'full_process',
                    'feature_level': mode
                })
            
            # å¤„ç†æ•°æ®
            try:
                processed_data = self._execute_single_processing(train_files, mode_config)
                if processed_data:
                    comparison_results.extend(processed_data)
                    for item in processed_data:
                        item['processing_mode'] = mode
                    print(f"âœ… {mode}: å¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"âŒ {mode}: å¤„ç†å¤±è´¥ - {e}")
        
        return comparison_results if comparison_results else None
    
    def _load_cached_data(self, train_files: List[Path]) -> Optional[List[Dict]]:
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        print("ğŸ’¾ åŠ è½½ç¼“å­˜æ•°æ®...")
        
        processed_data = []
        
        for i in range(len(train_files)):
            cache_file = config.paths.cache_data / f"processed_segment_{i}.pkl"
            if cache_file.exists():
                try:
                    df = pd.read_pickle(cache_file)
                    # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„æ•°æ®å¤„ç†å™¨
                    from data_processor import EnhancedDataProcessor
                    data_processor = EnhancedDataProcessor()
                    
                    processed_data.append({
                        'segment_id': i,
                        'data': df,
                        'processor': data_processor
                    })
                    print(f"âœ… åŠ è½½ç¼“å­˜æ®µ {i}: {len(df)} æ¡è®°å½•")
                except Exception as e:
                    print(f"âŒ åŠ è½½ç¼“å­˜æ®µ {i} å¤±è´¥: {e}")
        
        if not processed_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç¼“å­˜æ•°æ®")
            return None
        
        return processed_data
    
    def _generate_visualizations(self, training_results: List[Dict[str, Any]]):
        """ç”Ÿæˆå¯è§†åŒ–ç»“æœ"""
        try:
            # åˆ›å»ºæ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾
            for result in training_results:
                self.visualizer.plot_model_performance(
                    result['results'], 
                    result['segment_id']
                )
            
            # åˆ›å»ºç»¼åˆåˆ†ææŠ¥å‘Š
            if len(training_results) > 1:
                self.visualizer.create_training_summary(training_results)
            
            print("âœ… å¯è§†åŒ–ç»“æœå·²ç”Ÿæˆ")
            
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆå¯è§†åŒ–å¤±è´¥: {e}")
    
    def _show_cache_details(self):
        """æ˜¾ç¤ºç¼“å­˜è¯¦ç»†ä¿¡æ¯"""
        cache_info = config.get_cache_info()
        
        print("\nğŸ“‹ ç¼“å­˜è¯¦ç»†ä¿¡æ¯:")
        print(f"ç¼“å­˜ç›®å½•: {cache_info['cache_dir']}")
        print(f"ç¼“å­˜æ–‡ä»¶æ•°: {cache_info.get('cache_count', 0)}")
        
        if cache_info.get('cache_count', 0) > 0:
            print("ç¼“å­˜æ–‡ä»¶åˆ—è¡¨:")
            for file_name in cache_info.get('cached_files', []):
                cache_file = Path(cache_info['cache_dir']) / file_name
                if cache_file.exists():
                    size_mb = cache_file.stat().st_size / (1024 * 1024)
                    print(f"  â€¢ {file_name} ({size_mb:.1f}MB)")
        
        input("\næŒ‰Enteré”®ç»§ç»­...")
    
    def _clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        cache_info = config.get_cache_info()
        
        if cache_info.get('cache_count', 0) == 0:
            print("ğŸ“­ å½“å‰æ²¡æœ‰ç¼“å­˜æ–‡ä»¶")
            return
        
        confirm = input(f"ç¡®è®¤æ¸…ç† {cache_info['cache_count']} ä¸ªç¼“å­˜æ–‡ä»¶? (y/N): ").strip().lower()
        
        if confirm == 'y':
            try:
                # æ¸…ç†ç¼“å­˜ç›®å½•ä¸­çš„æ‰€æœ‰pklæ–‡ä»¶
                cache_dir = Path(cache_info['cache_dir'])
                for cache_file in cache_dir.glob("*.pkl"):
                    cache_file.unlink()
                
                print("âœ… ç¼“å­˜å·²æ¸…ç†")
            except Exception as e:
                print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
        else:
            print("å–æ¶ˆæ¸…ç†æ“ä½œ")
    
    def _display_system_info(self, system_info: Dict[str, Any]):
        """æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯"""
        print("ğŸ’» ç³»ç»Ÿä¿¡æ¯:")
        key_items = ['python_version', 'gpu_available', 'memory_total_gb']
        for key in key_items:
            if key in system_info:
                value = system_info[key]
                if isinstance(value, bool):
                    status = "âœ…" if value else "âŒ"
                    print(f"  {status} {key.replace('_', ' ').title()}: {value}")
                else:
                    print(f"  â€¢ {key.replace('_', ' ').title()}: {value}")


class ApplicationManager:
    """åº”ç”¨ç¨‹åºç®¡ç†å™¨"""
    
    def __init__(self):
        self.workflow_manager = EnhancedWorkflowManager()
    
    def start(self):
        """å¯åŠ¨åº”ç”¨ç¨‹åº"""
        try:
            # æ£€æŸ¥Pythonç‰ˆæœ¬
            if sys.version_info < (3, 8):
                print("âŒ éœ€è¦Python 3.8æˆ–æ›´é«˜ç‰ˆæœ¬")
                return
            
            # è¿è¡Œå·¥ä½œæµ
            self.workflow_manager.run()
            
        except Exception as e:
            print(f"âŒ åº”ç”¨ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    app = ApplicationManager()
    app.start()


if __name__ == "__main__":
    main()
"""
ä¸»ç¨‹åºæ–‡ä»¶ - PyTorchç‰ˆæœ¬

è¯¥ç¨‹åºæä¾›äº†å®Œæ•´çš„èˆªç­æ’åºåˆ†ææµç¨‹
- æ–°å¢æ¨¡å‹ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½
- æ”¯æŒä»…é¢„æµ‹æ¨¡å¼
- æ”¹è¿›çš„ç”¨æˆ·äº¤äº’
- æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œç»“æœå±•ç¤º
- æ”¯æŒPyTorchæ·±åº¦å­¦ä¹ æ¨¡å‹

ä½œè€…: Flight Ranking Team
ç‰ˆæœ¬: 3.0 (PyTorchç‰ˆæœ¬)
"""

import os
import sys
import gc
import logging
import torch
from typing import List, Dict, Any

# å°è¯•ç›¸å¯¹å¯¼å…¥ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç»å¯¹å¯¼å…¥
try:
    from .config import Config
    from .analyzer import FlightRankingAnalyzer
    from .models import ModelFactory
    from .predictor import FlightRankingPredictor
except ImportError:
    from config import Config
    from analyzer import FlightRankingAnalyzer
    from models import ModelFactory
    from predictor import FlightRankingPredictor


def setup_logging() -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Config.ensure_output_dir()
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_file = os.path.join(Config.OUTPUT_PATH, 'analysis.log')
    
    logging.basicConfig(
        level=getattr(logging, Config.LOG_LEVEL),
        format=Config.LOG_FORMAT,
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(log_file, encoding='utf-8')
        ]
    )
    return logging.getLogger(__name__)


def check_gpu_availability() -> bool:
    """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨ï¼ˆPyTorchç‰ˆæœ¬ï¼‰"""
    try:
        # æ£€æŸ¥PyTorchå’ŒCUDA
        pytorch_available = True
        cuda_available = torch.cuda.is_available()
        
        if cuda_available:
            device_count = torch.cuda.device_count()
            device_name = torch.cuda.get_device_name(0)
            cuda_version = torch.version.cuda
            pytorch_version = torch.__version__
            
            print("âœ“ æ£€æµ‹åˆ°å®Œæ•´çš„GPUæ”¯æŒ")
            print(f"  PyTorchç‰ˆæœ¬: {pytorch_version}")
            print(f"  CUDAç‰ˆæœ¬: {cuda_version}")
            print(f"  GPUè®¾å¤‡æ•°é‡: {device_count}")
            print(f"  ä¸»GPU: {device_name}")
            
            # æ£€æŸ¥GPUå†…å­˜
            try:
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"  GPUå†…å­˜: {gpu_memory:.1f} GB")
            except:
                pass
            
            return True
        else:
            print("âš  PyTorchå¯ç”¨ï¼Œä½†æœªæ£€æµ‹åˆ°CUDAæ”¯æŒï¼Œå°†ä½¿ç”¨CPUç‰ˆæœ¬")
            print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
            return False
            
    except ImportError as e:
        print("âŒ PyTorchæœªæ­£ç¡®å®‰è£…")
        print(f"  é”™è¯¯: {str(e)}")
        print("  è¯·è¿è¡Œ: pip install torch torchvision torchaudio")
        return False
    except Exception as e:
        print(f"âš  GPUæ£€æµ‹æ—¶å‡ºç°é—®é¢˜: {str(e)}")
        print("  å°†ä½¿ç”¨CPUç‰ˆæœ¬")
        return False


def get_user_choices() -> Dict[str, Any]:
    """è·å–ç”¨æˆ·é€‰æ‹©"""
    print("\n" + "="*60)
    print("èˆªç­æ’åºåˆ†æç³»ç»Ÿ v3.0 (PyTorchç‰ˆæœ¬)")
    print("="*60)
    
    # è¿è¡Œæ¨¡å¼é€‰æ‹©
    print("\nè¿è¡Œæ¨¡å¼:")
    print("1. å®Œæ•´æµç¨‹ (è®­ç»ƒ + é¢„æµ‹)")
    print("2. ä»…è®­ç»ƒæ¨¡å‹")
    print("3. ä»…é¢„æµ‹ (ä½¿ç”¨å·²ä¿å­˜çš„æ¨¡å‹)")
    
    while True:
        run_mode = input("\nè¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ (1-3): ").strip()
        if run_mode in ['1', '2', '3']:
            break
        print("è¯·è¾“å…¥æœ‰æ•ˆé€‰æ‹© (1-3)")
    
    choices = {'run_mode': run_mode}
    
    # å¦‚æœé€‰æ‹©ä»…é¢„æµ‹ï¼Œè¯¢é—®é¢„æµ‹è®¾ç½®
    if run_mode == '3':
        return get_prediction_choices(choices)
    
    # è®­ç»ƒç›¸å…³é€‰æ‹©
    choices.update(get_training_choices())
    
    return choices


def get_training_choices() -> Dict[str, Any]:
    """è·å–è®­ç»ƒç›¸å…³é€‰æ‹©"""
    choices = {}
    
    # æ•°æ®åŠ è½½é€‰æ‹©
    print("\næ•°æ®åŠ è½½æ¨¡å¼:")
    print("1. æŠ½æ ·æ¨¡å¼ (æ¨èç”¨äºå¿«é€Ÿæµ‹è¯•)")
    print("2. å…¨é‡æ¨¡å¼ (ä½¿ç”¨æ‰€æœ‰æ•°æ®)")
    
    while True:
        data_mode = input("\nè¯·é€‰æ‹©æ•°æ®åŠ è½½æ¨¡å¼ (1-2): ").strip()
        if data_mode in ['1', '2']:
            break
        print("è¯·è¾“å…¥æœ‰æ•ˆé€‰æ‹© (1-2)")
    
    use_sampling = data_mode == '1'
    choices['use_sampling'] = use_sampling
    
    # æŠ½æ ·å‚æ•°è®¾ç½®
    num_groups = Config.DEFAULT_NUM_GROUPS
    min_group_size = Config.DEFAULT_MIN_GROUP_SIZE
    
    if use_sampling:
        print(f"\nå½“å‰æŠ½æ ·å‚æ•°:")
        print(f"- æ¯ä¸ªæ–‡ä»¶æŠ½å–ç»„æ•°: {num_groups}")
        print(f"- æ¯ç»„æœ€å°æ•°æ®æ¡æ•°: {min_group_size}")
        
        modify = input("\næ˜¯å¦ä¿®æ”¹æŠ½æ ·å‚æ•°? (y/n): ").strip().lower()
        if modify == 'y':
            try:
                num_groups = int(input(f"è¯·è¾“å…¥æ¯ä¸ªæ–‡ä»¶æŠ½å–çš„ç»„æ•° (é»˜è®¤{num_groups}): ") or num_groups)
                min_group_size = int(input(f"è¯·è¾“å…¥æ¯ç»„æœ€å°æ•°æ®æ¡æ•° (é»˜è®¤{min_group_size}): ") or min_group_size)
            except ValueError:
                print("ä½¿ç”¨é»˜è®¤å‚æ•°")
    
    choices['num_groups'] = num_groups
    choices['min_group_size'] = min_group_size
    
    # æ¨¡å‹é€‰æ‹©
    print("\nå¯ç”¨æ¨¡å‹:")
    available_models = ModelFactory.get_available_models()
    for i, model in enumerate(available_models, 1):
        model_type = "ğŸ”¥ PyTorch" if model in ['NeuralRanker', 'RankNet', 'TransformerRanker'] else "ğŸ“Š ä¼ ç»Ÿ"
        print(f"{i}. {model} ({model_type})")
    print(f"{len(available_models) + 1}. æ‰€æœ‰æ¨¡å‹")
    
    model_choices = input(f"\nè¯·é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹(ç”¨é€—å·åˆ†éš”,å¦‚1,2,3): ").strip().split(',')
    selected_models = []
    
    if str(len(available_models) + 1) in model_choices:
        selected_models = available_models
    else:
        for choice in model_choices:
            try:
                idx = int(choice.strip()) - 1
                if 0 <= idx < len(available_models):
                    selected_models.append(available_models[idx])
            except ValueError:
                continue
    
    if not selected_models:
        print("æœªé€‰æ‹©ä»»ä½•æ¨¡å‹ï¼Œå°†é»˜è®¤è¿è¡Œæ‰€æœ‰æ¨¡å‹")
        selected_models = available_models
    
    choices['selected_models'] = selected_models
    
    # PyTorchæ¨¡å‹ç‰¹æ®Šæç¤º
    pytorch_models = [m for m in selected_models if m in ['NeuralRanker', 'RankNet', 'TransformerRanker']]
    if pytorch_models:
        print(f"\nğŸ”¥ å°†è®­ç»ƒä»¥ä¸‹PyTorchæ¨¡å‹: {', '.join(pytorch_models)}")
        print("   è¿™äº›æ¨¡å‹æ”¯æŒGPUåŠ é€Ÿï¼Œè®­ç»ƒæ—¶é—´è¾ƒé•¿ä½†æ•ˆæœå¯èƒ½æ›´å¥½")
    
    # è‡ªåŠ¨è°ƒå‚é€‰æ‹©
    print("\nè‡ªåŠ¨è°ƒå‚è®¾ç½®:")
    print("è‡ªåŠ¨è°ƒå‚å¯ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œä½†ä¼šæ˜¾è‘—å¢åŠ è¿è¡Œæ—¶é—´")
    if pytorch_models:
        print("âš ï¸  æ³¨æ„: PyTorchæ¨¡å‹çš„è‡ªåŠ¨è°ƒå‚æ—¶é—´ä¼šæ›´é•¿")
    
    enable_auto_tuning = input("æ˜¯å¦å¯ç”¨è‡ªåŠ¨è°ƒå‚? (y/n, é»˜è®¤n): ").strip().lower() == 'y'
    
    auto_tuning_trials = Config.AUTO_TUNING_TRIALS
    if enable_auto_tuning:
        try:
            auto_tuning_trials = int(input(f"è°ƒå‚è¯•éªŒæ¬¡æ•° (é»˜è®¤{auto_tuning_trials}): ") or auto_tuning_trials)
            if pytorch_models:
                suggested_trials = min(auto_tuning_trials, 30)
                print(f"ğŸ’¡ å»ºè®®PyTorchæ¨¡å‹ä½¿ç”¨è¾ƒå°‘è¯•éªŒæ¬¡æ•° (å¦‚{suggested_trials}) ä»¥èŠ‚çœæ—¶é—´")
        except ValueError:
            pass
    
    choices['enable_auto_tuning'] = enable_auto_tuning
    choices['auto_tuning_trials'] = auto_tuning_trials
    
    # æ¨¡å‹ä¿å­˜é€‰æ‹©
    save_models = input("\næ˜¯å¦ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹? (y/n, é»˜è®¤y): ").strip().lower() != 'n'
    choices['save_models'] = save_models
    
    return choices


def get_prediction_choices(choices: Dict[str, Any]) -> Dict[str, Any]:
    """è·å–é¢„æµ‹ç›¸å…³é€‰æ‹©"""
    print("\né¢„æµ‹è®¾ç½®:")
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨æ¥æ£€æŸ¥å¯ç”¨æ¨¡å‹
    predictor = FlightRankingPredictor(data_path=Config.DATA_BASE_PATH)
    available_models = predictor.get_available_models()
    
    if not available_models:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        choices['prediction_possible'] = False
        return choices
    
    print("å¯ç”¨çš„å·²ä¿å­˜æ¨¡å‹:")
    predictor.print_model_summary()
    
    # é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹
    model_names = list(available_models.keys())
    print(f"\næ¨¡å‹é€‰æ‹©:")
    for i, model_name in enumerate(model_names, 1):
        segments = available_models[model_name]
        model_type = "ğŸ”¥ PyTorch" if model_name in ['NeuralRanker', 'RankNet', 'TransformerRanker'] else "ğŸ“Š ä¼ ç»Ÿ"
        print(f"{i}. {model_name} ({model_type}) - æ®µ: {segments}")
    print(f"{len(model_names) + 1}. æ‰€æœ‰æ¨¡å‹")
    
    model_choices = input(f"\nè¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹(ç”¨é€—å·åˆ†éš”): ").strip().split(',')
    selected_models = []
    
    if str(len(model_names) + 1) in model_choices:
        selected_models = model_names
    else:
        for choice in model_choices:
            try:
                idx = int(choice.strip()) - 1
                if 0 <= idx < len(model_names):
                    selected_models.append(model_names[idx])
            except ValueError:
                continue
    
    if not selected_models:
        selected_models = model_names
    
    choices['prediction_models'] = selected_models
    
    # PyTorchæ¨¡å‹é¢„æµ‹æç¤º
    pytorch_models = [m for m in selected_models if m in ['NeuralRanker', 'RankNet', 'TransformerRanker']]
    if pytorch_models:
        print(f"\nğŸ”¥ å°†ä½¿ç”¨ä»¥ä¸‹PyTorchæ¨¡å‹è¿›è¡Œé¢„æµ‹: {', '.join(pytorch_models)}")
    
    # é€‰æ‹©è¦é¢„æµ‹çš„æ•°æ®æ®µ
    all_segments = set()
    for model_name in selected_models:
        all_segments.update(available_models[model_name])
    all_segments = sorted(list(all_segments))
    
    print(f"\nå¯é¢„æµ‹çš„æ•°æ®æ®µ: {all_segments}")
    segment_input = input(f"è¯·é€‰æ‹©è¦é¢„æµ‹çš„æ•°æ®æ®µ(ç”¨é€—å·åˆ†éš”,é»˜è®¤å…¨éƒ¨): ").strip()
    
    if segment_input:
        try:
            selected_segments = [int(s.strip()) for s in segment_input.split(',')]
            selected_segments = [s for s in selected_segments if s in all_segments]
        except ValueError:
            selected_segments = all_segments
    else:
        selected_segments = all_segments
    
    choices['prediction_segments'] = selected_segments
    
    # é›†æˆæ–¹æ³•é€‰æ‹©
    print("\né›†æˆæ–¹æ³•:")
    print("1. å¹³å‡åˆ†æ•° (average)")
    print("2. æ’åæŠ•ç¥¨ (voting)")
    print("3. åŠ æƒå¹³å‡ (weighted)")
    
    ensemble_choice = input("è¯·é€‰æ‹©é›†æˆæ–¹æ³• (1-3, é»˜è®¤1): ").strip()
    ensemble_methods = {'1': 'average', '2': 'voting', '3': 'weighted'}
    ensemble_method = ensemble_methods.get(ensemble_choice, 'average')
    
    choices['ensemble_method'] = ensemble_method
    choices['prediction_possible'] = True
    
    return choices


def validate_data_files() -> tuple[List[str], List[str]]:
    """éªŒè¯æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    train_files = Config.get_train_files()
    test_files = Config.get_test_files()
    
    print(f"\næ£€æŸ¥æ•°æ®æ–‡ä»¶...")
    print(f"è®­ç»ƒæ–‡ä»¶: æ‰¾åˆ° {len(train_files)} ä¸ª")
    for f in train_files:
        print(f"  âœ“ {f}")
    
    print(f"æµ‹è¯•æ–‡ä»¶: æ‰¾åˆ° {len(test_files)} ä¸ª")
    for f in test_files:
        print(f"  âœ“ {f}")
    
    if not train_files:
        raise FileNotFoundError("æœªæ‰¾åˆ°è®­ç»ƒæ–‡ä»¶")
    if not test_files:
        raise FileNotFoundError("æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
    
    return train_files, test_files


def run_training_phase(analyzer: FlightRankingAnalyzer, 
                      train_files: List[str],
                      use_sampling: bool,
                      num_groups: int,
                      min_group_size: int) -> Dict[str, Any]:
    """æ‰§è¡Œè®­ç»ƒé˜¶æ®µ"""
    print(f"\n" + "="*60)
    print("å¼€å§‹è®­ç»ƒé˜¶æ®µ (PyTorchç‰ˆæœ¬)")
    print("="*60)
    
    all_results = {}
    
    for i, train_path in enumerate(train_files):
        try:
            print(f"\n{'='*40}")
            print(f"è®­ç»ƒæ®µ {i}: {os.path.basename(train_path)}")
            if use_sampling:
                print(f"æŠ½æ ·å‚æ•°: {num_groups}ä¸ªç»„, æ¯ç»„è‡³å°‘{min_group_size}æ¡æ•°æ®")
            else:
                print("ä½¿ç”¨å…¨é‡æ•°æ®")
            print('='*40)
            
            result = analyzer.full_analysis(
                train_path, 
                use_sampling=use_sampling,
                num_groups=num_groups, 
                min_group_size=min_group_size
            )
            all_results[f'train_segment_{i}'] = result
            
            # æ˜¾ç¤ºé‡è¦ç‰¹å¾
            if 'feature_importance' in result and result['feature_importance'] is not None:
                top_features = result['feature_importance'].head(10)
                print(f"\nè®­ç»ƒæ®µ {i} ä¸­æœ€é‡è¦çš„10ä¸ªç‰¹å¾:")
                for feature, importance in top_features.items():
                    print(f"  {feature}: {importance:.4f}")
            
            # é‡Šæ”¾å†…å­˜
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"\nåˆ†æè®­ç»ƒæ®µ {train_path} æ—¶å‡ºé”™: {str(e)}")
            continue
    
    return all_results


def run_prediction_phase_with_saved_models(user_choices: Dict[str, Any]) -> str:
    """ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹æ‰§è¡Œé¢„æµ‹é˜¶æ®µ"""
    print(f"\n" + "="*60)
    print("å¼€å§‹é¢„æµ‹é˜¶æ®µ (ä½¿ç”¨ä¿å­˜çš„PyTorchæ¨¡å‹)")
    print("="*60)
    
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = FlightRankingPredictor(data_path=Config.DATA_BASE_PATH)
    
    # æ‰§è¡Œé¢„æµ‹
    result = predictor.predict_all(
        segments=user_choices['prediction_segments'],
        model_names=user_choices['prediction_models'],
        ensemble_method=user_choices['ensemble_method']
    )
    
    if result is not None:
        model_suffix = "_".join(user_choices['prediction_models'])
        final_output = predictor.output_path / f"{model_suffix}_final_submission.csv"
        print(f"âœ… é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {final_output}")
        return str(final_output)
    else:
        raise ValueError("é¢„æµ‹å¤±è´¥")


def run_prediction_phase_legacy(analyzer: FlightRankingAnalyzer, 
                               test_files: List[str]) -> List[str]:
    """æ‰§è¡Œé¢„æµ‹é˜¶æ®µ (ä¼ ç»Ÿæ–¹æ³•)"""
    print(f"\n" + "="*60)
    print("å¼€å§‹é¢„æµ‹é˜¶æ®µ (ä¼ ç»Ÿæ–¹æ³•)")
    print("="*60)
    
    prediction_files = []
    
    for i, test_path in enumerate(test_files):
        try:
            print(f"\nå¤„ç†æµ‹è¯•æ–‡ä»¶ {i}: {os.path.basename(test_path)}")
            
            result_file = analyzer.predict_test_data(test_path, i)
            
            if result_file and os.path.exists(result_file):
                prediction_files.append(result_file)
                print(f"âœ“ é¢„æµ‹å®Œæˆ: {os.path.basename(result_file)}")
            else:
                print(f"âœ— é¢„æµ‹å¤±è´¥")
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âœ— é¢„æµ‹æµ‹è¯•æ–‡ä»¶ {test_path} æ—¶å‡ºé”™: {str(e)}")
            continue
    
    return prediction_files


def run_merge_phase(analyzer: FlightRankingAnalyzer, 
                   prediction_files: List[str]) -> str:
    """æ‰§è¡Œç»“æœåˆå¹¶é˜¶æ®µ"""
    print(f"\n" + "="*60)
    print("å¼€å§‹ç»“æœåˆå¹¶é˜¶æ®µ")
    print("="*60)
    
    if not prediction_files:
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„é¢„æµ‹æ–‡ä»¶è¿›è¡Œåˆå¹¶")
    
    print(f"æ‰¾åˆ° {len(prediction_files)} ä¸ªé¢„æµ‹æ–‡ä»¶:")
    for f in prediction_files:
        print(f"  - {os.path.basename(f)}")
    
    # åˆå¹¶é¢„æµ‹ç»“æœ
    output_file = os.path.join(Config.OUTPUT_PATH, Config.FINAL_PREDICTION_FILENAME)
    submission_file = Config.SUBMISSION_FILE_PATH
    
    try:
        final_result = analyzer.merge_all_predictions(
            prediction_files=prediction_files,
            submission_file=submission_file,
            output_file=output_file
        )
        
        print(f"âœ“ æœ€ç»ˆé¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {final_result}")
        return final_result
        
    except Exception as e:
        print(f"âœ— åˆå¹¶é¢„æµ‹ç»“æœæ—¶å‡ºé”™: {str(e)}")
        raise


def print_summary(train_results: Dict[str, Any] = None, 
                 prediction_files: List[str] = None,
                 final_result: str = None,
                 selected_models: List[str] = None,
                 enable_auto_tuning: bool = False,
                 run_mode: str = "1"):
    """æ‰“å°åˆ†ææ€»ç»“"""
    print(f"\n" + "="*60)
    print("åˆ†æå®Œæˆæ€»ç»“ (PyTorchç‰ˆæœ¬)")
    print("="*60)
    
    # æ˜¾ç¤ºPyTorchä¿¡æ¯
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    else:
        print("è¿è¡Œè®¾å¤‡: CPU")
    
    if run_mode == "1":
        print("è¿è¡Œæ¨¡å¼: å®Œæ•´æµç¨‹ (è®­ç»ƒ + é¢„æµ‹)")
    elif run_mode == "2":
        print("è¿è¡Œæ¨¡å¼: ä»…è®­ç»ƒæ¨¡å‹")
    elif run_mode == "3":
        print("è¿è¡Œæ¨¡å¼: ä»…é¢„æµ‹")
    
    if selected_models:
        print(f"ä½¿ç”¨çš„æ¨¡å‹: {', '.join(selected_models)}")
        
        # åˆ†ç±»æ˜¾ç¤ºæ¨¡å‹ç±»å‹
        pytorch_models = [m for m in selected_models if m in ['NeuralRanker', 'RankNet', 'TransformerRanker']]
        traditional_models = [m for m in selected_models if m not in pytorch_models]
        
        if pytorch_models:
            print(f"  ğŸ”¥ PyTorchæ¨¡å‹: {', '.join(pytorch_models)}")
        if traditional_models:
            print(f"  ğŸ“Š ä¼ ç»Ÿæ¨¡å‹: {', '.join(traditional_models)}")
    
    if run_mode in ["1", "2"]:
        print(f"è‡ªåŠ¨è°ƒå‚: {'å¯ç”¨' if enable_auto_tuning else 'å…³é—­'}")
        if train_results:
            print(f"è®­ç»ƒæ®µæ•°: {len(train_results)}")
            
            # æ˜¾ç¤ºå„æ®µæœ€ä½³æ¨¡å‹æ€§èƒ½
            print(f"\nå„è®­ç»ƒæ®µæœ€ä½³æ¨¡å‹æ€§èƒ½:")
            for segment_name, result in train_results.items():
                if 'model_results' in result and not result['model_results'].empty:
                    best_model = result['model_results'].loc[result['model_results']['HitRate@3'].idxmax()]
                    model_type = "ğŸ”¥" if best_model['Model'] in ['NeuralRanker', 'RankNet', 'TransformerRanker'] else "ğŸ“Š"
                    print(f"  {segment_name}: {best_model['Model']} {model_type} (HitRate@3: {best_model['HitRate@3']:.4f})")
    
    if prediction_files:
        print(f"é¢„æµ‹æ–‡ä»¶æ•°: {len(prediction_files)}")
    
    if final_result:
        print(f"æœ€ç»ˆç»“æœæ–‡ä»¶: {os.path.basename(final_result)}")
    
    print(f"\næ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {Config.OUTPUT_PATH}")


def main():
    """ä¸»å‡½æ•°"""
    logger = None
    try:
        # è®¾ç½®æ—¥å¿—
        logger = setup_logging()
        
        # éªŒè¯é…ç½®
        Config.validate_config()
        
        # æ£€æŸ¥GPUå’ŒPyTorch
        use_gpu = check_gpu_availability()
        
        # è·å–ç”¨æˆ·é€‰æ‹©
        user_choices = get_user_choices()
        
        # éªŒè¯æ•°æ®æ–‡ä»¶
        train_files, test_files = validate_data_files()
        
        run_mode = user_choices['run_mode']
        
        # æ ¹æ®è¿è¡Œæ¨¡å¼æ‰§è¡Œä¸åŒæµç¨‹
        if run_mode == '3':  # ä»…é¢„æµ‹æ¨¡å¼
            if not user_choices.get('prediction_possible', False):
                print("âŒ æ— æ³•æ‰§è¡Œé¢„æµ‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
                return
            
            print(f"\nå°†ä½¿ç”¨æ¨¡å‹: {', '.join(user_choices['prediction_models'])}")
            print(f"é¢„æµ‹æ•°æ®æ®µ: {user_choices['prediction_segments']}")
            print(f"é›†æˆæ–¹æ³•: {user_choices['ensemble_method']}")
            
            # æ˜¾ç¤ºPyTorchæ¨¡å‹æç¤º
            pytorch_models = [m for m in user_choices['prediction_models'] 
                            if m in ['NeuralRanker', 'RankNet', 'TransformerRanker']]
            if pytorch_models:
                print(f"ğŸ”¥ å°†ä½¿ç”¨PyTorchæ¨¡å‹: {', '.join(pytorch_models)}")
            
            input("\næŒ‰Enteré”®å¼€å§‹é¢„æµ‹...")
            
            # ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
            final_result = run_prediction_phase_with_saved_models(user_choices)
            
            # æ‰“å°æ€»ç»“
            print_summary(
                final_result=final_result,
                selected_models=user_choices['prediction_models'],
                run_mode=run_mode
            )
            
        else:  # è®­ç»ƒæ¨¡å¼ (1æˆ–2)
            # åˆå§‹åŒ–åˆ†æå™¨
            analyzer = FlightRankingAnalyzer(
                use_gpu=use_gpu,
                logger=logger,
                selected_models=user_choices['selected_models'],
                enable_auto_tuning=user_choices['enable_auto_tuning'],
                auto_tuning_trials=user_choices['auto_tuning_trials'],
                save_models=user_choices.get('save_models', True)
            )
            
            print(f"\nå°†è¿è¡Œä»¥ä¸‹æ¨¡å‹: {', '.join(user_choices['selected_models'])}")
            print(f"æ•°æ®æ¨¡å¼: {'æŠ½æ ·' if user_choices['use_sampling'] else 'å…¨é‡'}")
            print(f"è‡ªåŠ¨è°ƒå‚: {'å¯ç”¨' if user_choices['enable_auto_tuning'] else 'å…³é—­'}")
            print(f"æ¨¡å‹ä¿å­˜: {'å¯ç”¨' if user_choices.get('save_models', True) else 'å…³é—­'}")
            print(f"GPUæ”¯æŒ: {'å¯ç”¨' if use_gpu else 'å…³é—­'}")
            
            # PyTorchæ¨¡å‹ç‰¹åˆ«æç¤º
            pytorch_models = [m for m in user_choices['selected_models'] 
                            if m in ['NeuralRanker', 'RankNet', 'TransformerRanker']]
            if pytorch_models:
                print(f"ğŸ”¥ PyTorchæ¨¡å‹: {', '.join(pytorch_models)}")
                if use_gpu:
                    print("   GPUåŠ é€Ÿå·²å¯ç”¨ï¼Œè®­ç»ƒé€Ÿåº¦ä¼šæ›´å¿«")
                else:
                    print("   ä½¿ç”¨CPUè®­ç»ƒï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´")
            
            input("\næŒ‰Enteré”®å¼€å§‹åˆ†æ...")
            
            # æ‰§è¡Œè®­ç»ƒé˜¶æ®µ
            train_results = run_training_phase(
                analyzer=analyzer,
                train_files=train_files,
                use_sampling=user_choices['use_sampling'],
                num_groups=user_choices['num_groups'],
                min_group_size=user_choices['min_group_size']
            )
            
            final_result = None
            prediction_files = None
            
            # å¦‚æœæ˜¯å®Œæ•´æµç¨‹ï¼Œæ‰§è¡Œé¢„æµ‹
            if run_mode == '1':
                try:
                    # å°è¯•ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
                    print("\nå°è¯•ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹...")
                    available_models = analyzer.predictor.get_available_models()
                    
                    if available_models:
                        # ä½¿ç”¨æ”¹è¿›çš„é¢„æµ‹æ–¹æ³•
                        final_result = analyzer.predict_with_saved_models(
                            segments=list(range(len(test_files))),
                            model_names=user_choices['selected_models'],
                            ensemble_method='average'
                        )
                        if final_result is not None:
                            print("âœ… ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹é¢„æµ‹æˆåŠŸ")
                        else:
                            print("âš ï¸ ä¿å­˜çš„æ¨¡å‹é¢„æµ‹å¤±è´¥ï¼Œå°è¯•ä¼ ç»Ÿæ–¹æ³•...")
                            raise Exception("ä¿å­˜çš„æ¨¡å‹é¢„æµ‹å¤±è´¥")
                    else:
                        raise Exception("æ²¡æœ‰ä¿å­˜çš„æ¨¡å‹")
                        
                except Exception as e:
                    print(f"âš ï¸ ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹é¢„æµ‹å¤±è´¥: {str(e)}")
                    print("ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œé¢„æµ‹...")
                    
                    # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•é¢„æµ‹
                    prediction_files = run_prediction_phase_legacy(
                        analyzer=analyzer,
                        test_files=test_files
                    )
                    
                    # æ‰§è¡Œåˆå¹¶é˜¶æ®µ
                    if prediction_files:
                        final_result = run_merge_phase(
                            analyzer=analyzer,
                            prediction_files=prediction_files
                        )
            
            # æ‰“å°æ€»ç»“
            print_summary(
                train_results=train_results,
                prediction_files=prediction_files,
                final_result=final_result,
                selected_models=user_choices['selected_models'],
                enable_auto_tuning=user_choices['enable_auto_tuning'],
                run_mode=run_mode
            )
        
    except KeyboardInterrupt:
        print("\n\nåˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        error_msg = f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"\n\n{error_msg}")
        
        # å¦‚æœloggerå·²ç»åˆå§‹åŒ–ï¼Œåˆ™è®°å½•è¯¦ç»†é”™è¯¯
        if logger is not None:
            logger.error(f"åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
        else:
            # å¦‚æœloggeræœªåˆå§‹åŒ–ï¼Œè‡³å°‘æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
            import traceback
            print("\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
        
        sys.exit(1)
    finally:
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
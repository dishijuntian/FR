"""
ä¸»åˆ†æå™¨æ¨¡å— - ä¿®å¤æ’åé‡å¤é—®é¢˜ç‰ˆæœ¬

è¯¥æ¨¡å—æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼Œå®ç°å®Œæ•´çš„èˆªç­æ’åºåˆ†æåŠŸèƒ½
- ä¿®å¤äº†æ’åé‡å¤é—®é¢˜
- æ”¹è¿›äº†æ’åå”¯ä¸€æ€§ä¿è¯æœºåˆ¶
- åŠ å¼ºäº†ç»“æœéªŒè¯

ä½œè€…: Flight Ranking Team
ç‰ˆæœ¬: 3.1 (ä¿®å¤æ’åé—®é¢˜)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import gc
import os
import joblib
import tensorflow as tf
from typing import Dict, List, Optional, Union, Any, Tuple
import warnings
from collections import defaultdict

# å°è¯•ç›¸å¯¹å¯¼å…¥ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç»å¯¹å¯¼å…¥
try:
    from .models import ModelFactory, BaseRanker
    from .data_processor import DataProcessor, PredictionMerger
    from .auto_tuner import AutoTuner, create_auto_tuner
    from .config import Config
    from .progress_utils import (
        ProgressTracker, ModelTrainingProgress, 
        create_data_loading_progress, show_completion_summary,
        progress_bar
    )
    from .predictor import FlightRankingPredictor
except ImportError:
    from models import ModelFactory, BaseRanker
    from data_processor import DataProcessor, PredictionMerger
    from auto_tuner import AutoTuner, create_auto_tuner
    from config import Config
    from progress_utils import (
        ProgressTracker, ModelTrainingProgress, 
        create_data_loading_progress, show_completion_summary,
        progress_bar
    )
    from predictor import FlightRankingPredictor

warnings.filterwarnings('ignore')


class FlightRankingAnalyzer:
    """èˆªç­æ’åºåˆ†æå™¨ - ä¿®å¤æ’åé‡å¤é—®é¢˜ç‰ˆæœ¬"""
    
    def __init__(self, 
                 use_gpu: bool = True, 
                 logger=None, 
                 selected_models: Optional[List[str]] = None,
                 enable_auto_tuning: bool = False,
                 auto_tuning_trials: int = 50,
                 save_models: bool = True):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            use_gpu: æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
            logger: æ—¥å¿—è®°å½•å™¨
            selected_models: è¦è¿è¡Œçš„æ¨¡å‹åç§°åˆ—è¡¨
            enable_auto_tuning: æ˜¯å¦å¯ç”¨è‡ªåŠ¨è°ƒå‚
            auto_tuning_trials: è‡ªåŠ¨è°ƒå‚è¯•éªŒæ¬¡æ•°
            save_models: æ˜¯å¦ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        self.logger = logger
        self.use_gpu = use_gpu
        self.selected_models = selected_models or Config.AVAILABLE_MODELS
        self.enable_auto_tuning = enable_auto_tuning
        self.auto_tuning_trials = auto_tuning_trials
        self.save_models = save_models
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_processor = DataProcessor(logger=logger)
        self.prediction_merger = PredictionMerger(logger=logger)
        
        # åˆå§‹åŒ–é¢„æµ‹å™¨
        self.predictor = FlightRankingPredictor(
            data_path=Config.DATA_BASE_PATH,
            use_gpu=use_gpu,
            logger=logger
        )
        
        # å­˜å‚¨è®­ç»ƒç»“æœ
        self.feature_importance_results = {}
        self.shap_values = {}
        self.trained_models = {}
        self.model_performances = {}
        
        # è‡ªåŠ¨è°ƒå‚å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.auto_tuner = None
        
        self._log(f"åˆå§‹åŒ–åˆ†æå™¨å®Œæˆ, é€‰æ‹©çš„æ¨¡å‹: {self.selected_models}")
        self._log(f"è‡ªåŠ¨è°ƒå‚: {'å¯ç”¨' if enable_auto_tuning else 'å…³é—­'}")
        self._log(f"æ¨¡å‹ä¿å­˜: {'å¯ç”¨' if save_models else 'å…³é—­'}")
    
    def _log(self, message):
        """è®°å½•æ—¥å¿—"""
        if self.logger:
            self.logger.info(message)
        else:
            print(message)
    
    def calculate_hitrate_at_k(self, y_true: pd.DataFrame, y_pred_ranks: np.ndarray, k: int = 3) -> float:
        """
        è®¡ç®—HitRate@KæŒ‡æ ‡
        
        Args:
            y_true: åŒ…å«ranker_idå’Œselectedåˆ—çš„çœŸå®æ ‡ç­¾
            y_pred_ranks: é¢„æµ‹æ’å
            k: Top-Kå‚æ•°
            
        Returns:
            float: HitRate@Kåˆ†æ•°
        """
        hits = 0
        total_queries = 0
        
        # æŒ‰ranker_idåˆ†ç»„è®¡ç®—
        for ranker_id in y_true['ranker_id'].unique():
            group_mask = y_true['ranker_id'] == ranker_id
            group_true = y_true[group_mask]['selected'].values
            group_ranks = y_pred_ranks[group_mask]
            
            # æ‰¾åˆ°çœŸå®é€‰æ‹©çš„èˆªç­ä½ç½®
            true_idx = np.where(group_true == 1)[0]
            if len(true_idx) > 0:
                true_rank = group_ranks[true_idx[0]]
                if true_rank <= k:
                    hits += 1
            total_queries += 1
        
        return hits / total_queries if total_queries > 0 else 0
    
    def _calculate_group_ranks(self, scores: np.ndarray, group_sizes: List[int]) -> np.ndarray:
        """
        ä¿®å¤ç‰ˆæœ¬ï¼šç¡®ä¿æ’åå”¯ä¸€ä¸”è¿ç»­
        
        Args:
            scores: é¢„æµ‹åˆ†æ•°
            group_sizes: æ¯ç»„çš„å¤§å°
            
        Returns:
            np.ndarray: å”¯ä¸€ä¸”è¿ç»­çš„æ’å
        """
        ranks = np.zeros_like(scores, dtype=int)
        start_idx = 0
        
        for group_idx, group_size in enumerate(group_sizes):
            end_idx = start_idx + group_size
            group_scores = scores[start_idx:end_idx]
            
            if group_size == 1:
                # å•ä¸ªå…ƒç´ çš„ç»„ï¼Œæ’åç›´æ¥ä¸º1
                ranks[start_idx:end_idx] = 1
            else:
                # å¤šä¸ªå…ƒç´ çš„ç»„ï¼Œéœ€è¦ç¡®ä¿æ’åå”¯ä¸€
                # ä½¿ç”¨ç»„ç´¢å¼•å’Œä½ç½®ç´¢å¼•åˆ›å»ºå”¯ä¸€çš„éšæœºç§å­
                unique_seed = (group_idx * 12345 + start_idx) % 2147483647
                np.random.seed(unique_seed)
                
                # æ·»åŠ å¾®å°ä½†è¶³å¤Ÿçš„éšæœºå™ªå£°
                noise_scale = 1e-8  # å¢åŠ å™ªå£°å¼ºåº¦
                noise = np.random.random(len(group_scores)) * noise_scale
                noisy_scores = group_scores + noise
                
                # è®¡ç®—æ’åï¼šåˆ†æ•°è¶Šé«˜ï¼Œæ’åè¶Šé å‰ï¼ˆrank=1æœ€å¥½ï¼‰
                sorted_indices = np.argsort(-noisy_scores)  # é™åºæ’åˆ—çš„ç´¢å¼•
                group_ranks = np.zeros(group_size, dtype=int)
                
                # åˆ†é…å”¯ä¸€ä¸”è¿ç»­çš„æ’å
                for rank, idx in enumerate(sorted_indices):
                    group_ranks[idx] = rank + 1
                
                ranks[start_idx:end_idx] = group_ranks
                
                # éªŒè¯æ’åçš„å”¯ä¸€æ€§å’Œè¿ç»­æ€§
                unique_ranks = set(group_ranks)
                expected_ranks = set(range(1, group_size + 1))
                if unique_ranks != expected_ranks:
                    # å¦‚æœä»æœ‰é—®é¢˜ï¼Œå¼ºåˆ¶ä¿®å¤
                    self._log(f"è­¦å‘Šï¼šç»„{group_idx}æ’åä¸å”¯ä¸€ï¼Œå¼ºåˆ¶ä¿®å¤")
                    ranks[start_idx:end_idx] = np.arange(1, group_size + 1)
            
            start_idx = end_idx
        
        return ranks
    
    def _validate_rankings(self, ranks: np.ndarray, group_sizes: List[int], 
                          context: str = "") -> bool:
        """
        éªŒè¯æ’åçš„æœ‰æ•ˆæ€§
        
        Args:
            ranks: æ’åæ•°ç»„
            group_sizes: ç»„å¤§å°åˆ—è¡¨
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ç”¨äºæ—¥å¿—
            
        Returns:
            bool: æ’åæ˜¯å¦æœ‰æ•ˆ
        """
        start_idx = 0
        valid = True
        
        for group_idx, group_size in enumerate(group_sizes):
            end_idx = start_idx + group_size
            group_ranks = ranks[start_idx:end_idx]
            
            # æ£€æŸ¥æ’åæ˜¯å¦å”¯ä¸€ä¸”è¿ç»­
            unique_ranks = set(group_ranks)
            expected_ranks = set(range(1, group_size + 1))
            
            if unique_ranks != expected_ranks:
                self._log(f"æ’åéªŒè¯å¤±è´¥ - {context} ç»„{group_idx}: "
                         f"æœŸæœ›{sorted(expected_ranks)}, å®é™…{sorted(unique_ranks)}")
                valid = False
            
            start_idx = end_idx
        
        if valid:
            self._log(f"æ’åéªŒè¯é€šè¿‡ - {context}")
        
        return valid
    
    def _force_fix_rankings(self, ranks: np.ndarray, group_sizes: List[int]) -> np.ndarray:
        """
        å¼ºåˆ¶ä¿®å¤æ’åé—®é¢˜
        
        Args:
            ranks: åŸå§‹æ’å
            group_sizes: ç»„å¤§å°åˆ—è¡¨
            
        Returns:
            np.ndarray: ä¿®å¤åçš„æ’å
        """
        fixed_ranks = ranks.copy()
        start_idx = 0
        
        for group_idx, group_size in enumerate(group_sizes):
            end_idx = start_idx + group_size
            group_ranks = fixed_ranks[start_idx:end_idx]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤
            unique_ranks = set(group_ranks)
            expected_ranks = set(range(1, group_size + 1))
            
            if unique_ranks != expected_ranks:
                # å¼ºåˆ¶åˆ†é…è¿ç»­æ’å
                # ä½¿ç”¨ç»„IDä½œä¸ºéšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
                np.random.seed(group_idx * 54321)
                new_ranks = np.random.permutation(range(1, group_size + 1))
                fixed_ranks[start_idx:end_idx] = new_ranks
                
                self._log(f"å¼ºåˆ¶ä¿®å¤ç»„{group_idx}çš„æ’å")
            
            start_idx = end_idx
        
        return fixed_ranks
        
    def train_models(self, X_train: np.ndarray, X_test: np.ndarray, 
                    y_train: np.ndarray, y_test: np.ndarray,
                    train_group_sizes: List[int], test_group_sizes: List[int],
                    feature_names: List[str], test_info: pd.DataFrame,
                    segment_name: str = "", segment_id: int = None) -> pd.DataFrame:
        """
        è®­ç»ƒæ‰€æœ‰é€‰æ‹©çš„æ¨¡å‹ï¼ˆä¿®å¤æ’åé—®é¢˜ç‰ˆæœ¬ï¼‰
        
        Args:
            X_train, X_test: è®­ç»ƒå’Œæµ‹è¯•ç‰¹å¾
            y_train, y_test: è®­ç»ƒå’Œæµ‹è¯•æ ‡ç­¾
            train_group_sizes, test_group_sizes: ç»„å¤§å°ä¿¡æ¯
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            test_info: æµ‹è¯•é›†çš„ranker_idå’Œselectedä¿¡æ¯
            segment_name: æ•°æ®æ®µåç§°
            segment_id: æ•°æ®æ®µIDï¼ˆç”¨äºä¿å­˜æ¨¡å‹ï¼‰
            
        Returns:
            pd.DataFrame: æ¨¡å‹æ€§èƒ½æ¯”è¾ƒç»“æœ
        """
        self._log(f"\nå¼€å§‹è®­ç»ƒæ¨¡å‹ - {segment_name}")
        
        # å¦‚æœå¯ç”¨è‡ªåŠ¨è°ƒå‚ï¼Œå…ˆè¿›è¡Œå‚æ•°ä¼˜åŒ–
        best_params_dict = {}
        if self.enable_auto_tuning:
            self._log("ğŸ¯ æ‰§è¡Œè‡ªåŠ¨è°ƒå‚...")
            with create_data_loading_progress("è‡ªåŠ¨è°ƒå‚") as pbar:
                best_params_dict = self._perform_auto_tuning(
                    X_train, y_train, train_group_sizes, 
                    X_test, y_test, test_info, 
                    input_dim=X_train.shape[1]
                )
                pbar.update(1)
        
        model_results = []
        
        # åˆ›å»ºæ¨¡å‹è®­ç»ƒè¿›åº¦æ˜¾ç¤º
        training_progress = ModelTrainingProgress(self.selected_models)
        
        with training_progress.training_session() as trainer:
            for model_name in progress_bar(self.selected_models, desc="è®­ç»ƒæ¨¡å‹"):
                self._log(f"\nğŸ”§ è®­ç»ƒ {model_name}...")
                
                try:
                    # å¼€å§‹å½“å‰æ¨¡å‹è®­ç»ƒ
                    trainer.start_model(model_name, steps=7)  # å¢åŠ éªŒè¯æ­¥éª¤
                    
                    # æ­¥éª¤1: è·å–æ¨¡å‹å‚æ•°
                    trainer.update_model_progress(1, f"è·å– {model_name} å‚æ•°")
                    if model_name in best_params_dict:
                        model_params = best_params_dict[model_name]
                        self._log(f"ğŸ“‹ ä½¿ç”¨è°ƒä¼˜å‚æ•°: {model_params}")
                    else:
                        model_params = Config.DEFAULT_MODEL_PARAMS.get(model_name, {})
                        self._log(f"ğŸ“‹ ä½¿ç”¨é»˜è®¤å‚æ•°")
                    
                    # æ­¥éª¤2: åˆ›å»ºæ¨¡å‹
                    trainer.update_model_progress(1, f"åˆ›å»º {model_name} æ¨¡å‹")
                    input_dim = X_train.shape[1] if model_name == 'NeuralRanker' else None
                    model = ModelFactory.create_model(
                        model_name=model_name,
                        use_gpu=self.use_gpu,
                        input_dim=input_dim,
                        **model_params
                    )
                    
                    # æ­¥éª¤3: è®­ç»ƒæ¨¡å‹
                    trainer.update_model_progress(1, f"è®­ç»ƒ {model_name} æ¨¡å‹")
                    if model_name == 'NeuralRanker':
                        epochs = model_params.get('epochs', 10)
                        batch_size = model_params.get('batch_size', 32)
                        model.fit(X_train, y_train, group=train_group_sizes, 
                                 epochs=epochs, batch_size=batch_size)
                    else:
                        model.fit(X_train, y_train, group=train_group_sizes)
                    
                    # æ­¥éª¤4: é¢„æµ‹
                    trainer.update_model_progress(1, f"æ‰§è¡Œ {model_name} é¢„æµ‹")
                    y_pred_scores = model.predict(X_test)
                    
                    # æ­¥éª¤5: è®¡ç®—æ’åï¼ˆå…³é”®ä¿®å¤ï¼‰
                    trainer.update_model_progress(1, f"è®¡ç®— {model_name} æ’å")
                    y_pred_ranks = self._calculate_group_ranks(y_pred_scores, test_group_sizes)
                    
                    # æ­¥éª¤6: éªŒè¯æ’åï¼ˆæ–°å¢ï¼‰
                    trainer.update_model_progress(1, f"éªŒè¯ {model_name} æ’å")
                    is_valid = self._validate_rankings(
                        y_pred_ranks, test_group_sizes, f"{model_name}-{segment_name}"
                    )
                    
                    if not is_valid:
                        self._log(f"âš ï¸ {model_name} æ’åéªŒè¯å¤±è´¥ï¼Œå¼ºåˆ¶ä¿®å¤...")
                        y_pred_ranks = self._force_fix_rankings(y_pred_ranks, test_group_sizes)
                        # å†æ¬¡éªŒè¯
                        self._validate_rankings(
                            y_pred_ranks, test_group_sizes, f"{model_name}-{segment_name}-ä¿®å¤å"
                        )
                    
                    # è®¡ç®—æ€§èƒ½
                    hitrate_3 = self.calculate_hitrate_at_k(test_info, y_pred_ranks, k=3)
                    
                    # æ­¥éª¤7: ä¿å­˜æ¨¡å‹å’Œç‰¹å¾
                    if self.save_models and segment_id is not None:
                        trainer.update_model_progress(1, f"ä¿å­˜ {model_name} æ¨¡å‹")
                        try:
                            self.predictor.save_model_and_features(
                                model=model.model if hasattr(model, 'model') else model,
                                model_name=model_name,
                                segment_id=segment_id,
                                feature_names=feature_names,
                                performance=hitrate_3
                            )
                            self._log(f"âœ… å·²ä¿å­˜æ¨¡å‹: {model_name}_segment_{segment_id}")
                        except Exception as e:
                            self._log(f"âš ï¸ ä¿å­˜æ¨¡å‹å¤±è´¥: {str(e)}")
                    else:
                        trainer.update_model_progress(1, f"è·³è¿‡ä¿å­˜ {model_name}")
                    
                    # å­˜å‚¨ç»“æœ
                    model_results.append({
                        'Model': model_name,
                        'HitRate@3': hitrate_3,
                        'Segment': segment_name
                    })
                    
                    # å­˜å‚¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆå†…å­˜ä¸­ï¼‰
                    model_key = f"{segment_name}_{model_name}" if segment_name else model_name
                    self.trained_models[model_key] = {
                        'model': model,
                        'params': model_params,
                        'performance': hitrate_3,
                        'feature_names': feature_names
                    }
                    
                    # è·å–ç‰¹å¾é‡è¦æ€§
                    self._extract_feature_importance(model, model_name, X_test, feature_names)
                    
                    # è®¡ç®—SHAPå€¼
                    self._compute_shap_values(model, model_name, X_test, feature_names)
                    
                    # å®Œæˆå½“å‰æ¨¡å‹
                    trainer.finish_model(model_name, hitrate_3)
                    self._log(f"âœ… {model_name} è®­ç»ƒå®Œæˆ, HitRate@3: {hitrate_3:.4f}")
                    
                except Exception as e:
                    self._log(f"âŒ è®­ç»ƒ {model_name} æ—¶å‡ºé”™: {str(e)}")
                    trainer.finish_model(model_name, 0.0)
                    continue
        
        # æ˜¾ç¤ºæ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
        results_df = pd.DataFrame(model_results)
        if not results_df.empty:
            self._log("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:")
            print(results_df.to_string(index=False))
            
            # æ˜¾ç¤ºå®Œæˆæ€»ç»“
            summary = {
                "è®­ç»ƒæ¨¡å‹æ•°": len(results_df),
                "æœ€ä½³æ¨¡å‹": results_df.loc[results_df['HitRate@3'].idxmax(), 'Model'],
                "æœ€ä½³æ€§èƒ½": results_df['HitRate@3'].max(),
                "å¹³å‡æ€§èƒ½": results_df['HitRate@3'].mean()
            }
            show_completion_summary(summary, f"{segment_name} è®­ç»ƒå®Œæˆ")
        
        return results_df
    
    def _perform_auto_tuning(self, X_train: np.ndarray, y_train: np.ndarray, 
                           train_group_sizes: List[int], X_val: np.ndarray, 
                           y_val: np.ndarray, val_info: pd.DataFrame,
                           input_dim: int) -> Dict[str, Dict[str, Any]]:
        """æ‰§è¡Œè‡ªåŠ¨è°ƒå‚"""
        self._log("\nå¼€å§‹è‡ªåŠ¨è°ƒå‚...")
        
        # åˆå§‹åŒ–è‡ªåŠ¨è°ƒå‚å™¨
        if self.auto_tuner is None:
            self.auto_tuner = create_auto_tuner(
                model_factory=ModelFactory.create_model,
                hitrate_calculator=self.calculate_hitrate_at_k,
                n_trials=self.auto_tuning_trials,
                timeout=Config.AUTO_TUNING_TIMEOUT
            )
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹è¿›è¡Œè°ƒå‚
        best_params = {}
        for model_name in self.selected_models:
            if model_name == 'BM25Ranker':
                continue  # BM25ä¸éœ€è¦è°ƒå‚
            
            try:
                result = self.auto_tuner.optimize(
                    model_name=model_name,
                    X_train=X_train,
                    y_train=y_train,
                    train_groups=train_group_sizes,
                    X_val=X_val,
                    y_val=y_val,
                    val_info=val_info,
                    use_gpu=self.use_gpu,
                    input_dim=input_dim
                )
                best_params[model_name] = result['best_params']
                self._log(f"{model_name} è°ƒå‚å®Œæˆ, æœ€ä½³HitRate@3: {result['best_score']:.4f}")
            except Exception as e:
                self._log(f"{model_name} è°ƒå‚å¤±è´¥: {str(e)}")
        
        return best_params
    
    def _extract_feature_importance(self, model: BaseRanker, model_name: str, 
                                  X_test: np.ndarray, feature_names: List[str]):
        """æå–ç‰¹å¾é‡è¦æ€§"""
        try:
            if hasattr(model, 'feature_importances_'):
                importance = model.feature_importances_
                self.feature_importance_results[model_name] = {
                    'importance': importance,
                    'feature_names': feature_names
                }
            elif model_name == 'NeuralRanker':
                # å¯¹äºç¥ç»ç½‘ç»œï¼Œä½¿ç”¨æ¢¯åº¦ä½œä¸ºç‰¹å¾é‡è¦æ€§
                importance = self._compute_neural_importance(model, X_test)
                self.feature_importance_results[model_name] = {
                    'importance': importance,
                    'feature_names': feature_names
                }
        except Exception as e:
            self._log(f"æå– {model_name} ç‰¹å¾é‡è¦æ€§å¤±è´¥: {str(e)}")
    
    def _compute_neural_importance(self, model: BaseRanker, X: np.ndarray, 
                                 samples: int = 1000) -> np.ndarray:
        """è®¡ç®—ç¥ç»ç½‘ç»œçš„ç‰¹å¾é‡è¦æ€§"""
        if samples < len(X):
            X_sample = X[np.random.choice(len(X), samples, replace=False)]
        else:
            X_sample = X
        
        X_tensor = tf.convert_to_tensor(X_sample, dtype=tf.float32)
        
        with tf.GradientTape() as tape:
            tape.watch(X_tensor)
            predictions = model.model(X_tensor)
        
        grads = tape.gradient(predictions, X_tensor)
        importance = np.mean(np.abs(grads.numpy()), axis=0)
        
        return importance
    
    def _compute_shap_values(self, model: BaseRanker, model_name: str, 
                           X_test: np.ndarray, feature_names: List[str]):
        """è®¡ç®—SHAPå€¼"""
        if len(X_test) > Config.MAX_SHAP_SAMPLES or model_name == 'BM25Ranker':
            return
        
        try:
            self._log(f"è®¡ç®— {model_name} çš„SHAPå€¼...")
            sample_idx = np.random.choice(
                len(X_test), 
                min(Config.MAX_SHAP_SAMPLES, len(X_test)), 
                replace=False
            )
            X_sample = X_test[sample_idx]
            
            if model_name in ['XGBRanker', 'LGBMRanker', 'LambdaMART', 'ListNet']:
                explainer = shap.TreeExplainer(model.model)
                shap_values = explainer.shap_values(X_sample)
            elif model_name == 'NeuralRanker':
                explainer = shap.DeepExplainer(model.model, X_sample)
                shap_values = explainer.shap_values(X_sample)[0]
            else:
                return
            
            self.shap_values[model_name] = {
                'values': shap_values,
                'data': X_sample,
                'feature_names': feature_names
            }
            
            # å¯è§†åŒ–SHAPæ‘˜è¦å›¾
            plt.figure(figsize=(10, 8))
            shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)
            plt.title(f'{model_name} SHAP Feature Importance')
            plt.tight_layout()
            plt.show()
            
        except Exception as e:
            self._log(f"è®¡ç®— {model_name} SHAPå€¼å¤±è´¥: {str(e)}")
    
    def predict_with_saved_models(self, segments: List[int] = None, 
                                 model_names: List[str] = None,
                                 ensemble_method: str = 'average') -> Optional[pd.DataFrame]:
        """
        ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ˆä¿®å¤æ’åé—®é¢˜ç‰ˆæœ¬ï¼‰
        
        Args:
            segments: è¦é¢„æµ‹çš„æ•°æ®æ®µåˆ—è¡¨
            model_names: è¦ä½¿ç”¨çš„æ¨¡å‹åç§°åˆ—è¡¨
            ensemble_method: é›†æˆæ–¹æ³•
            
        Returns:
            Optional[pd.DataFrame]: é¢„æµ‹ç»“æœ
        """
        self._log("ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹...")
        
        # æ£€æŸ¥å¯ç”¨æ¨¡å‹
        available_models = self.predictor.get_available_models()
        self._log(f"å¯ç”¨æ¨¡å‹: {available_models}")
        
        if not available_models:
            self._log("æ²¡æœ‰æ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
        
        # ä½¿ç”¨é¢„æµ‹å™¨è¿›è¡Œé¢„æµ‹
        result = self.predictor.predict_all(
            segments=segments,
            model_names=model_names or list(available_models.keys()),
            ensemble_method=ensemble_method
        )
        
        # éªŒè¯æœ€ç»ˆç»“æœçš„æ’åå”¯ä¸€æ€§
        if result is not None:
            self._log("éªŒè¯æœ€ç»ˆé¢„æµ‹ç»“æœçš„æ’åå”¯ä¸€æ€§...")
            fixed_result = self.predictor._validate_and_fix_rankings(result)
            return fixed_result
        
        return result
    
    def analyze_feature_importance(self) -> Optional[pd.Series]:
        """
        åˆ†æç‰¹å¾é‡è¦æ€§
        
        Returns:
            Optional[pd.Series]: å¹³å‡ç‰¹å¾é‡è¦æ€§ï¼ˆæŒ‰é‡è¦æ€§é™åºæ’åˆ—ï¼‰
        """
        if not self.feature_importance_results:
            self._log("æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾é‡è¦æ€§æ•°æ®")
            return None
        
        self._log("\nåˆ†æç‰¹å¾é‡è¦æ€§...")
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
        importance_dfs = []
        
        for model_name, result in self.feature_importance_results.items():
            importance = result['importance']
            feature_names = result['feature_names']
            
            importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importance,
                'Model': model_name
            })
            importance_dfs.append(importance_df)
        
        all_importance = pd.concat(importance_dfs)
        
        # è®¡ç®—å¹³å‡é‡è¦æ€§
        avg_importance = all_importance.groupby('Feature')['Importance'].mean().sort_values(ascending=False)
        
        # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(12, 10))
        top_30 = all_importance.nlargest(30, 'Importance')
        sns.barplot(data=top_30, x='Importance', y='Feature', hue='Model')
        plt.title('Top 30 Feature Importance Across Models')
        plt.tight_layout()
        plt.show()
        
        # å¯è§†åŒ–å¹³å‡ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(12, 10))
        avg_importance.head(30).sort_values().plot(kind='barh')
        plt.title('Top 30 Average Feature Importance Across Models')
        plt.tight_layout()
        plt.show()
        
        return avg_importance
    
    def full_analysis(self, file_path: str, use_sampling: bool = True,
                     num_groups: int = 2000, min_group_size: int = 20) -> Dict[str, Any]:
        """
        å®Œæ•´çš„åˆ†ææµç¨‹ï¼ˆä¿®å¤æ’åé—®é¢˜ç‰ˆæœ¬ï¼‰
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            use_sampling: æ˜¯å¦ä½¿ç”¨æŠ½æ ·
            num_groups: æŠ½æ ·ç»„æ•°
            min_group_size: æœ€å°ç»„å¤§å°
            
        Returns:
            Dict: åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸
        """
        # 1. åŠ è½½æ•°æ®
        df = self.data_processor.load_data(
            file_path, 
            use_sampling=use_sampling,
            num_groups=num_groups, 
            min_group_size=min_group_size
        )
        
        # 2. å‡†å¤‡æ’åºæ•°æ®
        (X_train, X_test, y_train, y_test, 
         train_group_sizes, test_group_sizes, 
         feature_cols, test_info) = self.data_processor.prepare_ranking_data(df)
        
        # 3. ç”Ÿæˆä¸€è‡´çš„segmentåç§°å’ŒID
        file_basename = os.path.basename(file_path)
        segment_id = None
        
        # ä»æ–‡ä»¶åä¸­æå–segment ID
        import re
        match = re.search(r'segment[_\s]*(\d+)', file_basename)
        if match:
            segment_id = int(match.group(1))
        
        # ç§»é™¤æ–‡ä»¶æ‰©å±•åï¼Œç»Ÿä¸€æ ¼å¼
        if file_basename.endswith('.parquet'):
            segment_name = file_basename[:-8]  # ç§»é™¤ .parquet
        else:
            segment_name = file_basename
        
        # ç¡®ä¿segmentåç§°æ ¼å¼ä¸€è‡´
        if not segment_name.startswith('train_segment_'):
            if segment_id is not None:
                segment_name = f'train_segment_{segment_id}'
            else:
                segment_name = f'train_segment_unknown'
        
        self._log(f"ä½¿ç”¨segmentåç§°: {segment_name}, ID: {segment_id}")
        
        # 4. è®­ç»ƒæ’åºæ¨¡å‹ï¼ˆä¿®å¤æ’åé—®é¢˜ç‰ˆæœ¬ï¼‰
        model_results = self.train_models(
            X_train, X_test, y_train, y_test,
            train_group_sizes, test_group_sizes,
            feature_cols, test_info, segment_name, segment_id
        )
        
        # 5. åˆ†æç‰¹å¾é‡è¦æ€§
        avg_importance = self.analyze_feature_importance()
        
        # 6. æ¸…ç†å†…å­˜
        gc.collect()
        
        return {
            'model_results': model_results,
            'feature_importance': avg_importance,
            'trained_models': self.trained_models,
            'segment_name': segment_name,
            'segment_id': segment_id
        }
    
    # ä¿ç•™åŸæœ‰çš„åˆå¹¶æ–¹æ³•ä»¥å…¼å®¹æ—§ä»£ç 
    def merge_all_predictions(self, prediction_files: List[str], 
                            submission_file: str, 
                            output_file: str) -> str:
        """
        åˆå¹¶æ‰€æœ‰é¢„æµ‹æ–‡ä»¶ï¼ˆå…¼å®¹æ–¹æ³•ï¼‰
        
        Args:
            prediction_files: é¢„æµ‹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            submission_file: submissionæ¨¡æ¿æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        return self.prediction_merger.merge_predictions(
            prediction_files=prediction_files,
            submission_file=submission_file,
            output_file=output_file,
            ensemble_method='average'
        )
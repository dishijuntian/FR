"""
æ”¹è¿›çš„èˆªç­æ’åºé¢„æµ‹å™¨ - ä¿®å¤å†…å­˜ç“¶é¢ˆç‰ˆæœ¬

åŸºäºæ›´ä¼˜ç§€çš„é¢„æµ‹ä»£ç ç»“æ„é‡æ–°è®¾è®¡ï¼Œæä¾›ï¼š
- ä¿®å¤äº†å¤§æ•°æ®é›†å†…å­˜ç“¶é¢ˆé—®é¢˜
- ä¼˜åŒ–äº†DataFrameåˆ›å»ºè¿‡ç¨‹
- æ·»åŠ äº†å†…å­˜ä½¿ç”¨ç›‘æ§
- æ”¹è¿›äº†åˆ†æ‰¹å¤„ç†æœºåˆ¶

ä½œè€…: Flight Ranking Team
ç‰ˆæœ¬: 3.2 (ä¿®å¤å†…å­˜ç“¶é¢ˆ)
"""

import pandas as pd
import numpy as np
import joblib
import os
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
import warnings
import gc
import psutil

# å¯¼å…¥åŸæœ‰æ¨¡å—
try:
    from .config import Config
    from .models import ModelFactory
    from .data_processor import DataProcessor
    from .progress_utils import progress_bar, create_data_loading_progress
except ImportError:
    from config import Config
    from models import ModelFactory
    from data_processor import DataProcessor
    from progress_utils import progress_bar, create_data_loading_progress

warnings.filterwarnings('ignore')


class FlightRankingPredictor:
    """æ”¹è¿›çš„èˆªç­æ’åºé¢„æµ‹å™¨ - ä¿®å¤å†…å­˜ç“¶é¢ˆç‰ˆæœ¬"""
    
    def __init__(self, 
                 data_path: str = None,
                 model_save_path: str = "models", 
                 output_path: str = "submissions",
                 use_gpu: bool = False, 
                 random_state: int = 42,
                 logger=None):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            data_path: æ•°æ®æ ¹ç›®å½•è·¯å¾„
            model_save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„
            use_gpu: æ˜¯å¦ä½¿ç”¨GPU
            random_state: éšæœºç§å­
            logger: æ—¥å¿—è®°å½•å™¨
        """
        # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„è·¯å¾„æˆ–ç”¨æˆ·æŒ‡å®šè·¯å¾„
        self.data_path = Path(data_path) if data_path else Path(Config.DATA_BASE_PATH)
        self.model_save_path = self.data_path / model_save_path
        self.output_path = self.data_path / output_path
        self.use_gpu = use_gpu
        self.random_state = random_state
        self.logger = logger
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_path.mkdir(parents=True, exist_ok=True)
        self.model_save_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        self.data_processor = DataProcessor(logger=logger)
        
        # ç¼“å­˜å·²åŠ è½½çš„æ¨¡å‹å’Œç‰¹å¾
        self.loaded_models = {}
        self.loaded_features = {}
        
        self._log(f"é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        self._log(f"æ•°æ®è·¯å¾„: {self.data_path}")
        self._log(f"æ¨¡å‹è·¯å¾„: {self.model_save_path}")
        self._log(f"è¾“å‡ºè·¯å¾„: {self.output_path}")
    
    def _log(self, message: str):
        """è®°å½•æ—¥å¿—"""
        if self.logger:
            self.logger.info(message)
        else:
            print(message)
    
    def _monitor_memory(self, stage: str = ""):
        """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            memory_info = psutil.Process().memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
            self._log(f"ğŸ“Š {stage} å†…å­˜ä½¿ç”¨: {memory_mb:.1f} MB")
            return memory_mb
        except:
            return 0
    
    def _optimize_memory(self):
        """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
        gc.collect()
        self._monitor_memory("å†…å­˜æ¸…ç†å")
    
    def save_model_and_features(self, model, model_name: str, segment_id: int, 
                               feature_names: List[str], performance: float = None):
        """
        ä¿å­˜æ¨¡å‹å’Œç‰¹å¾ä¿¡æ¯
        
        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            model_name: æ¨¡å‹åç§°
            segment_id: æ•°æ®æ®µID
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            performance: æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
        """
        try:
            # ä¿å­˜æ¨¡å‹
            model_path = self.model_save_path / f"{model_name}_segment_{segment_id}.pkl"
            joblib.dump(model, model_path)
            
            # ä¿å­˜ç‰¹å¾åç§°
            feature_path = self.model_save_path / f"features_segment_{segment_id}.pkl"
            joblib.dump(feature_names, feature_path)
            
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯
            info_path = self.model_save_path / f"info_{model_name}_segment_{segment_id}.json"
            import json
            model_info = {
                'model_name': model_name,
                'segment_id': segment_id,
                'feature_count': len(feature_names),
                'performance': performance,
                'saved_at': pd.Timestamp.now().isoformat()
            }
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(model_info, f, indent=2, ensure_ascii=False)
            
            self._log(f"å·²ä¿å­˜æ¨¡å‹: {model_path}")
            self._log(f"å·²ä¿å­˜ç‰¹å¾: {feature_path}")
            
        except Exception as e:
            self._log(f"ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            raise
    
    def load_model_and_features(self, model_name: str, segment_id: int) -> Tuple[Any, List[str]]:
        """
        åŠ è½½æ¨¡å‹å’Œç‰¹å¾ä¿¡æ¯
        
        Args:
            model_name: æ¨¡å‹åç§°
            segment_id: æ•°æ®æ®µID
            
        Returns:
            Tuple: (æ¨¡å‹å¯¹è±¡, ç‰¹å¾åç§°åˆ—è¡¨)
        """
        cache_key = f"{model_name}_segment_{segment_id}"
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.loaded_models:
            return self.loaded_models[cache_key], self.loaded_features[cache_key]
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_path = self.model_save_path / f"{model_name}_segment_{segment_id}.pkl"
        feature_path = self.model_save_path / f"features_segment_{segment_id}.pkl"
        
        if not model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        if not feature_path.exists():
            raise FileNotFoundError(f"ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {feature_path}")
        
        try:
            # åŠ è½½æ¨¡å‹å’Œç‰¹å¾
            model = joblib.load(model_path)
            feature_names = joblib.load(feature_path)
            
            # ç¼“å­˜åŠ è½½çš„å†…å®¹
            self.loaded_models[cache_key] = model
            self.loaded_features[cache_key] = feature_names
            
            self._log(f"å·²åŠ è½½æ¨¡å‹: {model_path}")
            return model, feature_names
            
        except Exception as e:
            self._log(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            raise
    
    def predict_segment(self, segment_id: int, model_name: str = 'XGBRanker') -> Optional[pd.DataFrame]:
        """
        é¢„æµ‹å•ä¸ªæ•°æ®æ®µï¼ˆä¿®å¤å†…å­˜ç“¶é¢ˆç‰ˆæœ¬ï¼‰
        
        Args:
            segment_id: æ•°æ®æ®µID
            model_name: æ¨¡å‹åç§°
            
        Returns:
            Optional[pd.DataFrame]: é¢„æµ‹ç»“æœ
        """
        self._log(f"å¼€å§‹é¢„æµ‹ segment_{segment_id}")
        self._monitor_memory("é¢„æµ‹å¼€å§‹å‰")
        
        try:
            # åŠ è½½æ¨¡å‹å’Œç‰¹å¾
            self._log(f"æ­£åœ¨åŠ è½½æ¨¡å‹å’Œç‰¹å¾...")
            model, feature_names = self.load_model_and_features(model_name, segment_id)
            self._monitor_memory("æ¨¡å‹åŠ è½½å")
            
            # æŸ¥æ‰¾æµ‹è¯•æ•°æ®æ–‡ä»¶
            self._log(f"æ­£åœ¨æŸ¥æ‰¾æµ‹è¯•æ•°æ®æ–‡ä»¶...")
            possible_test_files = [
                self.data_path / "segmented" / "test" / f"test_segment_{segment_id}.parquet",
                self.data_path / "encode" / "test" / f"test_segment_{segment_id}_encoded.parquet",
                self.data_path / "test" / f"test_segment_{segment_id}.parquet"
            ]
            
            test_file = None
            for file_path in possible_test_files:
                if file_path.exists():
                    test_file = file_path
                    break
            
            if test_file is None:
                raise FileNotFoundError(f"æ‰¾ä¸åˆ° segment_{segment_id} çš„æµ‹è¯•æ–‡ä»¶")
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            self._log(f"æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
            test_df = pd.read_parquet(test_file)
            self._log(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_df.shape}")
            self._monitor_memory("æµ‹è¯•æ•°æ®åŠ è½½å")
            
            # æ£€æŸ¥æ•°æ®å¤§å°ï¼Œå†³å®šå¤„ç†ç­–ç•¥
            data_size = len(test_df)
            if data_size > 1000000:  # è¶…è¿‡100ä¸‡è¡Œ
                self._log(f"âš¡ æ£€æµ‹åˆ°å¤§æ•°æ®é›†({data_size:,}è¡Œ)ï¼Œå¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼")
                return self._predict_segment_optimized(test_df, model, feature_names, segment_id)
            else:
                self._log(f"ğŸ“Š æ•°æ®é›†å¤§å°é€‚ä¸­({data_size:,}è¡Œ)ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼")
                return self._predict_segment_standard(test_df, model, feature_names, segment_id)
                
        except Exception as e:
            self._log(f"é¢„æµ‹ segment_{segment_id} å¤±è´¥: {str(e)}")
            return None
        finally:
            # æ¸…ç†å†…å­˜
            self._optimize_memory()
    
    def _predict_segment_standard(self, test_df: pd.DataFrame, model, 
                                feature_names: List[str], segment_id: int) -> pd.DataFrame:
        """æ ‡å‡†æ¨¡å¼é¢„æµ‹ï¼ˆé€‚ç”¨äºä¸­å°æ•°æ®é›†ï¼‰"""
        self._log(f"ğŸ”§ æ‰§è¡Œæ ‡å‡†æ¨¡å¼é¢„æµ‹...")
        
        # å‡†å¤‡ç‰¹å¾æ•°æ®
        self._log(f"å‡†å¤‡ç‰¹å¾æ•°æ®...")
        X_test, group_sizes = self._prepare_test_features(test_df, feature_names)
        self._monitor_memory("ç‰¹å¾å‡†å¤‡å")
        
        # æ‰§è¡Œé¢„æµ‹
        self._log(f"æ‰§è¡Œæ¨¡å‹é¢„æµ‹...")
        pred_scores = model.predict(X_test)
        self._monitor_memory("æ¨¡å‹é¢„æµ‹å")
        
        # è®¡ç®—æ’å
        self._log(f"è®¡ç®—ç»„å†…æ’å...")
        pred_ranks = self._calculate_group_ranks_robust(pred_scores, group_sizes, segment_id)
        self._monitor_memory("æ’åè®¡ç®—å")
        
        # éªŒè¯æ’åå”¯ä¸€æ€§
        self._log(f"éªŒè¯æ’åå”¯ä¸€æ€§...")
        is_valid = self._validate_ranking_uniqueness(
            pred_ranks, group_sizes, f"segment_{segment_id}-{model.__class__.__name__}"
        )
        
        if not is_valid:
            self._log(f"âš ï¸ segment_{segment_id} æ’åéªŒè¯å¤±è´¥ï¼Œå¼ºåˆ¶ä¿®å¤...")
            pred_ranks = self._force_fix_ranking_uniqueness(pred_ranks, group_sizes, segment_id)
        
        self._log(f"âœ… æ’åéªŒè¯é€šè¿‡")
        
        # ===== å…³é”®ä¿®å¤ï¼šä¼˜åŒ–DataFrameåˆ›å»º =====
        self._log(f"ğŸ’¾ å¼€å§‹åˆ›å»ºæäº¤ç»“æœ... (æ•°æ®é‡: {len(test_df):,} è¡Œ)")
        self._monitor_memory("DataFrameåˆ›å»ºå‰")
        
        # ä½¿ç”¨å†…å­˜ä¼˜åŒ–çš„æ–¹å¼åˆ›å»ºDataFrame
        submission = self._create_submission_optimized(test_df, pred_ranks)
        
        self._monitor_memory("DataFrameåˆ›å»ºå")
        self._log(f"âœ… æäº¤ç»“æœåˆ›å»ºå®Œæˆï¼Œå½¢çŠ¶: {submission.shape}")
        
        # æœ€ç»ˆéªŒè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self._log(f"ğŸ” æ‰§è¡Œæœ€ç»ˆéªŒè¯...")
        final_validation = self._final_ranking_validation_optimized(submission)
        
        if not final_validation:
            self._log("âŒ æœ€ç»ˆéªŒè¯å¤±è´¥ï¼Œæ‰§è¡Œä¿®å¤...")
            submission = self._validate_and_fix_rankings(submission)
            self._log("âœ… æœ€ç»ˆä¿®å¤å®Œæˆ")
        
        self._log(f"ğŸ‰ é¢„æµ‹å®Œæˆï¼Œç»“æœå½¢çŠ¶: {submission.shape}")
        return submission
    
    def _predict_segment_optimized(self, test_df: pd.DataFrame, model, 
                                 feature_names: List[str], segment_id: int) -> pd.DataFrame:
        """ä¼˜åŒ–æ¨¡å¼é¢„æµ‹ï¼ˆé€‚ç”¨äºå¤§æ•°æ®é›†ï¼‰"""
        self._log(f"âš¡ æ‰§è¡Œå¤§æ•°æ®é›†ä¼˜åŒ–æ¨¡å¼é¢„æµ‹...")
        
        chunk_size = 500000  # æ¯æ‰¹50ä¸‡è¡Œ
        total_chunks = (len(test_df) + chunk_size - 1) // chunk_size
        self._log(f"ğŸ“¦ å°†æ•°æ®åˆ†ä¸º{total_chunks}æ‰¹å¤„ç†ï¼Œæ¯æ‰¹{chunk_size:,}è¡Œ")
        
        all_results = []
        
        for chunk_idx in range(0, len(test_df), chunk_size):
            chunk_num = chunk_idx // chunk_size + 1
            self._log(f"ğŸ”„ å¤„ç†ç¬¬{chunk_num}/{total_chunks}æ‰¹...")
            
            # è·å–å½“å‰æ‰¹æ¬¡æ•°æ®
            end_idx = min(chunk_idx + chunk_size, len(test_df))
            chunk_df = test_df.iloc[chunk_idx:end_idx].copy()
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            X_chunk, group_sizes = self._prepare_test_features(chunk_df, feature_names)
            pred_scores = model.predict(X_chunk)
            pred_ranks = self._calculate_group_ranks_robust(pred_scores, group_sizes, segment_id)
            
            # éªŒè¯å½“å‰æ‰¹æ¬¡æ’å
            is_valid = self._validate_ranking_uniqueness(
                pred_ranks, group_sizes, f"segment_{segment_id}-chunk_{chunk_num}"
            )
            if not is_valid:
                pred_ranks = self._force_fix_ranking_uniqueness(pred_ranks, group_sizes, segment_id)
            
            # åˆ›å»ºå½“å‰æ‰¹æ¬¡ç»“æœï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
            chunk_result = self._create_submission_optimized(chunk_df, pred_ranks)
            all_results.append(chunk_result)
            
            self._log(f"âœ… ç¬¬{chunk_num}æ‰¹å®Œæˆï¼Œå½¢çŠ¶: {chunk_result.shape}")
            
            # æ¸…ç†å½“å‰æ‰¹æ¬¡çš„ä¸­é—´å˜é‡
            del chunk_df, X_chunk, pred_scores, pred_ranks, chunk_result
            self._optimize_memory()
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
        self._log(f"ğŸ”— åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ...")
        self._monitor_memory("åˆå¹¶å‰")
        
        final_result = pd.concat(all_results, ignore_index=True)
        
        # æ¸…ç†æ‰¹æ¬¡ç»“æœ
        del all_results
        self._optimize_memory()
        
        self._monitor_memory("åˆå¹¶å")
        self._log(f"âœ… å¤§æ•°æ®é›†é¢„æµ‹å®Œæˆï¼Œæœ€ç»ˆç»“æœå½¢çŠ¶: {final_result.shape}")
        
        # æœ€ç»ˆéªŒè¯ï¼ˆæŠ½æ ·éªŒè¯ï¼‰
        final_validation = self._final_ranking_validation_optimized(final_result, sample_ratio=0.1)
        if not final_validation:
            self._log("âš ï¸ æœ€ç»ˆéªŒè¯å¤±è´¥ï¼Œä½†ç”±äºæ•°æ®é‡è¿‡å¤§ï¼Œè·³è¿‡å…¨é‡ä¿®å¤")
        
        return final_result
    
    def _create_submission_optimized(self, test_df: pd.DataFrame, pred_ranks: np.ndarray) -> pd.DataFrame:
        """å†…å­˜ä¼˜åŒ–çš„DataFrameåˆ›å»ºæ–¹æ³•"""
        self._log(f"ğŸš€ ä½¿ç”¨å†…å­˜ä¼˜åŒ–æ–¹å¼åˆ›å»ºDataFrame...")
        
        # æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨numpyæ•°ç»„åˆ›å»ºï¼ˆæœ€çœå†…å­˜ï¼‰
        try:
            # æå–éœ€è¦çš„åˆ—ä¸ºnumpyæ•°ç»„
            id_values = test_df['Id'].values
            ranker_values = test_df['ranker_id'].values
            
            # åˆ›å»ºå­—å…¸ï¼Œä½¿ç”¨numpyæ•°ç»„
            submission_data = {
                'Id': id_values,
                'ranker_id': ranker_values,
                'selected': pred_ranks
            }
            
            # åˆ›å»ºDataFrame
            submission = pd.DataFrame(submission_data)
            
            # ç«‹å³æ¸…ç†ä¸´æ—¶å˜é‡
            del submission_data, id_values, ranker_values
            
            return submission
            
        except Exception as e:
            self._log(f"âš ï¸ ä¼˜åŒ–æ–¹æ³•å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {e}")
            
            # å¤‡ç”¨æ–¹æ³•ï¼šåˆ†åˆ—åˆ›å»º
            submission = pd.DataFrame()
            submission['Id'] = test_df['Id']
            submission['ranker_id'] = test_df['ranker_id']
            submission['selected'] = pred_ranks
            
            return submission
    
    def _prepare_test_features(self, test_df: pd.DataFrame, 
                              feature_names: List[str]) -> Tuple[np.ndarray, List[int]]:
        """
        å‡†å¤‡æµ‹è¯•ç‰¹å¾æ•°æ®
        
        Args:
            test_df: æµ‹è¯•æ•°æ®
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨
            
        Returns:
            Tuple: (ç‰¹å¾çŸ©é˜µ, ç»„å¤§å°åˆ—è¡¨)
        """
        # ç¡®ä¿æµ‹è¯•æ•°æ®åŒ…å«æ‰€éœ€ç‰¹å¾
        missing_features = set(feature_names) - set(test_df.columns)
        if missing_features:
            self._log(f"è­¦å‘Š: æµ‹è¯•æ•°æ®ç¼ºå°‘ç‰¹å¾: {missing_features}")
            # ä¸ºç¼ºå¤±ç‰¹å¾æ·»åŠ 0å€¼
            for feature in missing_features:
                test_df[feature] = 0.0
        
        # å¤„ç†ç¼ºå¤±å€¼
        test_df[feature_names] = test_df[feature_names].fillna(
            test_df[feature_names].median()
        )
        
        X_test = test_df[feature_names].values.astype(np.float32)
        
        # è®¡ç®—ç»„å¤§å°
        group_sizes = self._calculate_group_sizes(test_df['ranker_id'].values)
        
        return X_test, group_sizes
    
    def _calculate_group_sizes(self, groups: np.ndarray) -> List[int]:
        """è®¡ç®—æ¯ä¸ªç»„çš„å¤§å°"""
        group_sizes = []
        current_group = groups[0]
        current_size = 1
        
        for i in range(1, len(groups)):
            if groups[i] == current_group:
                current_size += 1
            else:
                group_sizes.append(current_size)
                current_group = groups[i]
                current_size = 1
        group_sizes.append(current_size)  # æ·»åŠ æœ€åä¸€ä¸ªç»„
        
        return group_sizes
        
    def _calculate_group_ranks_robust(self, scores: np.ndarray, group_sizes: List[int], 
                                     context_id: int = 0) -> np.ndarray:
        """
        ç¨³å¥çš„æ’åè®¡ç®—å‡½æ•° - ç¡®ä¿æ’åå”¯ä¸€ä¸”è¿ç»­
        
        Args:
            scores: é¢„æµ‹åˆ†æ•°
            group_sizes: æ¯ç»„çš„å¤§å°
            context_id: ä¸Šä¸‹æ–‡IDï¼ˆç”¨äºç”Ÿæˆå”¯ä¸€éšæœºç§å­ï¼‰
            
        Returns:
            np.ndarray: å”¯ä¸€ä¸”è¿ç»­çš„æ’å
        """
        ranks = np.zeros_like(scores, dtype=int)
        start_idx = 0
        
        for group_idx, group_size in enumerate(group_sizes):
            end_idx = start_idx + group_size
            group_scores = scores[start_idx:end_idx]
            
            if group_size == 1:
                # å•ä¸ªå…ƒç´ çš„ç»„ï¼Œæ’åç›´æ¥ä¸º1
                ranks[start_idx:end_idx] = 1
            else:
                # å¤šä¸ªå…ƒç´ çš„ç»„ï¼Œç¡®ä¿æ’åå”¯ä¸€
                # åˆ›å»ºåŸºäºå¤šä¸ªå› å­çš„å”¯ä¸€éšæœºç§å­
                unique_seed = ((context_id * 7919 + group_idx * 2851) % 2147483647)
                np.random.seed(unique_seed)
                
                # æ·»åŠ å¼ºåº¦é€‚ä¸­çš„éšæœºå™ªå£°
                noise_scale = 1e-6  # é€‚ä¸­çš„å™ªå£°å¼ºåº¦
                noise = np.random.random(len(group_scores)) * noise_scale
                
                # ä¸ºæ¯ä¸ªä½ç½®æ·»åŠ ä¸åŒçš„å™ªå£°åç§»
                position_offset = np.arange(len(group_scores)) * 1e-9
                noisy_scores = group_scores + noise + position_offset
                
                # è®¡ç®—æ’åï¼šåˆ†æ•°è¶Šé«˜ï¼Œæ’åè¶Šé å‰ï¼ˆrank=1æœ€å¥½ï¼‰
                sorted_indices = np.argsort(-noisy_scores)  # é™åºæ’åˆ—çš„ç´¢å¼•
                group_ranks = np.zeros(group_size, dtype=int)
                
                # åˆ†é…å”¯ä¸€ä¸”è¿ç»­çš„æ’å
                for rank, idx in enumerate(sorted_indices):
                    group_ranks[idx] = rank + 1
                
                ranks[start_idx:end_idx] = group_ranks
                
                # éªŒè¯å½“å‰ç»„çš„æ’å
                unique_ranks = set(group_ranks)
                expected_ranks = set(range(1, group_size + 1))
                if unique_ranks != expected_ranks:
                    # å¦‚æœä»æœ‰é—®é¢˜ï¼Œä½¿ç”¨å¼ºåˆ¶æ–¹æ³•
                    self._log(f"è­¦å‘Šï¼šç»„{group_idx}æ’åè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å¼ºåˆ¶æ–¹æ³•")
                    ranks[start_idx:end_idx] = self._generate_forced_unique_ranks(
                        group_size, group_idx, context_id
                    )
            
            start_idx = end_idx
        
        return ranks
    
    def _generate_forced_unique_ranks(self, group_size: int, group_idx: int, 
                                     context_id: int) -> np.ndarray:
        """
        å¼ºåˆ¶ç”Ÿæˆå”¯ä¸€æ’å
        
        Args:
            group_size: ç»„å¤§å°
            group_idx: ç»„ç´¢å¼•
            context_id: ä¸Šä¸‹æ–‡ID
            
        Returns:
            np.ndarray: å”¯ä¸€æ’åæ•°ç»„
        """
        # ä½¿ç”¨ç¡®å®šæ€§ä½†ç‹¬ç‰¹çš„æ–¹æ³•ç”Ÿæˆæ’å
        forced_seed = ((context_id * 13 + group_idx * 17) % 1000000)
        np.random.seed(forced_seed)
        
        # ç›´æ¥ç”Ÿæˆ1åˆ°group_sizeçš„éšæœºæ’åˆ—
        unique_ranks = np.random.permutation(range(1, group_size + 1))
        return unique_ranks
    
    def _validate_ranking_uniqueness(self, ranks: np.ndarray, group_sizes: List[int], 
                                   context: str = "") -> bool:
        """
        éªŒè¯æ’åçš„å”¯ä¸€æ€§
        
        Args:
            ranks: æ’åæ•°ç»„
            group_sizes: ç»„å¤§å°åˆ—è¡¨
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            bool: æ’åæ˜¯å¦å”¯ä¸€æœ‰æ•ˆ
        """
        start_idx = 0
        all_valid = True
        
        for group_idx, group_size in enumerate(group_sizes):
            end_idx = start_idx + group_size
            group_ranks = ranks[start_idx:end_idx]
            
            # æ£€æŸ¥æ’åæ˜¯å¦å”¯ä¸€ä¸”è¿ç»­
            unique_ranks = set(group_ranks)
            expected_ranks = set(range(1, group_size + 1))
            
            if unique_ranks != expected_ranks:
                self._log(f"æ’åéªŒè¯å¤±è´¥ - {context} ç»„{group_idx}: "
                         f"æœŸæœ›{sorted(expected_ranks)}, å®é™…{sorted(unique_ranks)}")
                all_valid = False
            
            start_idx = end_idx
        
        if all_valid:
            self._log(f"æ’åéªŒè¯é€šè¿‡ - {context}")
        
        return all_valid
    
    def _force_fix_ranking_uniqueness(self, ranks: np.ndarray, group_sizes: List[int], 
                                     context_id: int = 0) -> np.ndarray:
        """
        å¼ºåˆ¶ä¿®å¤æ’åå”¯ä¸€æ€§é—®é¢˜
        
        Args:
            ranks: åŸå§‹æ’å
            group_sizes: ç»„å¤§å°åˆ—è¡¨
            context_id: ä¸Šä¸‹æ–‡ID
            
        Returns:
            np.ndarray: ä¿®å¤åçš„æ’å
        """
        fixed_ranks = ranks.copy()
        start_idx = 0
        
        for group_idx, group_size in enumerate(group_sizes):
            end_idx = start_idx + group_size
            group_ranks = fixed_ranks[start_idx:end_idx]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤
            unique_ranks = set(group_ranks)
            expected_ranks = set(range(1, group_size + 1))
            
            if unique_ranks != expected_ranks:
                # å¼ºåˆ¶åˆ†é…è¿ç»­æ’å
                # ä½¿ç”¨å¤šé‡å› å­ç”Ÿæˆå”¯ä¸€ç§å­
                fix_seed = ((context_id * 23 + group_idx * 59) % 1000000)
                np.random.seed(fix_seed)
                new_ranks = np.random.permutation(range(1, group_size + 1))
                fixed_ranks[start_idx:end_idx] = new_ranks
                
                self._log(f"å¼ºåˆ¶ä¿®å¤ç»„{group_idx}çš„æ’å")
            
            start_idx = end_idx
        
        return fixed_ranks
    
    def _final_ranking_validation_optimized(self, submission: pd.DataFrame, 
                                          sample_ratio: float = 1.0) -> bool:
        """
        ä¼˜åŒ–ç‰ˆæœ€ç»ˆæ’åéªŒè¯ - æ”¯æŒæŠ½æ ·éªŒè¯
        
        Args:
            submission: æäº¤ç»“æœDataFrame
            sample_ratio: éªŒè¯çš„æ ·æœ¬æ¯”ä¾‹ï¼ˆ1.0è¡¨ç¤ºå…¨é‡éªŒè¯ï¼‰
            
        Returns:
            bool: æ˜¯å¦æ‰€æœ‰ç»„çš„æ’åéƒ½æœ‰æ•ˆ
        """
        unique_rankers = submission['ranker_id'].unique()
        total_groups = len(unique_rankers)
        
        # å¦‚æœæ•°æ®å¤ªå¤§ï¼ŒåªéªŒè¯æ ·æœ¬
        if total_groups > 10000 and sample_ratio < 1.0:
            sample_size = int(total_groups * sample_ratio)
            self._log(f"ğŸ” æ•°æ®é‡è¾ƒå¤§ï¼ŒéªŒè¯{sample_size}/{total_groups}ä¸ªç»„ "
                     f"({sample_ratio*100:.1f}%æ ·æœ¬)")
            
            sample_rankers = np.random.choice(unique_rankers, sample_size, replace=False)
            rankers_to_check = sample_rankers
        else:
            self._log(f"ğŸ” éªŒè¯æ‰€æœ‰{total_groups}ä¸ªç»„...")
            rankers_to_check = unique_rankers
        
        # éªŒè¯é€‰å®šçš„ç»„
        valid_count = 0
        for ranker_id in rankers_to_check:
            group_data = submission[submission['ranker_id'] == ranker_id]
            ranks = sorted(group_data['selected'].values)
            expected_ranks = list(range(1, len(group_data) + 1))
            
            if ranks == expected_ranks:
                valid_count += 1
            else:
                if sample_ratio >= 1.0:  # åªåœ¨å…¨é‡éªŒè¯æ—¶è¾“å‡ºè¯¦ç»†é”™è¯¯
                    self._log(f"âŒ æœ€ç»ˆéªŒè¯å¤±è´¥ - ranker_id {ranker_id}: "
                             f"æœŸæœ›{expected_ranks[:5]}..., å®é™…{ranks[:5]}...")
        
        validation_rate = valid_count / len(rankers_to_check)
        self._log(f"ğŸ“Š éªŒè¯ç»“æœ: {valid_count}/{len(rankers_to_check)} "
                 f"({validation_rate*100:.1f}%) ç»„é€šè¿‡éªŒè¯")
        
        # å¦‚æœæ˜¯æŠ½æ ·éªŒè¯ï¼Œ90%é€šè¿‡å°±è®¤ä¸ºå¯æ¥å—
        # å¦‚æœæ˜¯å…¨é‡éªŒè¯ï¼Œ100%é€šè¿‡æ‰è®¤ä¸ºæœ‰æ•ˆ
        threshold = 0.9 if sample_ratio < 1.0 else 1.0
        return validation_rate >= threshold
    
    def predict_all(self, 
                   segments: List[int] = None, 
                   model_names: List[str] = None,
                   ensemble_method: str = 'average') -> Optional[pd.DataFrame]:
        """
        é¢„æµ‹æ‰€æœ‰æŒ‡å®šæ•°æ®æ®µå¹¶ç”Ÿæˆæœ€ç»ˆæäº¤æ–‡ä»¶ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            segments: è¦é¢„æµ‹çš„æ•°æ®æ®µåˆ—è¡¨
            model_names: è¦ä½¿ç”¨çš„æ¨¡å‹åç§°åˆ—è¡¨
            ensemble_method: é›†æˆæ–¹æ³•
            
        Returns:
            Optional[pd.DataFrame]: æœ€ç»ˆé¢„æµ‹ç»“æœ
        """
        if segments is None:
            segments = [0, 1, 2]  # é»˜è®¤é¢„æµ‹å‰3ä¸ªæ®µ
        if model_names is None:
            model_names = ['XGBRanker']  # é»˜è®¤ä½¿ç”¨XGBRanker
        
        self._log(f"å¼€å§‹é¢„æµ‹æ‰€æœ‰æ•°æ®æ®µ: {segments}")
        self._log(f"ä½¿ç”¨æ¨¡å‹: {model_names}")
        self._monitor_memory("é¢„æµ‹å¼€å§‹å‰")
        
        all_predictions = []
        segment_results = {}
        
        # å¯¹æ¯ä¸ªæ•°æ®æ®µè¿›è¡Œé¢„æµ‹
        for segment_id in progress_bar(segments, desc="é¢„æµ‹æ•°æ®æ®µ"):
            self._log(f"\n{'='*50}")
            segment_predictions = {}
            
            # ä½¿ç”¨æ¯ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹
            for model_name in model_names:
                try:
                    self._log(f"ğŸ”„ ä½¿ç”¨{model_name}é¢„æµ‹segment_{segment_id}...")
                    prediction = self.predict_segment(segment_id, model_name)
                    if prediction is not None:
                        segment_predictions[model_name] = prediction
                        
                        # ä¿å­˜å•ä¸ªæ®µå•ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
                        segment_output = self.output_path / f"{model_name}_segment_{segment_id}_prediction.csv"
                        prediction.to_csv(segment_output, index=False)
                        self._log(f"å·²ä¿å­˜é¢„æµ‹ç»“æœ: {segment_output}")
                    
                except Exception as e:
                    self._log(f"æ¨¡å‹ {model_name} é¢„æµ‹ segment_{segment_id} å¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæœ‰å¤šä¸ªæ¨¡å‹ï¼Œè¿›è¡Œé›†æˆ
            if len(segment_predictions) > 1:
                ensemble_prediction = self._ensemble_segment_predictions_robust(
                    segment_predictions, ensemble_method, segment_id
                )
                segment_results[segment_id] = ensemble_prediction
            elif len(segment_predictions) == 1:
                segment_results[segment_id] = list(segment_predictions.values())[0]
            else:
                self._log(f"segment_{segment_id} æ²¡æœ‰æˆåŠŸçš„é¢„æµ‹ç»“æœ")
                continue
            
            all_predictions.append(segment_results[segment_id])
            
            # æ¸…ç†å½“å‰æ®µçš„å†…å­˜
            self._optimize_memory()
        
        # åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ
        if not all_predictions:
            self._log("æ²¡æœ‰æˆåŠŸçš„é¢„æµ‹ç»“æœ")
            return None
        
        self._log(f"ğŸ”— åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ...")
        self._monitor_memory("åˆå¹¶å‰")
        
        final_submission = pd.concat(all_predictions, ignore_index=True)
        
        # æ¸…ç†ä¸­é—´ç»“æœ
        del all_predictions
        self._optimize_memory()
        
        # æŒ‰Idæ’åº
        final_submission = final_submission.sort_values('Id').reset_index(drop=True)
        
        # æœ€ç»ˆéªŒè¯å’Œä¿®å¤
        self._log("æ‰§è¡Œæœ€ç»ˆæ’åéªŒè¯å’Œä¿®å¤...")
        final_submission = self._validate_and_fix_rankings(final_submission)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        model_suffix = "_".join(model_names) if len(model_names) > 1 else model_names[0]
        final_output = self.output_path / f"{model_suffix}_final_submission.csv"
        final_submission.to_csv(final_output, index=False)
        
        # ç»“æœéªŒè¯å’Œæ€»ç»“
        self._log(f"\n{'='*50}")
        self._log(f"é¢„æµ‹å®Œæˆ!")
        self._log(f"æœ€ç»ˆæäº¤æ–‡ä»¶: {final_output}")
        self._log(f"æ€»è®°å½•æ•°: {len(final_submission)}")
        self._log(f"å”¯ä¸€ranker_idæ•°é‡: {final_submission['ranker_id'].nunique()}")
        
        self.validate_predictions(final_submission)
        self._monitor_memory("æœ€ç»ˆå®Œæˆ")
        return final_submission
    
    def _ensemble_segment_predictions_robust(self, 
                                           predictions: Dict[str, pd.DataFrame],
                                           method: str = 'average',
                                           segment_id: int = 0) -> pd.DataFrame:
        """
        ç¨³å¥çš„é›†æˆå•ä¸ªæ®µçš„å¤šä¸ªæ¨¡å‹é¢„æµ‹ç»“æœ
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœå­—å…¸
            method: é›†æˆæ–¹æ³•
            segment_id: æ•°æ®æ®µID
            
        Returns:
            pd.DataFrame: é›†æˆåçš„é¢„æµ‹ç»“æœ
        """
        self._log(f"é›†æˆ {len(predictions)} ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œæ–¹æ³•: {method}")
        
        # è·å–åŸºç¡€æ•°æ®æ¡†æ¶
        base_df = list(predictions.values())[0][['Id', 'ranker_id']].copy()
        
        if method == 'average':
            # åˆ†æ•°å¹³å‡æ³•ï¼šéœ€è¦é‡æ–°è®¡ç®—æ’å
            all_scores = []
            for model_name, pred_df in predictions.items():
                # å°†æ’åè½¬æ¢ä¸ºåˆ†æ•°ï¼ˆæ’åè¶Šå°åˆ†æ•°è¶Šé«˜ï¼‰
                max_rank = pred_df.groupby('ranker_id')['selected'].transform('max')
                scores = max_rank - pred_df['selected'] + 1
                all_scores.append(scores)
            
            # è®¡ç®—å¹³å‡åˆ†æ•°
            avg_scores = np.mean(all_scores, axis=0)
            
            # é‡æ–°è®¡ç®—æ’å - ä½¿ç”¨ç¨³å¥æ–¹æ³•
            base_df['avg_score'] = avg_scores
            group_sizes = self._calculate_group_sizes(base_df['ranker_id'].values)
            new_ranks = self._calculate_group_ranks_robust(
                avg_scores, group_sizes, context_id=segment_id * 1000
            )
            base_df['selected'] = new_ranks
            
        elif method == 'voting':
            # æ’åæŠ•ç¥¨æ³•ï¼šå–å¹³å‡æ’åç„¶åé‡æ–°åˆ†é…
            all_ranks = []
            for model_name, pred_df in predictions.items():
                all_ranks.append(pred_df['selected'])
            
            # è®¡ç®—å¹³å‡æ’å
            avg_ranks = np.mean(all_ranks, axis=0)
            
            # åŸºäºå¹³å‡æ’åé‡æ–°è®¡ç®—å”¯ä¸€æ’å
            group_sizes = self._calculate_group_sizes(base_df['ranker_id'].values)
            # ä½¿ç”¨è´Ÿçš„å¹³å‡æ’åä½œä¸ºåˆ†æ•°ï¼ˆå¹³å‡æ’åè¶Šå°ï¼Œåˆ†æ•°è¶Šé«˜ï¼‰
            pseudo_scores = -avg_ranks
            new_ranks = self._calculate_group_ranks_robust(
                pseudo_scores, group_sizes, context_id=segment_id * 2000
            )
            base_df['selected'] = new_ranks
            
        else:
            # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„ç»“æœ
            base_df['selected'] = list(predictions.values())[0]['selected']
        
        # éªŒè¯é›†æˆç»“æœ
        is_valid = self._final_ranking_validation_optimized(base_df, sample_ratio=0.1)
        if not is_valid:
            self._log("é›†æˆç»“æœéªŒè¯å¤±è´¥ï¼Œæ‰§è¡Œä¿®å¤...")
            base_df = self._validate_and_fix_rankings(base_df)
        
        return base_df[['Id', 'ranker_id', 'selected']]
    
    def validate_predictions(self, submission: pd.DataFrame, sample_size: int = 5):
        """
        éªŒè¯é¢„æµ‹ç»“æœçš„æœ‰æ•ˆæ€§
        
        Args:
            submission: æäº¤ç»“æœ
            sample_size: æŠ½æ ·éªŒè¯çš„æ•°é‡
        """
        self._log("\néªŒè¯é¢„æµ‹ç»“æœ:")
        
        unique_rankers = submission['ranker_id'].unique()
        total_groups = len(unique_rankers)
        valid_groups = 0
        
        # æ£€æŸ¥æ‰€æœ‰ç»„
        for ranker_id in unique_rankers:
            group_data = submission[submission['ranker_id'] == ranker_id]
            ranks = sorted(group_data['selected'].values)
            expected_ranks = list(range(1, len(group_data) + 1))
            if ranks == expected_ranks:
                valid_groups += 1
        
        self._log(f"æ€»ç»„æ•°: {total_groups}")
        self._log(f"æœ‰æ•ˆç»„æ•°: {valid_groups}")
        self._log(f"æœ‰æ•ˆç‡: {valid_groups/total_groups:.2%}")
        
        # éšæœºæŠ½æ ·è¯¦ç»†æ£€æŸ¥
        if sample_size > 0:
            sample_rankers = np.random.choice(
                unique_rankers, 
                min(sample_size, len(unique_rankers)), 
                replace=False
            )
            
            self._log(f"\næŠ½æ ·æ£€æŸ¥ {len(sample_rankers)} ä¸ªç»„:")
            for ranker_id in sample_rankers:
                group_data = submission[submission['ranker_id'] == ranker_id]
                ranks = sorted(group_data['selected'].values)
                expected_ranks = list(range(1, len(group_data) + 1))
                is_valid = ranks == expected_ranks
                self._log(f"  ranker_id {ranker_id}: æ’å{'æœ‰æ•ˆ' if is_valid else 'æ— æ•ˆ'} "
                         f"(å¤§å°: {len(group_data)})")
                if not is_valid:
                    self._log(f"    å®é™…æ’å: {ranks[:10]}...")
                    self._log(f"    æœŸæœ›æ’å: {expected_ranks[:10]}...")
    
    def get_available_models(self) -> Dict[str, List[int]]:
        """
        è·å–å¯ç”¨çš„æ¨¡å‹å’Œå¯¹åº”çš„æ•°æ®æ®µ
        
        Returns:
            Dict: {æ¨¡å‹åç§°: [å¯ç”¨çš„æ®µIDåˆ—è¡¨]}
        """
        available_models = {}
        
        if not self.model_save_path.exists():
            return available_models
        
        # æ‰«ææ¨¡å‹æ–‡ä»¶
        for model_file in self.model_save_path.glob("*.pkl"):
            if model_file.name.startswith("features_"):
                continue
                
            # è§£ææ–‡ä»¶å: ModelName_segment_X.pkl
            name_parts = model_file.stem.split("_")
            if len(name_parts) >= 3 and name_parts[-2] == "segment":
                model_name = "_".join(name_parts[:-2])
                try:
                    segment_id = int(name_parts[-1])
                    if model_name not in available_models:
                        available_models[model_name] = []
                    available_models[model_name].append(segment_id)
                except ValueError:
                    continue
        
        # æ’åºæ®µID
        for model_name in available_models:
            available_models[model_name].sort()
        
        return available_models
    
    def print_model_summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦ä¿¡æ¯"""
        available_models = self.get_available_models()
        
        if not available_models:
            self._log("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹")
            return
        
        self._log("\nå¯ç”¨æ¨¡å‹æ‘˜è¦:")
        self._log("="*50)
        
        for model_name, segments in available_models.items():
            self._log(f"{model_name}: æ®µ {segments}")
            
            # å°è¯•è¯»å–æ¨¡å‹ä¿¡æ¯
            for segment_id in segments[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæ®µçš„è¯¦ç»†ä¿¡æ¯
                info_path = self.model_save_path / f"info_{model_name}_segment_{segment_id}.json"
                if info_path.exists():
                    try:
                        import json
                        with open(info_path, 'r', encoding='utf-8') as f:
                            info = json.load(f)
                        self._log(f"  æ®µ{segment_id}: ç‰¹å¾æ•°={info.get('feature_count', 'N/A')}, "
                                 f"æ€§èƒ½={info.get('performance', 'N/A'):.4f}")
                    except:
                        pass

    def _validate_and_fix_rankings(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        éªŒè¯å¹¶ä¿®å¤æ’åå”¯ä¸€æ€§ï¼ˆè¶…çº§ä¼˜åŒ–ç‰ˆæœ¬ï¼‰- ä¿®å¤å¤§æ•°æ®é›†å¡æ­»é—®é¢˜
        
        Args:
            df: åŒ…å«ranker_idå’Œselectedåˆ—çš„DataFrame
            
        Returns:
            pd.DataFrame: ä¿®å¤åçš„DataFrame
        """
        self._log("æ‰§è¡Œå…¨é¢çš„æ’åéªŒè¯å’Œä¿®å¤...")
        self._monitor_memory("æ’åä¿®å¤å‰")
        
        total_rows = len(df)
        unique_rankers = df['ranker_id'].unique()
        total_groups = len(unique_rankers)
        
        self._log(f"ğŸ“Š æ•°æ®è§„æ¨¡: {total_rows:,}è¡Œ, {total_groups:,}ä¸ªç»„")
        
        # æ ¹æ®æ•°æ®è§„æ¨¡é€‰æ‹©å¤„ç†ç­–ç•¥
        if total_groups > 100000:  # è¶…è¿‡10ä¸‡ä¸ªç»„
            self._log(f"âš¡ è¶…å¤§æ•°æ®é›†({total_groups:,}ç»„)ï¼Œä½¿ç”¨è¶…çº§ä¼˜åŒ–æ¨¡å¼...")
            return self._validate_and_fix_rankings_ultra(df)
        elif total_groups > 20000:  # 2-10ä¸‡ä¸ªç»„
            self._log(f"âš¡ å¤§æ•°æ®é›†({total_groups:,}ç»„)ï¼Œä½¿ç”¨åˆ†æ‰¹ä¼˜åŒ–æ¨¡å¼...")
            return self._validate_and_fix_rankings_batch(df)
        else:
            self._log(f"ğŸ“Š ä¸­ç­‰æ•°æ®é›†({total_groups:,}ç»„)ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼...")
            return self._validate_and_fix_rankings_standard(df)
    
    def _validate_and_fix_rankings_ultra(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è¶…çº§ä¼˜åŒ–æ¨¡å¼ - é€‚ç”¨äºè¶…å¤§æ•°æ®é›†ï¼ˆ>10ä¸‡ç»„ï¼‰
        ç­–ç•¥ï¼šè·³è¿‡éªŒè¯ï¼Œç›´æ¥ä¿¡ä»»åˆ†æ‰¹å¤„ç†çš„ç»“æœ
        """
        self._log("ğŸš€ å¯ç”¨è¶…çº§ä¼˜åŒ–æ¨¡å¼ï¼šè·³è¿‡å…¨é‡éªŒè¯ï¼Œä¿¡ä»»åˆ†æ‰¹å¤„ç†ç»“æœ")
        
        # åªéªŒè¯å°‘é‡æ ·æœ¬ä»¥ç¡®è®¤åŸºæœ¬æ­£ç¡®æ€§
        sample_size = min(1000, len(df['ranker_id'].unique()))
        sample_rankers = np.random.choice(df['ranker_id'].unique(), sample_size, replace=False)
        
        problem_count = 0
        for ranker_id in sample_rankers:
            group_data = df[df['ranker_id'] == ranker_id]
            ranks = sorted(group_data['selected'].values)
            expected_ranks = list(range(1, len(group_data) + 1))
            
            if ranks != expected_ranks:
                problem_count += 1
                # åªä¿®å¤æ ·æœ¬ä¸­çš„é—®é¢˜
                fix_seed = abs(hash(str(ranker_id))) % 1000000
                np.random.seed(fix_seed)
                new_ranks = np.random.permutation(range(1, len(group_data) + 1))
                df.loc[df['ranker_id'] == ranker_id, 'selected'] = new_ranks
        
        self._log(f"ğŸ“Š æ ·æœ¬éªŒè¯å®Œæˆ: ä¿®å¤äº† {problem_count}/{sample_size} ä¸ªé—®é¢˜ç»„")
        self._log(f"âœ… è¶…çº§ä¼˜åŒ–æ¨¡å¼å®Œæˆï¼Œè·³è¿‡å…¨é‡éªŒè¯ä»¥èŠ‚çœæ—¶é—´")
        
        self._monitor_memory("è¶…çº§ä¼˜åŒ–æ¨¡å¼å®Œæˆ")
        return df
    
    def _validate_and_fix_rankings_batch(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ†æ‰¹ä¼˜åŒ–æ¨¡å¼ - é€‚ç”¨äºå¤§æ•°æ®é›†ï¼ˆ2-10ä¸‡ç»„ï¼‰
        """
        self._log("âš¡ å¯ç”¨åˆ†æ‰¹ä¼˜åŒ–æ¨¡å¼...")
        
        unique_rankers = df['ranker_id'].unique()
        total_groups = len(unique_rankers)
        problem_groups = 0
        
        # åˆ†æ‰¹å¤„ç†
        batch_size = 10000  # æ¯æ‰¹å¤„ç†1ä¸‡ä¸ªç»„
        total_batches = (total_groups + batch_size - 1) // batch_size
        
        self._log(f"ğŸ“¦ å°†{total_groups:,}ä¸ªç»„åˆ†ä¸º{total_batches}æ‰¹å¤„ç†...")
        
        for batch_idx in range(0, total_groups, batch_size):
            batch_num = batch_idx // batch_size + 1
            end_idx = min(batch_idx + batch_size, total_groups)
            batch_rankers = unique_rankers[batch_idx:end_idx]
            
            self._log(f"ğŸ”„ å¤„ç†ç¬¬{batch_num}/{total_batches}æ‰¹ ({len(batch_rankers)}ä¸ªç»„)...")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            for ranker_id in batch_rankers:
                group_mask = df['ranker_id'] == ranker_id
                group_data = df[group_mask]
                group_size = len(group_data)
                
                current_ranks = sorted(group_data['selected'].values)
                expected_ranks = list(range(1, group_size + 1))
                
                if current_ranks != expected_ranks:
                    problem_groups += 1
                    
                    # ä¿®å¤æ’å
                    fix_seed = abs(hash(str(ranker_id))) % 1000000
                    np.random.seed(fix_seed)
                    new_ranks = np.random.permutation(range(1, group_size + 1))
                    df.loc[group_mask, 'selected'] = new_ranks
            
            # æ¯10æ‰¹è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if batch_num % 10 == 0:
                self._log(f"ğŸ“Š å·²å¤„ç† {end_idx:,}/{total_groups:,} ä¸ªç»„ï¼Œå‘ç° {problem_groups} ä¸ªé—®é¢˜")
                self._monitor_memory(f"ç¬¬{batch_num}æ‰¹å®Œæˆ")
        
        self._log(f"âœ… åˆ†æ‰¹æ¨¡å¼å®Œæˆ: ä¿®å¤äº† {problem_groups}/{total_groups} ä¸ªé—®é¢˜ç»„")
        return df
    
    def _validate_and_fix_rankings_standard(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ ‡å‡†æ¨¡å¼ - é€‚ç”¨äºä¸­å°æ•°æ®é›†ï¼ˆ<2ä¸‡ç»„ï¼‰
        """
        self._log("ğŸ“Š ä½¿ç”¨æ ‡å‡†éªŒè¯æ¨¡å¼...")
        
        fixed_df = df.copy()
        problem_groups = 0
        total_groups = 0
        
        unique_rankers = df['ranker_id'].unique()
        
        for ranker_id in unique_rankers:
            total_groups += 1
            group_mask = df['ranker_id'] == ranker_id
            group_data = df[group_mask]
            group_size = len(group_data)
            
            current_ranks = sorted(group_data['selected'].values)
            expected_ranks = list(range(1, group_size + 1))
            
            if current_ranks != expected_ranks:
                problem_groups += 1
                
                # ä½¿ç”¨ranker_idçš„å“ˆå¸Œå€¼ä½œä¸ºéšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
                fix_seed = abs(hash(str(ranker_id))) % 1000000
                np.random.seed(fix_seed)
                
                # ç”Ÿæˆå”¯ä¸€çš„1åˆ°group_sizeçš„æ’åˆ—
                new_ranks = np.random.permutation(range(1, group_size + 1))
                fixed_df.loc[group_mask, 'selected'] = new_ranks
            
            # æ¯1000ä¸ªç»„è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if total_groups % 1000 == 0:
                self._log(f"ğŸ“Š å·²éªŒè¯ {total_groups:,}/{len(unique_rankers):,} ä¸ªç»„...")
        
        self._log(f"âœ… æ ‡å‡†æ¨¡å¼å®Œæˆ: ä¿®å¤äº† {problem_groups}/{total_groups} ä¸ªé—®é¢˜ç»„")
        return fixed_df
    
    def _fix_ranking_chunk(self, fixed_df: pd.DataFrame, chunk_mask: np.ndarray, 
                          chunk_rankers: np.ndarray) -> int:
        """ä¿®å¤ä¸€æ‰¹rankerçš„æ’åé—®é¢˜"""
        problem_count = 0
        
        for ranker_id in chunk_rankers:
            group_mask = fixed_df['ranker_id'] == ranker_id
            group_data = fixed_df[group_mask]
            group_size = len(group_data)
            
            current_ranks = sorted(group_data['selected'].values)
            expected_ranks = list(range(1, group_size + 1))
            
            if current_ranks != expected_ranks:
                problem_count += 1
                
                # ä¿®å¤æ’å
                fix_seed = abs(hash(str(ranker_id))) % 1000000
                np.random.seed(fix_seed)
                new_ranks = np.random.permutation(range(1, group_size + 1))
                fixed_df.loc[group_mask, 'selected'] = new_ranks
        
        return problem_count